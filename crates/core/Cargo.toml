[package]
name = "vllm-core"
version = "0.1.0"
edition.workspace = true

[features]
test-utils = []
cuda = ["candle-core/cuda", "candle-nn/cuda"]
flash-attn = ["dep:candle-flash-attn"]
flashinfer = ["dep:flashinfer-rs", "flashinfer-rs/cuda", "cuda"]
cuda-kernels = ["cuda"]
cuda-fused-activations = ["cuda-kernels"]
cuda-layernorm = ["cuda-kernels"]
cuda-moe = ["cuda-kernels"]
marlin = ["cuda-kernels"]
image-loading = ["dep:image", "dep:base64"]
audio = ["dep:symphonia"]

# Test tiers based on GPU memory requirements
gpu-test-small = []      # Tests requiring < 2GB VRAM
gpu-test-medium = ["gpu-test-small"]  # Tests requiring < 6GB VRAM
gpu-test-large = ["gpu-test-medium"]  # Tests requiring > 6GB VRAM
multi-gpu-test = ["gpu-test-large"]   # Tests requiring multiple GPUs

[dependencies]
tokio.workspace = true
thiserror.workspace = true
anyhow.workspace = true
serde.workspace = true
serde_json.workspace = true
candle-core.workspace = true
candle-nn.workspace = true
hf-hub.workspace = true
tokenizers.workspace = true
minijinja.workspace = true
minijinja-contrib.workspace = true
ahash.workspace = true
candle-flash-attn = { workspace = true, optional = true }
flashinfer-rs = { git = "https://github.com/vasia123/flashinfer-rs", optional = true }
image = { workspace = true, optional = true }
base64 = { workspace = true, optional = true }
symphonia = { workspace = true, optional = true }
half.workspace = true
rand.workspace = true
libloading.workspace = true
tracing.workspace = true
regex.workspace = true
uuid.workspace = true

[dev-dependencies]
criterion = { version = "0.5", features = ["html_reports"] }

[[bench]]
name = "sampling_bench"
harness = false

[[bench]]
name = "attention_bench"
harness = false

[[bench]]
name = "quantization_bench"
harness = false
