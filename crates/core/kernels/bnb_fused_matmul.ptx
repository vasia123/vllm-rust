//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32267302
// Cuda compilation tools, release 12.0, V12.0.140
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_89
.address_size 64

	// .globl	bnb_nf4_gemm_f32_naive
.const .align 4 .b8 NF4_CODEBOOK[64] = {0, 0, 128, 191, 177, 57, 50, 191, 48, 107, 6, 191, 160, 50, 202, 190, 77, 162, 145, 190, 63, 53, 61, 190, 113, 120, 186, 189, 0, 0, 0, 0, 255, 250, 162, 61, 227, 202, 36, 62, 221, 4, 124, 62, 58, 3, 173, 62, 184, 164, 225, 62, 171, 7, 16, 63, 179, 19, 57, 63, 0, 0, 128, 63};
// _ZZ22bnb_nf4_gemm_f32_tiledE10input_tile has been demoted
// _ZZ21bnb_nf4_gemm_f32_biasE10input_tile has been demoted
// _ZZ23bnb_int8_gemm_f32_tiledE10input_tile has been demoted
// _ZZ22bnb_int8_gemm_f32_biasE10input_tile has been demoted

.visible .entry bnb_nf4_gemm_f32_naive(
	.param .u64 bnb_nf4_gemm_f32_naive_param_0,
	.param .u64 bnb_nf4_gemm_f32_naive_param_1,
	.param .u64 bnb_nf4_gemm_f32_naive_param_2,
	.param .u64 bnb_nf4_gemm_f32_naive_param_3,
	.param .u32 bnb_nf4_gemm_f32_naive_param_4,
	.param .u32 bnb_nf4_gemm_f32_naive_param_5,
	.param .u32 bnb_nf4_gemm_f32_naive_param_6,
	.param .u32 bnb_nf4_gemm_f32_naive_param_7
)
{
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<6>;
	.reg .f32 	%f<40>;
	.reg .b32 	%r<89>;
	.reg .b64 	%rd<54>;


	ld.param.u64 	%rd10, [bnb_nf4_gemm_f32_naive_param_0];
	ld.param.u64 	%rd11, [bnb_nf4_gemm_f32_naive_param_1];
	ld.param.u64 	%rd12, [bnb_nf4_gemm_f32_naive_param_2];
	ld.param.u64 	%rd13, [bnb_nf4_gemm_f32_naive_param_3];
	ld.param.u32 	%r21, [bnb_nf4_gemm_f32_naive_param_4];
	ld.param.u32 	%r18, [bnb_nf4_gemm_f32_naive_param_5];
	ld.param.u32 	%r19, [bnb_nf4_gemm_f32_naive_param_6];
	ld.param.u32 	%r20, [bnb_nf4_gemm_f32_naive_param_7];
	cvta.to.global.u64 	%rd1, %rd11;
	cvta.to.global.u64 	%rd2, %rd13;
	cvta.to.global.u64 	%rd3, %rd12;
	mov.u32 	%r22, %ntid.y;
	mov.u32 	%r23, %ctaid.y;
	mov.u32 	%r24, %tid.y;
	mad.lo.s32 	%r1, %r23, %r22, %r24;
	mov.u32 	%r25, %ntid.x;
	mov.u32 	%r26, %ctaid.x;
	mov.u32 	%r27, %tid.x;
	mad.lo.s32 	%r2, %r26, %r25, %r27;
	setp.ge.s32 	%p1, %r1, %r21;
	setp.ge.s32 	%p2, %r2, %r18;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB0_9;

	setp.lt.s32 	%p4, %r19, 1;
	mov.f32 	%f39, 0f00000000;
	@%p4 bra 	$L__BB0_8;

	mul.lo.s32 	%r3, %r2, %r19;
	and.b32  	%r88, %r19, 3;
	add.s32 	%r29, %r19, -1;
	setp.lt.u32 	%p5, %r29, 3;
	mov.f32 	%f39, 0f00000000;
	mov.u32 	%r86, 0;
	@%p5 bra 	$L__BB0_5;

	and.b32  	%r5, %r3, 1;
	mul.lo.s32 	%r31, %r19, %r1;
	mul.wide.s32 	%rd14, %r31, 4;
	add.s64 	%rd15, %rd1, %rd14;
	add.s64 	%rd52, %rd15, 8;
	add.s32 	%r84, %r3, 3;
	sub.s32 	%r7, %r88, %r19;
	mov.u64 	%rd21, NF4_CODEBOOK;

$L__BB0_4:
	add.s32 	%r32, %r84, -3;
	shr.u32 	%r33, %r32, 31;
	add.s32 	%r34, %r32, %r33;
	shr.s32 	%r35, %r34, 1;
	cvt.s64.s32 	%rd16, %r35;
	add.s64 	%rd17, %rd3, %rd16;
	ld.global.nc.u8 	%rs1, [%rd17];
	cvt.u32.u16 	%r36, %rs1;
	and.b32  	%r37, %r36, 240;
	shr.u32 	%r38, %r37, 4;
	and.b32  	%r39, %r36, 15;
	setp.ne.s32 	%p6, %r5, 0;
	selp.b32 	%r40, %r38, %r39, %p6;
	div.s32 	%r41, %r32, %r20;
	mul.wide.s32 	%rd18, %r41, 4;
	add.s64 	%rd19, %rd2, %rd18;
	mul.wide.u32 	%rd20, %r40, 4;
	add.s64 	%rd22, %rd21, %rd20;
	ld.const.f32 	%f12, [%rd22];
	ld.global.nc.f32 	%f13, [%rd19];
	mul.ftz.f32 	%f14, %f13, %f12;
	ld.global.nc.f32 	%f15, [%rd52+-8];
	fma.rn.ftz.f32 	%f16, %f15, %f14, %f39;
	add.s32 	%r42, %r84, -2;
	shr.u32 	%r43, %r42, 31;
	add.s32 	%r44, %r42, %r43;
	shr.s32 	%r45, %r44, 1;
	cvt.s64.s32 	%rd23, %r45;
	add.s64 	%rd24, %rd3, %rd23;
	ld.global.nc.u8 	%rs2, [%rd24];
	cvt.u32.u16 	%r46, %rs2;
	and.b32  	%r47, %r46, 240;
	shr.u32 	%r48, %r47, 4;
	and.b32  	%r49, %r46, 15;
	selp.b32 	%r50, %r49, %r48, %p6;
	div.s32 	%r51, %r42, %r20;
	mul.wide.s32 	%rd25, %r51, 4;
	add.s64 	%rd26, %rd2, %rd25;
	mul.wide.u32 	%rd27, %r50, 4;
	add.s64 	%rd28, %rd21, %rd27;
	ld.const.f32 	%f17, [%rd28];
	ld.global.nc.f32 	%f18, [%rd26];
	mul.ftz.f32 	%f19, %f18, %f17;
	ld.global.nc.f32 	%f20, [%rd52+-4];
	fma.rn.ftz.f32 	%f21, %f20, %f19, %f16;
	add.s32 	%r52, %r84, -1;
	shr.u32 	%r53, %r52, 31;
	add.s32 	%r54, %r52, %r53;
	shr.s32 	%r55, %r54, 1;
	cvt.s64.s32 	%rd29, %r55;
	add.s64 	%rd30, %rd3, %rd29;
	ld.global.nc.u8 	%rs3, [%rd30];
	cvt.u32.u16 	%r56, %rs3;
	and.b32  	%r57, %r56, 240;
	shr.u32 	%r58, %r57, 4;
	and.b32  	%r59, %r56, 15;
	selp.b32 	%r60, %r58, %r59, %p6;
	div.s32 	%r61, %r52, %r20;
	mul.wide.s32 	%rd31, %r61, 4;
	add.s64 	%rd32, %rd2, %rd31;
	mul.wide.u32 	%rd33, %r60, 4;
	add.s64 	%rd34, %rd21, %rd33;
	ld.const.f32 	%f22, [%rd34];
	ld.global.nc.f32 	%f23, [%rd32];
	mul.ftz.f32 	%f24, %f23, %f22;
	ld.global.nc.f32 	%f25, [%rd52];
	fma.rn.ftz.f32 	%f26, %f25, %f24, %f21;
	shr.u32 	%r62, %r84, 31;
	add.s32 	%r63, %r84, %r62;
	shr.s32 	%r64, %r63, 1;
	cvt.s64.s32 	%rd35, %r64;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.nc.u8 	%rs4, [%rd36];
	cvt.u32.u16 	%r65, %rs4;
	and.b32  	%r66, %r65, 240;
	shr.u32 	%r67, %r66, 4;
	and.b32  	%r68, %r65, 15;
	selp.b32 	%r69, %r68, %r67, %p6;
	div.s32 	%r70, %r84, %r20;
	mul.wide.s32 	%rd37, %r70, 4;
	add.s64 	%rd38, %rd2, %rd37;
	mul.wide.u32 	%rd39, %r69, 4;
	add.s64 	%rd40, %rd21, %rd39;
	ld.const.f32 	%f27, [%rd40];
	ld.global.nc.f32 	%f28, [%rd38];
	mul.ftz.f32 	%f29, %f28, %f27;
	ld.global.nc.f32 	%f30, [%rd52+4];
	fma.rn.ftz.f32 	%f39, %f30, %f29, %f26;
	add.s64 	%rd52, %rd52, 16;
	add.s32 	%r84, %r84, 4;
	add.s32 	%r86, %r86, 4;
	add.s32 	%r71, %r7, %r86;
	setp.ne.s32 	%p7, %r71, 0;
	@%p7 bra 	$L__BB0_4;

$L__BB0_5:
	setp.eq.s32 	%p8, %r88, 0;
	@%p8 bra 	$L__BB0_8;

	mad.lo.s32 	%r72, %r19, %r1, %r86;
	mul.wide.s32 	%rd41, %r72, 4;
	add.s64 	%rd53, %rd1, %rd41;
	add.s32 	%r87, %r86, %r3;
	mov.u64 	%rd47, NF4_CODEBOOK;

$L__BB0_7:
	.pragma "nounroll";
	shr.u32 	%r73, %r87, 31;
	add.s32 	%r74, %r87, %r73;
	shr.s32 	%r75, %r74, 1;
	cvt.s64.s32 	%rd42, %r75;
	add.s64 	%rd43, %rd3, %rd42;
	and.b32  	%r76, %r87, 1;
	setp.eq.b32 	%p9, %r76, 1;
	ld.global.nc.u8 	%rs5, [%rd43];
	cvt.u32.u16 	%r77, %rs5;
	and.b32  	%r78, %r77, 240;
	shr.u32 	%r79, %r78, 4;
	and.b32  	%r80, %r77, 15;
	selp.b32 	%r81, %r79, %r80, %p9;
	div.s32 	%r82, %r87, %r20;
	mul.wide.s32 	%rd44, %r82, 4;
	add.s64 	%rd45, %rd2, %rd44;
	mul.wide.u32 	%rd46, %r81, 4;
	add.s64 	%rd48, %rd47, %rd46;
	ld.const.f32 	%f31, [%rd48];
	ld.global.nc.f32 	%f32, [%rd45];
	mul.ftz.f32 	%f33, %f32, %f31;
	ld.global.nc.f32 	%f34, [%rd53];
	fma.rn.ftz.f32 	%f39, %f34, %f33, %f39;
	add.s64 	%rd53, %rd53, 4;
	add.s32 	%r87, %r87, 1;
	add.s32 	%r88, %r88, -1;
	setp.ne.s32 	%p10, %r88, 0;
	@%p10 bra 	$L__BB0_7;

$L__BB0_8:
	mad.lo.s32 	%r83, %r1, %r18, %r2;
	cvta.to.global.u64 	%rd49, %rd10;
	mul.wide.s32 	%rd50, %r83, 4;
	add.s64 	%rd51, %rd49, %rd50;
	st.global.f32 	[%rd51], %f39;

$L__BB0_9:
	ret;

}
	// .globl	bnb_nf4_gemm_f32_tiled
.visible .entry bnb_nf4_gemm_f32_tiled(
	.param .u64 bnb_nf4_gemm_f32_tiled_param_0,
	.param .u64 bnb_nf4_gemm_f32_tiled_param_1,
	.param .u64 bnb_nf4_gemm_f32_tiled_param_2,
	.param .u64 bnb_nf4_gemm_f32_tiled_param_3,
	.param .u32 bnb_nf4_gemm_f32_tiled_param_4,
	.param .u32 bnb_nf4_gemm_f32_tiled_param_5,
	.param .u32 bnb_nf4_gemm_f32_tiled_param_6,
	.param .u32 bnb_nf4_gemm_f32_tiled_param_7
)
{
	.reg .pred 	%p<28>;
	.reg .b16 	%rs<8>;
	.reg .f32 	%f<55>;
	.reg .b32 	%r<141>;
	.reg .b64 	%rd<59>;
	// demoted variable
	.shared .align 4 .b8 _ZZ22bnb_nf4_gemm_f32_tiledE10input_tile[2112];

	ld.param.u64 	%rd4, [bnb_nf4_gemm_f32_tiled_param_0];
	ld.param.u64 	%rd5, [bnb_nf4_gemm_f32_tiled_param_1];
	ld.param.u64 	%rd6, [bnb_nf4_gemm_f32_tiled_param_2];
	ld.param.u64 	%rd7, [bnb_nf4_gemm_f32_tiled_param_3];
	ld.param.u32 	%r32, [bnb_nf4_gemm_f32_tiled_param_4];
	ld.param.u32 	%r33, [bnb_nf4_gemm_f32_tiled_param_5];
	ld.param.u32 	%r34, [bnb_nf4_gemm_f32_tiled_param_6];
	ld.param.u32 	%r35, [bnb_nf4_gemm_f32_tiled_param_7];
	cvta.to.global.u64 	%rd1, %rd7;
	cvta.to.global.u64 	%rd2, %rd6;
	mov.u32 	%r36, %ctaid.y;
	shl.b32 	%r37, %r36, 4;
	mov.u32 	%r1, %tid.y;
	add.s32 	%r2, %r37, %r1;
	mov.u32 	%r38, %ctaid.x;
	shl.b32 	%r39, %r38, 4;
	mov.u32 	%r3, %tid.x;
	add.s32 	%r4, %r39, %r3;
	setp.lt.s32 	%p2, %r34, 1;
	mov.f32 	%f53, 0f00000000;
	@%p2 bra 	$L__BB1_16;

	setp.lt.s32 	%p3, %r2, %r32;
	setp.lt.s32 	%p4, %r4, %r33;
	and.pred  	%p1, %p3, %p4;
	mul.lo.s32 	%r5, %r2, %r34;
	not.b32 	%r6, %r34;
	mul.lo.s32 	%r7, %r4, %r34;
	and.b32  	%r8, %r7, 1;
	mov.u32 	%r42, _ZZ22bnb_nf4_gemm_f32_tiledE10input_tile;
	mad.lo.s32 	%r43, %r1, 132, %r42;
	add.s32 	%r9, %r43, 8;
	add.s32 	%r10, %r7, 3;
	cvta.to.global.u64 	%rd3, %rd5;
	mov.u32 	%r134, 0;
	not.pred 	%p13, %p1;
	mov.u32 	%r135, %r134;

$L__BB1_2:
	add.s32 	%r44, %r134, %r6;
	max.s32 	%r45, %r44, -33;
	not.b32 	%r46, %r45;
	max.s32 	%r13, %r46, 1;
	add.s32 	%r47, %r3, %r135;
	setp.ge.s32 	%p5, %r47, %r34;
	setp.gt.s32 	%p6, %r3, 31;
	or.pred  	%p7, %p6, %p5;
	@%p7 bra 	$L__BB1_7;

	add.s32 	%r14, %r135, %r5;
	mov.u32 	%r136, %r3;

$L__BB1_4:
	setp.ge.s32 	%p8, %r2, %r32;
	mov.f32 	%f49, 0f00000000;
	@%p8 bra 	$L__BB1_6;

	add.s32 	%r48, %r14, %r136;
	mul.wide.s32 	%rd8, %r48, 4;
	add.s64 	%rd9, %rd3, %rd8;
	ld.global.nc.f32 	%f49, [%rd9];

$L__BB1_6:
	shl.b32 	%r51, %r136, 2;
	add.s32 	%r52, %r43, %r51;
	st.shared.f32 	[%r52], %f49;
	add.s32 	%r16, %r136, 16;
	add.s32 	%r53, %r16, %r135;
	setp.lt.s32 	%p9, %r53, %r34;
	setp.lt.s32 	%p10, %r136, 16;
	and.pred  	%p11, %p10, %p9;
	mov.u32 	%r136, %r16;
	@%p11 bra 	$L__BB1_4;

$L__BB1_7:
	bar.sync 	0;
	setp.ge.s32 	%p12, %r135, %r34;
	or.pred  	%p14, %p13, %p12;
	@%p14 bra 	$L__BB1_15;

	and.b32  	%r17, %r13, 3;
	add.s32 	%r55, %r13, -1;
	setp.lt.u32 	%p15, %r55, 3;
	mov.u32 	%r140, 0;
	@%p15 bra 	$L__BB1_11;

	add.s32 	%r137, %r10, %r135;
	sub.s32 	%r20, %r17, %r13;
	mov.u32 	%r138, %r9;

$L__BB1_10:
	add.s32 	%r57, %r137, -3;
	shr.u32 	%r58, %r57, 31;
	add.s32 	%r59, %r57, %r58;
	shr.s32 	%r60, %r59, 1;
	cvt.s64.s32 	%rd10, %r60;
	add.s64 	%rd11, %rd2, %rd10;
	ld.global.nc.u8 	%rs1, [%rd11];
	cvt.u32.u16 	%r61, %rs1;
	and.b32  	%r62, %r61, 240;
	shr.u32 	%r63, %r62, 4;
	and.b32  	%r64, %r61, 15;
	setp.ne.s32 	%p16, %r8, 0;
	selp.b32 	%r65, %r63, %r64, %p16;
	div.s32 	%r66, %r57, %r35;
	mul.wide.s32 	%rd12, %r66, 4;
	add.s64 	%rd13, %rd1, %rd12;
	mul.wide.u32 	%rd14, %r65, 4;
	mov.u64 	%rd15, NF4_CODEBOOK;
	add.s64 	%rd16, %rd15, %rd14;
	ld.const.f32 	%f17, [%rd16];
	ld.global.nc.f32 	%f18, [%rd13];
	mul.ftz.f32 	%f19, %f18, %f17;
	ld.shared.f32 	%f20, [%r138+-8];
	fma.rn.ftz.f32 	%f21, %f20, %f19, %f53;
	add.s32 	%r67, %r137, -2;
	shr.u32 	%r68, %r67, 31;
	add.s32 	%r69, %r67, %r68;
	shr.s32 	%r70, %r69, 1;
	cvt.s64.s32 	%rd17, %r70;
	add.s64 	%rd18, %rd2, %rd17;
	ld.global.nc.u8 	%rs2, [%rd18];
	cvt.u32.u16 	%r71, %rs2;
	and.b32  	%r72, %r71, 240;
	shr.u32 	%r73, %r72, 4;
	and.b32  	%r74, %r71, 15;
	selp.b32 	%r75, %r74, %r73, %p16;
	div.s32 	%r76, %r67, %r35;
	mul.wide.s32 	%rd19, %r76, 4;
	add.s64 	%rd20, %rd1, %rd19;
	mul.wide.u32 	%rd21, %r75, 4;
	add.s64 	%rd22, %rd15, %rd21;
	ld.const.f32 	%f22, [%rd22];
	ld.global.nc.f32 	%f23, [%rd20];
	mul.ftz.f32 	%f24, %f23, %f22;
	ld.shared.f32 	%f25, [%r138+-4];
	fma.rn.ftz.f32 	%f26, %f25, %f24, %f21;
	add.s32 	%r77, %r137, -1;
	shr.u32 	%r78, %r77, 31;
	add.s32 	%r79, %r77, %r78;
	shr.s32 	%r80, %r79, 1;
	cvt.s64.s32 	%rd23, %r80;
	add.s64 	%rd24, %rd2, %rd23;
	ld.global.nc.u8 	%rs3, [%rd24];
	cvt.u32.u16 	%r81, %rs3;
	and.b32  	%r82, %r81, 240;
	shr.u32 	%r83, %r82, 4;
	and.b32  	%r84, %r81, 15;
	selp.b32 	%r85, %r83, %r84, %p16;
	div.s32 	%r86, %r77, %r35;
	mul.wide.s32 	%rd25, %r86, 4;
	add.s64 	%rd26, %rd1, %rd25;
	mul.wide.u32 	%rd27, %r85, 4;
	add.s64 	%rd28, %rd15, %rd27;
	ld.const.f32 	%f27, [%rd28];
	ld.global.nc.f32 	%f28, [%rd26];
	mul.ftz.f32 	%f29, %f28, %f27;
	ld.shared.f32 	%f30, [%r138];
	fma.rn.ftz.f32 	%f31, %f30, %f29, %f26;
	shr.u32 	%r87, %r137, 31;
	add.s32 	%r88, %r137, %r87;
	shr.s32 	%r89, %r88, 1;
	cvt.s64.s32 	%rd29, %r89;
	add.s64 	%rd30, %rd2, %rd29;
	ld.global.nc.u8 	%rs4, [%rd30];
	cvt.u32.u16 	%r90, %rs4;
	and.b32  	%r91, %r90, 240;
	shr.u32 	%r92, %r91, 4;
	and.b32  	%r93, %r90, 15;
	selp.b32 	%r94, %r93, %r92, %p16;
	div.s32 	%r95, %r137, %r35;
	mul.wide.s32 	%rd31, %r95, 4;
	add.s64 	%rd32, %rd1, %rd31;
	mul.wide.u32 	%rd33, %r94, 4;
	add.s64 	%rd34, %rd15, %rd33;
	ld.const.f32 	%f32, [%rd34];
	ld.global.nc.f32 	%f33, [%rd32];
	mul.ftz.f32 	%f34, %f33, %f32;
	ld.shared.f32 	%f35, [%r138+4];
	fma.rn.ftz.f32 	%f53, %f35, %f34, %f31;
	add.s32 	%r138, %r138, 16;
	add.s32 	%r137, %r137, 4;
	add.s32 	%r140, %r140, 4;
	add.s32 	%r96, %r20, %r140;
	setp.ne.s32 	%p17, %r96, 0;
	@%p17 bra 	$L__BB1_10;

$L__BB1_11:
	setp.eq.s32 	%p18, %r17, 0;
	@%p18 bra 	$L__BB1_15;

	add.s32 	%r97, %r135, %r7;
	add.s32 	%r28, %r97, %r140;
	shr.u32 	%r98, %r28, 31;
	add.s32 	%r99, %r28, %r98;
	shr.s32 	%r100, %r99, 1;
	cvt.s64.s32 	%rd35, %r100;
	add.s64 	%rd36, %rd2, %rd35;
	and.b32  	%r101, %r28, 1;
	setp.eq.b32 	%p19, %r101, 1;
	ld.global.nc.u8 	%rs5, [%rd36];
	cvt.u32.u16 	%r102, %rs5;
	and.b32  	%r103, %r102, 240;
	shr.u32 	%r104, %r103, 4;
	and.b32  	%r105, %r102, 15;
	selp.b32 	%r106, %r104, %r105, %p19;
	div.s32 	%r107, %r28, %r35;
	mul.wide.s32 	%rd37, %r107, 4;
	add.s64 	%rd38, %rd1, %rd37;
	mul.wide.u32 	%rd39, %r106, 4;
	mov.u64 	%rd40, NF4_CODEBOOK;
	add.s64 	%rd41, %rd40, %rd39;
	ld.const.f32 	%f36, [%rd41];
	ld.global.nc.f32 	%f37, [%rd38];
	mul.ftz.f32 	%f38, %f37, %f36;
	shl.b32 	%r110, %r140, 2;
	add.s32 	%r29, %r43, %r110;
	ld.shared.f32 	%f39, [%r29];
	fma.rn.ftz.f32 	%f53, %f39, %f38, %f53;
	setp.eq.s32 	%p20, %r17, 1;
	@%p20 bra 	$L__BB1_15;

	add.s32 	%r111, %r28, 1;
	shr.u32 	%r112, %r111, 31;
	add.s32 	%r113, %r111, %r112;
	shr.s32 	%r114, %r113, 1;
	cvt.s64.s32 	%rd42, %r114;
	add.s64 	%rd43, %rd2, %rd42;
	and.b32  	%r115, %r111, 1;
	setp.eq.b32 	%p21, %r115, 1;
	ld.global.nc.u8 	%rs6, [%rd43];
	cvt.u32.u16 	%r116, %rs6;
	and.b32  	%r117, %r116, 240;
	shr.u32 	%r118, %r117, 4;
	and.b32  	%r119, %r116, 15;
	selp.b32 	%r120, %r118, %r119, %p21;
	div.s32 	%r121, %r111, %r35;
	mul.wide.s32 	%rd44, %r121, 4;
	add.s64 	%rd45, %rd1, %rd44;
	mul.wide.u32 	%rd46, %r120, 4;
	add.s64 	%rd48, %rd40, %rd46;
	ld.const.f32 	%f40, [%rd48];
	ld.global.nc.f32 	%f41, [%rd45];
	mul.ftz.f32 	%f42, %f41, %f40;
	ld.shared.f32 	%f43, [%r29+4];
	fma.rn.ftz.f32 	%f53, %f43, %f42, %f53;
	setp.eq.s32 	%p22, %r17, 2;
	@%p22 bra 	$L__BB1_15;

	add.s32 	%r122, %r28, 2;
	shr.u32 	%r123, %r122, 31;
	add.s32 	%r124, %r122, %r123;
	shr.s32 	%r125, %r124, 1;
	cvt.s64.s32 	%rd49, %r125;
	add.s64 	%rd50, %rd2, %rd49;
	and.b32  	%r126, %r122, 1;
	setp.eq.b32 	%p23, %r126, 1;
	ld.global.nc.u8 	%rs7, [%rd50];
	cvt.u32.u16 	%r127, %rs7;
	and.b32  	%r128, %r127, 240;
	shr.u32 	%r129, %r128, 4;
	and.b32  	%r130, %r127, 15;
	selp.b32 	%r131, %r129, %r130, %p23;
	div.s32 	%r132, %r122, %r35;
	mul.wide.s32 	%rd51, %r132, 4;
	add.s64 	%rd52, %rd1, %rd51;
	mul.wide.u32 	%rd53, %r131, 4;
	add.s64 	%rd55, %rd40, %rd53;
	ld.const.f32 	%f44, [%rd55];
	ld.global.nc.f32 	%f45, [%rd52];
	mul.ftz.f32 	%f46, %f45, %f44;
	ld.shared.f32 	%f47, [%r29+8];
	fma.rn.ftz.f32 	%f53, %f47, %f46, %f53;

$L__BB1_15:
	bar.sync 	0;
	add.s32 	%r135, %r135, 32;
	setp.lt.s32 	%p24, %r135, %r34;
	add.s32 	%r134, %r134, 32;
	@%p24 bra 	$L__BB1_2;

$L__BB1_16:
	setp.ge.s32 	%p25, %r4, %r33;
	setp.ge.s32 	%p26, %r2, %r32;
	or.pred  	%p27, %p26, %p25;
	@%p27 bra 	$L__BB1_18;

	mad.lo.s32 	%r133, %r2, %r33, %r4;
	cvta.to.global.u64 	%rd56, %rd4;
	mul.wide.s32 	%rd57, %r133, 4;
	add.s64 	%rd58, %rd56, %rd57;
	st.global.f32 	[%rd58], %f53;

$L__BB1_18:
	ret;

}
	// .globl	bnb_nf4_gemm_f32_bias
.visible .entry bnb_nf4_gemm_f32_bias(
	.param .u64 bnb_nf4_gemm_f32_bias_param_0,
	.param .u64 bnb_nf4_gemm_f32_bias_param_1,
	.param .u64 bnb_nf4_gemm_f32_bias_param_2,
	.param .u64 bnb_nf4_gemm_f32_bias_param_3,
	.param .u64 bnb_nf4_gemm_f32_bias_param_4,
	.param .u32 bnb_nf4_gemm_f32_bias_param_5,
	.param .u32 bnb_nf4_gemm_f32_bias_param_6,
	.param .u32 bnb_nf4_gemm_f32_bias_param_7,
	.param .u32 bnb_nf4_gemm_f32_bias_param_8,
	.param .u32 bnb_nf4_gemm_f32_bias_param_9
)
{
	.reg .pred 	%p<31>;
	.reg .b16 	%rs<8>;
	.reg .f32 	%f<59>;
	.reg .b32 	%r<137>;
	.reg .b64 	%rd<63>;
	// demoted variable
	.shared .align 4 .b8 _ZZ21bnb_nf4_gemm_f32_biasE10input_tile[2112];

	ld.param.u64 	%rd4, [bnb_nf4_gemm_f32_bias_param_0];
	ld.param.u64 	%rd5, [bnb_nf4_gemm_f32_bias_param_1];
	ld.param.u64 	%rd7, [bnb_nf4_gemm_f32_bias_param_2];
	ld.param.u64 	%rd8, [bnb_nf4_gemm_f32_bias_param_3];
	ld.param.u64 	%rd6, [bnb_nf4_gemm_f32_bias_param_4];
	ld.param.u32 	%r27, [bnb_nf4_gemm_f32_bias_param_5];
	ld.param.u32 	%r28, [bnb_nf4_gemm_f32_bias_param_6];
	ld.param.u32 	%r29, [bnb_nf4_gemm_f32_bias_param_7];
	ld.param.u32 	%r30, [bnb_nf4_gemm_f32_bias_param_8];
	ld.param.u32 	%r31, [bnb_nf4_gemm_f32_bias_param_9];
	cvta.to.global.u64 	%rd1, %rd8;
	cvta.to.global.u64 	%rd2, %rd7;
	mov.u32 	%r32, %ctaid.y;
	shl.b32 	%r33, %r32, 4;
	mov.u32 	%r1, %tid.y;
	add.s32 	%r2, %r33, %r1;
	mov.u32 	%r34, %ctaid.x;
	shl.b32 	%r35, %r34, 4;
	mov.u32 	%r3, %tid.x;
	add.s32 	%r4, %r35, %r3;
	setp.lt.s32 	%p2, %r29, 1;
	mov.f32 	%f56, 0f00000000;
	@%p2 bra 	$L__BB2_16;

	setp.lt.s32 	%p3, %r2, %r27;
	setp.lt.s32 	%p4, %r4, %r28;
	and.pred  	%p1, %p3, %p4;
	mul.lo.s32 	%r5, %r2, %r29;
	not.b32 	%r6, %r29;
	mul.lo.s32 	%r7, %r4, %r29;
	and.b32  	%r8, %r7, 1;
	cvta.to.global.u64 	%rd3, %rd5;
	mov.u32 	%r131, 0;
	not.pred 	%p13, %p1;
	mov.u32 	%r132, %r131;

$L__BB2_2:
	add.s32 	%r38, %r131, %r6;
	max.s32 	%r39, %r38, -33;
	not.b32 	%r40, %r39;
	max.s32 	%r11, %r40, 1;
	add.s32 	%r41, %r3, %r132;
	setp.ge.s32 	%p5, %r41, %r29;
	setp.gt.s32 	%p6, %r3, 31;
	or.pred  	%p7, %p6, %p5;
	@%p7 bra 	$L__BB2_7;

	add.s32 	%r12, %r132, %r5;
	mov.u32 	%r133, %r3;

$L__BB2_4:
	setp.ge.s32 	%p8, %r2, %r27;
	mov.f32 	%f52, 0f00000000;
	@%p8 bra 	$L__BB2_6;

	add.s32 	%r42, %r12, %r133;
	mul.wide.s32 	%rd9, %r42, 4;
	add.s64 	%rd10, %rd3, %rd9;
	ld.global.nc.f32 	%f52, [%rd10];

$L__BB2_6:
	mov.u32 	%r43, _ZZ21bnb_nf4_gemm_f32_biasE10input_tile;
	mad.lo.s32 	%r44, %r1, 132, %r43;
	shl.b32 	%r45, %r133, 2;
	add.s32 	%r46, %r44, %r45;
	st.shared.f32 	[%r46], %f52;
	add.s32 	%r14, %r133, 16;
	add.s32 	%r47, %r14, %r132;
	setp.lt.s32 	%p9, %r47, %r29;
	setp.lt.s32 	%p10, %r133, 16;
	and.pred  	%p11, %p10, %p9;
	mov.u32 	%r133, %r14;
	@%p11 bra 	$L__BB2_4;

$L__BB2_7:
	bar.sync 	0;
	setp.ge.s32 	%p12, %r132, %r29;
	or.pred  	%p14, %p13, %p12;
	@%p14 bra 	$L__BB2_15;

	add.s32 	%r15, %r132, %r7;
	and.b32  	%r16, %r11, 3;
	add.s32 	%r49, %r11, -1;
	setp.lt.u32 	%p15, %r49, 3;
	mov.u32 	%r136, 0;
	@%p15 bra 	$L__BB2_11;

	sub.s32 	%r135, %r11, %r16;

$L__BB2_10:
	add.s32 	%r51, %r15, %r136;
	shr.u32 	%r52, %r51, 31;
	add.s32 	%r53, %r51, %r52;
	shr.s32 	%r54, %r53, 1;
	cvt.s64.s32 	%rd11, %r54;
	add.s64 	%rd12, %rd2, %rd11;
	ld.global.nc.u8 	%rs1, [%rd12];
	cvt.u32.u16 	%r55, %rs1;
	and.b32  	%r56, %r55, 240;
	shr.u32 	%r57, %r56, 4;
	and.b32  	%r58, %r55, 15;
	setp.ne.s32 	%p16, %r8, 0;
	selp.b32 	%r59, %r57, %r58, %p16;
	div.s32 	%r60, %r51, %r30;
	mul.wide.s32 	%rd13, %r60, 4;
	add.s64 	%rd14, %rd1, %rd13;
	mul.wide.u32 	%rd15, %r59, 4;
	mov.u64 	%rd16, NF4_CODEBOOK;
	add.s64 	%rd17, %rd16, %rd15;
	ld.const.f32 	%f19, [%rd17];
	ld.global.nc.f32 	%f20, [%rd14];
	mul.ftz.f32 	%f21, %f20, %f19;
	mov.u32 	%r61, _ZZ21bnb_nf4_gemm_f32_biasE10input_tile;
	mad.lo.s32 	%r62, %r1, 132, %r61;
	shl.b32 	%r63, %r136, 2;
	add.s32 	%r64, %r62, %r63;
	ld.shared.f32 	%f22, [%r64];
	fma.rn.ftz.f32 	%f23, %f22, %f21, %f56;
	add.s32 	%r65, %r51, 1;
	shr.u32 	%r66, %r65, 31;
	add.s32 	%r67, %r65, %r66;
	shr.s32 	%r68, %r67, 1;
	cvt.s64.s32 	%rd18, %r68;
	add.s64 	%rd19, %rd2, %rd18;
	ld.global.nc.u8 	%rs2, [%rd19];
	cvt.u32.u16 	%r69, %rs2;
	and.b32  	%r70, %r69, 240;
	shr.u32 	%r71, %r70, 4;
	and.b32  	%r72, %r69, 15;
	selp.b32 	%r73, %r72, %r71, %p16;
	div.s32 	%r74, %r65, %r30;
	mul.wide.s32 	%rd20, %r74, 4;
	add.s64 	%rd21, %rd1, %rd20;
	mul.wide.u32 	%rd22, %r73, 4;
	add.s64 	%rd23, %rd16, %rd22;
	ld.const.f32 	%f24, [%rd23];
	ld.global.nc.f32 	%f25, [%rd21];
	mul.ftz.f32 	%f26, %f25, %f24;
	ld.shared.f32 	%f27, [%r64+4];
	fma.rn.ftz.f32 	%f28, %f27, %f26, %f23;
	add.s32 	%r75, %r51, 2;
	shr.u32 	%r76, %r75, 31;
	add.s32 	%r77, %r75, %r76;
	shr.s32 	%r78, %r77, 1;
	cvt.s64.s32 	%rd24, %r78;
	add.s64 	%rd25, %rd2, %rd24;
	ld.global.nc.u8 	%rs3, [%rd25];
	cvt.u32.u16 	%r79, %rs3;
	and.b32  	%r80, %r79, 240;
	shr.u32 	%r81, %r80, 4;
	and.b32  	%r82, %r79, 15;
	selp.b32 	%r83, %r81, %r82, %p16;
	div.s32 	%r84, %r75, %r30;
	mul.wide.s32 	%rd26, %r84, 4;
	add.s64 	%rd27, %rd1, %rd26;
	mul.wide.u32 	%rd28, %r83, 4;
	add.s64 	%rd29, %rd16, %rd28;
	ld.const.f32 	%f29, [%rd29];
	ld.global.nc.f32 	%f30, [%rd27];
	mul.ftz.f32 	%f31, %f30, %f29;
	ld.shared.f32 	%f32, [%r64+8];
	fma.rn.ftz.f32 	%f33, %f32, %f31, %f28;
	add.s32 	%r85, %r51, 3;
	shr.u32 	%r86, %r85, 31;
	add.s32 	%r87, %r85, %r86;
	shr.s32 	%r88, %r87, 1;
	cvt.s64.s32 	%rd30, %r88;
	add.s64 	%rd31, %rd2, %rd30;
	ld.global.nc.u8 	%rs4, [%rd31];
	cvt.u32.u16 	%r89, %rs4;
	and.b32  	%r90, %r89, 240;
	shr.u32 	%r91, %r90, 4;
	and.b32  	%r92, %r89, 15;
	selp.b32 	%r93, %r92, %r91, %p16;
	div.s32 	%r94, %r85, %r30;
	mul.wide.s32 	%rd32, %r94, 4;
	add.s64 	%rd33, %rd1, %rd32;
	mul.wide.u32 	%rd34, %r93, 4;
	add.s64 	%rd35, %rd16, %rd34;
	ld.const.f32 	%f34, [%rd35];
	ld.global.nc.f32 	%f35, [%rd33];
	mul.ftz.f32 	%f36, %f35, %f34;
	ld.shared.f32 	%f37, [%r64+12];
	fma.rn.ftz.f32 	%f56, %f37, %f36, %f33;
	add.s32 	%r136, %r136, 4;
	add.s32 	%r135, %r135, -4;
	setp.ne.s32 	%p17, %r135, 0;
	@%p17 bra 	$L__BB2_10;

$L__BB2_11:
	setp.eq.s32 	%p18, %r16, 0;
	@%p18 bra 	$L__BB2_15;

	add.s32 	%r23, %r15, %r136;
	shr.u32 	%r95, %r23, 31;
	add.s32 	%r96, %r23, %r95;
	shr.s32 	%r97, %r96, 1;
	cvt.s64.s32 	%rd36, %r97;
	add.s64 	%rd37, %rd2, %rd36;
	and.b32  	%r98, %r23, 1;
	setp.eq.b32 	%p19, %r98, 1;
	ld.global.nc.u8 	%rs5, [%rd37];
	cvt.u32.u16 	%r99, %rs5;
	and.b32  	%r100, %r99, 240;
	shr.u32 	%r101, %r100, 4;
	and.b32  	%r102, %r99, 15;
	selp.b32 	%r103, %r101, %r102, %p19;
	div.s32 	%r104, %r23, %r30;
	mul.wide.s32 	%rd38, %r104, 4;
	add.s64 	%rd39, %rd1, %rd38;
	mul.wide.u32 	%rd40, %r103, 4;
	mov.u64 	%rd41, NF4_CODEBOOK;
	add.s64 	%rd42, %rd41, %rd40;
	ld.const.f32 	%f38, [%rd42];
	ld.global.nc.f32 	%f39, [%rd39];
	mul.ftz.f32 	%f40, %f39, %f38;
	mov.u32 	%r105, _ZZ21bnb_nf4_gemm_f32_biasE10input_tile;
	mad.lo.s32 	%r106, %r1, 132, %r105;
	shl.b32 	%r107, %r136, 2;
	add.s32 	%r24, %r106, %r107;
	ld.shared.f32 	%f41, [%r24];
	fma.rn.ftz.f32 	%f56, %f41, %f40, %f56;
	setp.eq.s32 	%p20, %r16, 1;
	@%p20 bra 	$L__BB2_15;

	add.s32 	%r108, %r23, 1;
	shr.u32 	%r109, %r108, 31;
	add.s32 	%r110, %r108, %r109;
	shr.s32 	%r111, %r110, 1;
	cvt.s64.s32 	%rd43, %r111;
	add.s64 	%rd44, %rd2, %rd43;
	and.b32  	%r112, %r108, 1;
	setp.eq.b32 	%p21, %r112, 1;
	ld.global.nc.u8 	%rs6, [%rd44];
	cvt.u32.u16 	%r113, %rs6;
	and.b32  	%r114, %r113, 240;
	shr.u32 	%r115, %r114, 4;
	and.b32  	%r116, %r113, 15;
	selp.b32 	%r117, %r115, %r116, %p21;
	div.s32 	%r118, %r108, %r30;
	mul.wide.s32 	%rd45, %r118, 4;
	add.s64 	%rd46, %rd1, %rd45;
	mul.wide.u32 	%rd47, %r117, 4;
	add.s64 	%rd49, %rd41, %rd47;
	ld.const.f32 	%f42, [%rd49];
	ld.global.nc.f32 	%f43, [%rd46];
	mul.ftz.f32 	%f44, %f43, %f42;
	ld.shared.f32 	%f45, [%r24+4];
	fma.rn.ftz.f32 	%f56, %f45, %f44, %f56;
	setp.eq.s32 	%p22, %r16, 2;
	@%p22 bra 	$L__BB2_15;

	add.s32 	%r119, %r23, 2;
	shr.u32 	%r120, %r119, 31;
	add.s32 	%r121, %r119, %r120;
	shr.s32 	%r122, %r121, 1;
	cvt.s64.s32 	%rd50, %r122;
	add.s64 	%rd51, %rd2, %rd50;
	and.b32  	%r123, %r119, 1;
	setp.eq.b32 	%p23, %r123, 1;
	ld.global.nc.u8 	%rs7, [%rd51];
	cvt.u32.u16 	%r124, %rs7;
	and.b32  	%r125, %r124, 240;
	shr.u32 	%r126, %r125, 4;
	and.b32  	%r127, %r124, 15;
	selp.b32 	%r128, %r126, %r127, %p23;
	div.s32 	%r129, %r119, %r30;
	mul.wide.s32 	%rd52, %r129, 4;
	add.s64 	%rd53, %rd1, %rd52;
	mul.wide.u32 	%rd54, %r128, 4;
	add.s64 	%rd56, %rd41, %rd54;
	ld.const.f32 	%f46, [%rd56];
	ld.global.nc.f32 	%f47, [%rd53];
	mul.ftz.f32 	%f48, %f47, %f46;
	ld.shared.f32 	%f49, [%r24+8];
	fma.rn.ftz.f32 	%f56, %f49, %f48, %f56;

$L__BB2_15:
	bar.sync 	0;
	add.s32 	%r132, %r132, 32;
	setp.lt.s32 	%p24, %r132, %r29;
	add.s32 	%r131, %r131, 32;
	@%p24 bra 	$L__BB2_2;

$L__BB2_16:
	setp.ge.s32 	%p25, %r4, %r28;
	setp.ge.s32 	%p26, %r2, %r27;
	or.pred  	%p27, %p26, %p25;
	@%p27 bra 	$L__BB2_20;

	setp.eq.s32 	%p28, %r31, 0;
	setp.eq.s64 	%p29, %rd6, 0;
	or.pred  	%p30, %p29, %p28;
	@%p30 bra 	$L__BB2_19;

	cvta.to.global.u64 	%rd57, %rd6;
	mul.wide.s32 	%rd58, %r4, 4;
	add.s64 	%rd59, %rd57, %rd58;
	ld.global.nc.f32 	%f50, [%rd59];
	add.ftz.f32 	%f56, %f56, %f50;

$L__BB2_19:
	mad.lo.s32 	%r130, %r2, %r28, %r4;
	cvta.to.global.u64 	%rd60, %rd4;
	mul.wide.s32 	%rd61, %r130, 4;
	add.s64 	%rd62, %rd60, %rd61;
	st.global.f32 	[%rd62], %f56;

$L__BB2_20:
	ret;

}
	// .globl	bnb_int8_gemm_f32_naive
.visible .entry bnb_int8_gemm_f32_naive(
	.param .u64 bnb_int8_gemm_f32_naive_param_0,
	.param .u64 bnb_int8_gemm_f32_naive_param_1,
	.param .u64 bnb_int8_gemm_f32_naive_param_2,
	.param .u64 bnb_int8_gemm_f32_naive_param_3,
	.param .u32 bnb_int8_gemm_f32_naive_param_4,
	.param .u32 bnb_int8_gemm_f32_naive_param_5,
	.param .u32 bnb_int8_gemm_f32_naive_param_6,
	.param .u32 bnb_int8_gemm_f32_naive_param_7
)
{
	.reg .pred 	%p<9>;
	.reg .b16 	%rs<11>;
	.reg .f32 	%f<47>;
	.reg .b32 	%r<47>;
	.reg .b64 	%rd<43>;


	ld.param.u64 	%rd16, [bnb_int8_gemm_f32_naive_param_0];
	ld.param.u64 	%rd17, [bnb_int8_gemm_f32_naive_param_1];
	ld.param.u64 	%rd18, [bnb_int8_gemm_f32_naive_param_2];
	ld.param.u64 	%rd19, [bnb_int8_gemm_f32_naive_param_3];
	ld.param.u32 	%r20, [bnb_int8_gemm_f32_naive_param_4];
	ld.param.u32 	%r17, [bnb_int8_gemm_f32_naive_param_5];
	ld.param.u32 	%r18, [bnb_int8_gemm_f32_naive_param_6];
	ld.param.u32 	%r19, [bnb_int8_gemm_f32_naive_param_7];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	cvta.to.global.u64 	%rd3, %rd17;
	mov.u32 	%r21, %ntid.y;
	mov.u32 	%r22, %ctaid.y;
	mov.u32 	%r23, %tid.y;
	mad.lo.s32 	%r1, %r22, %r21, %r23;
	mov.u32 	%r24, %ntid.x;
	mov.u32 	%r25, %ctaid.x;
	mov.u32 	%r26, %tid.x;
	mad.lo.s32 	%r2, %r25, %r24, %r26;
	setp.ge.s32 	%p1, %r1, %r20;
	setp.ge.s32 	%p2, %r2, %r17;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB3_9;

	setp.lt.s32 	%p4, %r18, 1;
	mov.f32 	%f46, 0f00000000;
	@%p4 bra 	$L__BB3_8;

	add.s32 	%r28, %r18, -1;
	and.b32  	%r46, %r18, 3;
	setp.lt.u32 	%p5, %r28, 3;
	mov.f32 	%f46, 0f00000000;
	mov.u32 	%r44, 0;
	@%p5 bra 	$L__BB3_5;

	mul.lo.s32 	%r4, %r18, %r2;
	add.s32 	%r42, %r4, 3;
	sub.s32 	%r6, %r46, %r18;
	cvt.s64.s32 	%rd20, %r4;
	add.s64 	%rd21, %rd2, %rd20;
	add.s64 	%rd40, %rd21, 1;
	mul.lo.s32 	%r30, %r18, %r1;
	mul.wide.s32 	%rd22, %r30, 4;
	add.s64 	%rd23, %rd3, %rd22;
	add.s64 	%rd39, %rd23, 8;

$L__BB3_4:
	add.s32 	%r31, %r4, %r44;
	div.s32 	%r32, %r31, %r19;
	mul.wide.s32 	%rd24, %r32, 4;
	add.s64 	%rd25, %rd1, %rd24;
	ld.global.nc.f32 	%f12, [%rd25];
	mov.f32 	%f13, 0f42FE0000;
	div.approx.ftz.f32 	%f14, %f12, %f13;
	ld.global.nc.u8 	%rs1, [%rd40+-1];
	cvt.s16.s8 	%rs2, %rs1;
	cvt.rn.f32.s16 	%f15, %rs2;
	mul.ftz.f32 	%f16, %f14, %f15;
	ld.global.nc.f32 	%f17, [%rd39+-8];
	fma.rn.ftz.f32 	%f18, %f17, %f16, %f46;
	add.s32 	%r33, %r42, -2;
	div.s32 	%r34, %r33, %r19;
	mul.wide.s32 	%rd26, %r34, 4;
	add.s64 	%rd27, %rd1, %rd26;
	ld.global.nc.f32 	%f19, [%rd27];
	div.approx.ftz.f32 	%f20, %f19, %f13;
	ld.global.nc.u8 	%rs3, [%rd40];
	cvt.s16.s8 	%rs4, %rs3;
	cvt.rn.f32.s16 	%f21, %rs4;
	mul.ftz.f32 	%f22, %f20, %f21;
	ld.global.nc.f32 	%f23, [%rd39+-4];
	fma.rn.ftz.f32 	%f24, %f23, %f22, %f18;
	add.s32 	%r35, %r42, -1;
	div.s32 	%r36, %r35, %r19;
	mul.wide.s32 	%rd28, %r36, 4;
	add.s64 	%rd29, %rd1, %rd28;
	ld.global.nc.f32 	%f25, [%rd29];
	div.approx.ftz.f32 	%f26, %f25, %f13;
	ld.global.nc.u8 	%rs5, [%rd40+1];
	cvt.s16.s8 	%rs6, %rs5;
	cvt.rn.f32.s16 	%f27, %rs6;
	mul.ftz.f32 	%f28, %f26, %f27;
	ld.global.nc.f32 	%f29, [%rd39];
	fma.rn.ftz.f32 	%f30, %f29, %f28, %f24;
	div.s32 	%r37, %r42, %r19;
	mul.wide.s32 	%rd30, %r37, 4;
	add.s64 	%rd31, %rd1, %rd30;
	ld.global.nc.f32 	%f31, [%rd31];
	div.approx.ftz.f32 	%f32, %f31, %f13;
	ld.global.nc.u8 	%rs7, [%rd40+2];
	cvt.s16.s8 	%rs8, %rs7;
	cvt.rn.f32.s16 	%f33, %rs8;
	mul.ftz.f32 	%f34, %f32, %f33;
	ld.global.nc.f32 	%f35, [%rd39+4];
	fma.rn.ftz.f32 	%f46, %f35, %f34, %f30;
	add.s32 	%r42, %r42, 4;
	add.s32 	%r44, %r44, 4;
	add.s32 	%r38, %r6, %r44;
	add.s64 	%rd40, %rd40, 4;
	add.s64 	%rd39, %rd39, 16;
	setp.ne.s32 	%p6, %r38, 0;
	@%p6 bra 	$L__BB3_4;

$L__BB3_5:
	setp.eq.s32 	%p7, %r46, 0;
	@%p7 bra 	$L__BB3_8;

	mad.lo.s32 	%r45, %r18, %r2, %r44;
	cvt.s64.s32 	%rd32, %r45;
	add.s64 	%rd42, %rd2, %rd32;
	mad.lo.s32 	%r39, %r18, %r1, %r44;
	mul.wide.s32 	%rd33, %r39, 4;
	add.s64 	%rd41, %rd3, %rd33;

$L__BB3_7:
	.pragma "nounroll";
	div.s32 	%r40, %r45, %r19;
	mul.wide.s32 	%rd34, %r40, 4;
	add.s64 	%rd35, %rd1, %rd34;
	ld.global.nc.f32 	%f36, [%rd35];
	mov.f32 	%f37, 0f42FE0000;
	div.approx.ftz.f32 	%f38, %f36, %f37;
	ld.global.nc.u8 	%rs9, [%rd42];
	cvt.s16.s8 	%rs10, %rs9;
	cvt.rn.f32.s16 	%f39, %rs10;
	mul.ftz.f32 	%f40, %f38, %f39;
	ld.global.nc.f32 	%f41, [%rd41];
	fma.rn.ftz.f32 	%f46, %f41, %f40, %f46;
	add.s32 	%r45, %r45, 1;
	add.s64 	%rd42, %rd42, 1;
	add.s64 	%rd41, %rd41, 4;
	add.s32 	%r46, %r46, -1;
	setp.ne.s32 	%p8, %r46, 0;
	@%p8 bra 	$L__BB3_7;

$L__BB3_8:
	mad.lo.s32 	%r41, %r1, %r17, %r2;
	cvta.to.global.u64 	%rd36, %rd16;
	mul.wide.s32 	%rd37, %r41, 4;
	add.s64 	%rd38, %rd36, %rd37;
	st.global.f32 	[%rd38], %f46;

$L__BB3_9:
	ret;

}
	// .globl	bnb_int8_gemm_f32_tiled
.visible .entry bnb_int8_gemm_f32_tiled(
	.param .u64 bnb_int8_gemm_f32_tiled_param_0,
	.param .u64 bnb_int8_gemm_f32_tiled_param_1,
	.param .u64 bnb_int8_gemm_f32_tiled_param_2,
	.param .u64 bnb_int8_gemm_f32_tiled_param_3,
	.param .u32 bnb_int8_gemm_f32_tiled_param_4,
	.param .u32 bnb_int8_gemm_f32_tiled_param_5,
	.param .u32 bnb_int8_gemm_f32_tiled_param_6,
	.param .u32 bnb_int8_gemm_f32_tiled_param_7
)
{
	.reg .pred 	%p<24>;
	.reg .b16 	%rs<15>;
	.reg .f32 	%f<66>;
	.reg .b32 	%r<81>;
	.reg .b64 	%rd<35>;
	// demoted variable
	.shared .align 4 .b8 _ZZ23bnb_int8_gemm_f32_tiledE10input_tile[2112];

	ld.param.u64 	%rd9, [bnb_int8_gemm_f32_tiled_param_0];
	ld.param.u64 	%rd10, [bnb_int8_gemm_f32_tiled_param_1];
	ld.param.u64 	%rd11, [bnb_int8_gemm_f32_tiled_param_2];
	ld.param.u64 	%rd12, [bnb_int8_gemm_f32_tiled_param_3];
	ld.param.u32 	%r32, [bnb_int8_gemm_f32_tiled_param_4];
	ld.param.u32 	%r33, [bnb_int8_gemm_f32_tiled_param_5];
	ld.param.u32 	%r34, [bnb_int8_gemm_f32_tiled_param_6];
	ld.param.u32 	%r35, [bnb_int8_gemm_f32_tiled_param_7];
	cvta.to.global.u64 	%rd1, %rd12;
	cvta.to.global.u64 	%rd2, %rd11;
	mov.u32 	%r36, %ctaid.y;
	shl.b32 	%r37, %r36, 4;
	mov.u32 	%r1, %tid.y;
	add.s32 	%r2, %r37, %r1;
	mov.u32 	%r38, %ctaid.x;
	shl.b32 	%r39, %r38, 4;
	mov.u32 	%r3, %tid.x;
	add.s32 	%r4, %r39, %r3;
	setp.lt.s32 	%p2, %r34, 1;
	mov.f32 	%f64, 0f00000000;
	@%p2 bra 	$L__BB4_16;

	setp.lt.s32 	%p3, %r2, %r32;
	setp.lt.s32 	%p4, %r4, %r33;
	and.pred  	%p1, %p3, %p4;
	mul.lo.s32 	%r5, %r2, %r34;
	not.b32 	%r6, %r34;
	mul.lo.s32 	%r7, %r4, %r34;
	add.s32 	%r8, %r7, 3;
	mov.u32 	%r42, _ZZ23bnb_int8_gemm_f32_tiledE10input_tile;
	mad.lo.s32 	%r43, %r1, 132, %r42;
	add.s32 	%r9, %r43, 8;
	add.s64 	%rd3, %rd2, 1;
	cvta.to.global.u64 	%rd4, %rd10;
	mov.u32 	%r74, 0;
	not.pred 	%p13, %p1;
	mov.u32 	%r75, %r74;

$L__BB4_2:
	add.s32 	%r44, %r74, %r6;
	max.s32 	%r45, %r44, -33;
	not.b32 	%r46, %r45;
	max.s32 	%r12, %r46, 1;
	add.s32 	%r47, %r3, %r75;
	setp.ge.s32 	%p5, %r47, %r34;
	setp.gt.s32 	%p6, %r3, 31;
	or.pred  	%p7, %p6, %p5;
	@%p7 bra 	$L__BB4_7;

	add.s32 	%r13, %r75, %r5;
	mov.u32 	%r76, %r3;

$L__BB4_4:
	setp.ge.s32 	%p8, %r2, %r32;
	mov.f32 	%f60, 0f00000000;
	@%p8 bra 	$L__BB4_6;

	add.s32 	%r48, %r13, %r76;
	mul.wide.s32 	%rd13, %r48, 4;
	add.s64 	%rd14, %rd4, %rd13;
	ld.global.nc.f32 	%f60, [%rd14];

$L__BB4_6:
	shl.b32 	%r51, %r76, 2;
	add.s32 	%r52, %r43, %r51;
	st.shared.f32 	[%r52], %f60;
	add.s32 	%r15, %r76, 16;
	add.s32 	%r53, %r15, %r75;
	setp.lt.s32 	%p9, %r53, %r34;
	setp.lt.s32 	%p10, %r76, 16;
	and.pred  	%p11, %p10, %p9;
	mov.u32 	%r76, %r15;
	@%p11 bra 	$L__BB4_4;

$L__BB4_7:
	bar.sync 	0;
	setp.ge.s32 	%p12, %r75, %r34;
	or.pred  	%p14, %p13, %p12;
	@%p14 bra 	$L__BB4_15;

	add.s32 	%r16, %r75, %r7;
	and.b32  	%r17, %r12, 3;
	add.s32 	%r55, %r12, -1;
	setp.lt.u32 	%p15, %r55, 3;
	mov.u32 	%r80, 0;
	@%p15 bra 	$L__BB4_11;

	sub.s32 	%r19, %r17, %r12;
	add.s32 	%r78, %r8, %r75;
	cvt.s64.s32 	%rd15, %r16;
	add.s64 	%rd34, %rd3, %rd15;
	mov.u32 	%r77, %r9;

$L__BB4_10:
	add.s32 	%r57, %r16, %r80;
	div.s32 	%r58, %r57, %r35;
	mul.wide.s32 	%rd16, %r58, 4;
	add.s64 	%rd17, %rd1, %rd16;
	ld.global.nc.f32 	%f17, [%rd17];
	mov.f32 	%f18, 0f42FE0000;
	div.approx.ftz.f32 	%f19, %f17, %f18;
	ld.global.nc.u8 	%rs1, [%rd34+-1];
	cvt.s16.s8 	%rs2, %rs1;
	cvt.rn.f32.s16 	%f20, %rs2;
	mul.ftz.f32 	%f21, %f19, %f20;
	ld.shared.f32 	%f22, [%r77+-8];
	fma.rn.ftz.f32 	%f23, %f22, %f21, %f64;
	add.s32 	%r59, %r78, -2;
	div.s32 	%r60, %r59, %r35;
	mul.wide.s32 	%rd18, %r60, 4;
	add.s64 	%rd19, %rd1, %rd18;
	ld.global.nc.f32 	%f24, [%rd19];
	div.approx.ftz.f32 	%f25, %f24, %f18;
	ld.global.nc.u8 	%rs3, [%rd34];
	cvt.s16.s8 	%rs4, %rs3;
	cvt.rn.f32.s16 	%f26, %rs4;
	mul.ftz.f32 	%f27, %f25, %f26;
	ld.shared.f32 	%f28, [%r77+-4];
	fma.rn.ftz.f32 	%f29, %f28, %f27, %f23;
	add.s32 	%r61, %r78, -1;
	div.s32 	%r62, %r61, %r35;
	mul.wide.s32 	%rd20, %r62, 4;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.nc.f32 	%f30, [%rd21];
	div.approx.ftz.f32 	%f31, %f30, %f18;
	ld.global.nc.u8 	%rs5, [%rd34+1];
	cvt.s16.s8 	%rs6, %rs5;
	cvt.rn.f32.s16 	%f32, %rs6;
	mul.ftz.f32 	%f33, %f31, %f32;
	ld.shared.f32 	%f34, [%r77];
	fma.rn.ftz.f32 	%f35, %f34, %f33, %f29;
	div.s32 	%r63, %r78, %r35;
	mul.wide.s32 	%rd22, %r63, 4;
	add.s64 	%rd23, %rd1, %rd22;
	ld.global.nc.f32 	%f36, [%rd23];
	div.approx.ftz.f32 	%f37, %f36, %f18;
	ld.global.nc.u8 	%rs7, [%rd34+2];
	cvt.s16.s8 	%rs8, %rs7;
	cvt.rn.f32.s16 	%f38, %rs8;
	mul.ftz.f32 	%f39, %f37, %f38;
	ld.shared.f32 	%f40, [%r77+4];
	fma.rn.ftz.f32 	%f64, %f40, %f39, %f35;
	add.s32 	%r80, %r80, 4;
	add.s32 	%r64, %r19, %r80;
	add.s32 	%r78, %r78, 4;
	add.s32 	%r77, %r77, 16;
	add.s64 	%rd34, %rd34, 4;
	setp.ne.s32 	%p16, %r64, 0;
	@%p16 bra 	$L__BB4_10;

$L__BB4_11:
	setp.eq.s32 	%p17, %r17, 0;
	@%p17 bra 	$L__BB4_15;

	add.s32 	%r28, %r16, %r80;
	cvt.s64.s32 	%rd24, %r28;
	add.s64 	%rd8, %rd2, %rd24;
	div.s32 	%r65, %r28, %r35;
	mul.wide.s32 	%rd25, %r65, 4;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.nc.f32 	%f41, [%rd26];
	mov.f32 	%f42, 0f42FE0000;
	div.approx.ftz.f32 	%f43, %f41, %f42;
	ld.global.nc.u8 	%rs9, [%rd8];
	cvt.s16.s8 	%rs10, %rs9;
	cvt.rn.f32.s16 	%f44, %rs10;
	mul.ftz.f32 	%f45, %f43, %f44;
	shl.b32 	%r68, %r80, 2;
	add.s32 	%r29, %r43, %r68;
	ld.shared.f32 	%f46, [%r29];
	fma.rn.ftz.f32 	%f64, %f46, %f45, %f64;
	setp.eq.s32 	%p18, %r17, 1;
	@%p18 bra 	$L__BB4_15;

	add.s32 	%r69, %r28, 1;
	div.s32 	%r70, %r69, %r35;
	mul.wide.s32 	%rd27, %r70, 4;
	add.s64 	%rd28, %rd1, %rd27;
	ld.global.nc.f32 	%f47, [%rd28];
	div.approx.ftz.f32 	%f49, %f47, %f42;
	ld.global.nc.u8 	%rs11, [%rd8+1];
	cvt.s16.s8 	%rs12, %rs11;
	cvt.rn.f32.s16 	%f50, %rs12;
	mul.ftz.f32 	%f51, %f49, %f50;
	ld.shared.f32 	%f52, [%r29+4];
	fma.rn.ftz.f32 	%f64, %f52, %f51, %f64;
	setp.eq.s32 	%p19, %r17, 2;
	@%p19 bra 	$L__BB4_15;

	add.s32 	%r71, %r28, 2;
	div.s32 	%r72, %r71, %r35;
	mul.wide.s32 	%rd29, %r72, 4;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.nc.f32 	%f53, [%rd30];
	mov.f32 	%f54, 0f42FE0000;
	div.approx.ftz.f32 	%f55, %f53, %f54;
	ld.global.nc.u8 	%rs13, [%rd8+2];
	cvt.s16.s8 	%rs14, %rs13;
	cvt.rn.f32.s16 	%f56, %rs14;
	mul.ftz.f32 	%f57, %f55, %f56;
	ld.shared.f32 	%f58, [%r29+8];
	fma.rn.ftz.f32 	%f64, %f58, %f57, %f64;

$L__BB4_15:
	bar.sync 	0;
	add.s32 	%r75, %r75, 32;
	setp.lt.s32 	%p20, %r75, %r34;
	add.s32 	%r74, %r74, 32;
	@%p20 bra 	$L__BB4_2;

$L__BB4_16:
	setp.ge.s32 	%p21, %r4, %r33;
	setp.ge.s32 	%p22, %r2, %r32;
	or.pred  	%p23, %p22, %p21;
	@%p23 bra 	$L__BB4_18;

	mad.lo.s32 	%r73, %r2, %r33, %r4;
	cvta.to.global.u64 	%rd31, %rd9;
	mul.wide.s32 	%rd32, %r73, 4;
	add.s64 	%rd33, %rd31, %rd32;
	st.global.f32 	[%rd33], %f64;

$L__BB4_18:
	ret;

}
	// .globl	bnb_int8_gemm_f32_bias
.visible .entry bnb_int8_gemm_f32_bias(
	.param .u64 bnb_int8_gemm_f32_bias_param_0,
	.param .u64 bnb_int8_gemm_f32_bias_param_1,
	.param .u64 bnb_int8_gemm_f32_bias_param_2,
	.param .u64 bnb_int8_gemm_f32_bias_param_3,
	.param .u64 bnb_int8_gemm_f32_bias_param_4,
	.param .u32 bnb_int8_gemm_f32_bias_param_5,
	.param .u32 bnb_int8_gemm_f32_bias_param_6,
	.param .u32 bnb_int8_gemm_f32_bias_param_7,
	.param .u32 bnb_int8_gemm_f32_bias_param_8,
	.param .u32 bnb_int8_gemm_f32_bias_param_9
)
{
	.reg .pred 	%p<27>;
	.reg .b16 	%rs<15>;
	.reg .f32 	%f<70>;
	.reg .b32 	%r<82>;
	.reg .b64 	%rd<39>;
	// demoted variable
	.shared .align 4 .b8 _ZZ22bnb_int8_gemm_f32_biasE10input_tile[2112];

	ld.param.u64 	%rd9, [bnb_int8_gemm_f32_bias_param_0];
	ld.param.u64 	%rd10, [bnb_int8_gemm_f32_bias_param_1];
	ld.param.u64 	%rd12, [bnb_int8_gemm_f32_bias_param_2];
	ld.param.u64 	%rd13, [bnb_int8_gemm_f32_bias_param_3];
	ld.param.u64 	%rd11, [bnb_int8_gemm_f32_bias_param_4];
	ld.param.u32 	%r32, [bnb_int8_gemm_f32_bias_param_5];
	ld.param.u32 	%r33, [bnb_int8_gemm_f32_bias_param_6];
	ld.param.u32 	%r34, [bnb_int8_gemm_f32_bias_param_7];
	ld.param.u32 	%r35, [bnb_int8_gemm_f32_bias_param_8];
	ld.param.u32 	%r36, [bnb_int8_gemm_f32_bias_param_9];
	cvta.to.global.u64 	%rd1, %rd13;
	cvta.to.global.u64 	%rd2, %rd12;
	mov.u32 	%r37, %ctaid.y;
	shl.b32 	%r38, %r37, 4;
	mov.u32 	%r1, %tid.y;
	add.s32 	%r2, %r38, %r1;
	mov.u32 	%r39, %ctaid.x;
	shl.b32 	%r40, %r39, 4;
	mov.u32 	%r3, %tid.x;
	add.s32 	%r4, %r40, %r3;
	setp.lt.s32 	%p2, %r34, 1;
	mov.f32 	%f67, 0f00000000;
	@%p2 bra 	$L__BB5_16;

	setp.lt.s32 	%p3, %r2, %r32;
	setp.lt.s32 	%p4, %r4, %r33;
	and.pred  	%p1, %p3, %p4;
	mul.lo.s32 	%r5, %r2, %r34;
	not.b32 	%r6, %r34;
	mul.lo.s32 	%r7, %r4, %r34;
	add.s32 	%r8, %r7, 3;
	mov.u32 	%r43, _ZZ22bnb_int8_gemm_f32_biasE10input_tile;
	mad.lo.s32 	%r44, %r1, 132, %r43;
	add.s32 	%r9, %r44, 8;
	add.s64 	%rd3, %rd2, 1;
	cvta.to.global.u64 	%rd4, %rd10;
	mov.u32 	%r75, 0;
	not.pred 	%p13, %p1;
	mov.u32 	%r76, %r75;

$L__BB5_2:
	add.s32 	%r45, %r75, %r6;
	max.s32 	%r46, %r45, -33;
	not.b32 	%r47, %r46;
	max.s32 	%r12, %r47, 1;
	add.s32 	%r48, %r3, %r76;
	setp.ge.s32 	%p5, %r48, %r34;
	setp.gt.s32 	%p6, %r3, 31;
	or.pred  	%p7, %p6, %p5;
	@%p7 bra 	$L__BB5_7;

	add.s32 	%r13, %r76, %r5;
	mov.u32 	%r77, %r3;

$L__BB5_4:
	setp.ge.s32 	%p8, %r2, %r32;
	mov.f32 	%f63, 0f00000000;
	@%p8 bra 	$L__BB5_6;

	add.s32 	%r49, %r13, %r77;
	mul.wide.s32 	%rd14, %r49, 4;
	add.s64 	%rd15, %rd4, %rd14;
	ld.global.nc.f32 	%f63, [%rd15];

$L__BB5_6:
	shl.b32 	%r52, %r77, 2;
	add.s32 	%r53, %r44, %r52;
	st.shared.f32 	[%r53], %f63;
	add.s32 	%r15, %r77, 16;
	add.s32 	%r54, %r15, %r76;
	setp.lt.s32 	%p9, %r54, %r34;
	setp.lt.s32 	%p10, %r77, 16;
	and.pred  	%p11, %p10, %p9;
	mov.u32 	%r77, %r15;
	@%p11 bra 	$L__BB5_4;

$L__BB5_7:
	bar.sync 	0;
	setp.ge.s32 	%p12, %r76, %r34;
	or.pred  	%p14, %p13, %p12;
	@%p14 bra 	$L__BB5_15;

	add.s32 	%r16, %r76, %r7;
	and.b32  	%r17, %r12, 3;
	add.s32 	%r56, %r12, -1;
	setp.lt.u32 	%p15, %r56, 3;
	mov.u32 	%r81, 0;
	@%p15 bra 	$L__BB5_11;

	sub.s32 	%r19, %r17, %r12;
	add.s32 	%r79, %r8, %r76;
	cvt.s64.s32 	%rd16, %r16;
	add.s64 	%rd38, %rd3, %rd16;
	mov.u32 	%r78, %r9;

$L__BB5_10:
	add.s32 	%r58, %r16, %r81;
	div.s32 	%r59, %r58, %r35;
	mul.wide.s32 	%rd17, %r59, 4;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.nc.f32 	%f19, [%rd18];
	mov.f32 	%f20, 0f42FE0000;
	div.approx.ftz.f32 	%f21, %f19, %f20;
	ld.global.nc.u8 	%rs1, [%rd38+-1];
	cvt.s16.s8 	%rs2, %rs1;
	cvt.rn.f32.s16 	%f22, %rs2;
	mul.ftz.f32 	%f23, %f21, %f22;
	ld.shared.f32 	%f24, [%r78+-8];
	fma.rn.ftz.f32 	%f25, %f24, %f23, %f67;
	add.s32 	%r60, %r79, -2;
	div.s32 	%r61, %r60, %r35;
	mul.wide.s32 	%rd19, %r61, 4;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.nc.f32 	%f26, [%rd20];
	div.approx.ftz.f32 	%f27, %f26, %f20;
	ld.global.nc.u8 	%rs3, [%rd38];
	cvt.s16.s8 	%rs4, %rs3;
	cvt.rn.f32.s16 	%f28, %rs4;
	mul.ftz.f32 	%f29, %f27, %f28;
	ld.shared.f32 	%f30, [%r78+-4];
	fma.rn.ftz.f32 	%f31, %f30, %f29, %f25;
	add.s32 	%r62, %r79, -1;
	div.s32 	%r63, %r62, %r35;
	mul.wide.s32 	%rd21, %r63, 4;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.nc.f32 	%f32, [%rd22];
	div.approx.ftz.f32 	%f33, %f32, %f20;
	ld.global.nc.u8 	%rs5, [%rd38+1];
	cvt.s16.s8 	%rs6, %rs5;
	cvt.rn.f32.s16 	%f34, %rs6;
	mul.ftz.f32 	%f35, %f33, %f34;
	ld.shared.f32 	%f36, [%r78];
	fma.rn.ftz.f32 	%f37, %f36, %f35, %f31;
	div.s32 	%r64, %r79, %r35;
	mul.wide.s32 	%rd23, %r64, 4;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.nc.f32 	%f38, [%rd24];
	div.approx.ftz.f32 	%f39, %f38, %f20;
	ld.global.nc.u8 	%rs7, [%rd38+2];
	cvt.s16.s8 	%rs8, %rs7;
	cvt.rn.f32.s16 	%f40, %rs8;
	mul.ftz.f32 	%f41, %f39, %f40;
	ld.shared.f32 	%f42, [%r78+4];
	fma.rn.ftz.f32 	%f67, %f42, %f41, %f37;
	add.s32 	%r81, %r81, 4;
	add.s32 	%r65, %r19, %r81;
	add.s32 	%r79, %r79, 4;
	add.s32 	%r78, %r78, 16;
	add.s64 	%rd38, %rd38, 4;
	setp.ne.s32 	%p16, %r65, 0;
	@%p16 bra 	$L__BB5_10;

$L__BB5_11:
	setp.eq.s32 	%p17, %r17, 0;
	@%p17 bra 	$L__BB5_15;

	add.s32 	%r28, %r16, %r81;
	cvt.s64.s32 	%rd25, %r28;
	add.s64 	%rd8, %rd2, %rd25;
	div.s32 	%r66, %r28, %r35;
	mul.wide.s32 	%rd26, %r66, 4;
	add.s64 	%rd27, %rd1, %rd26;
	ld.global.nc.f32 	%f43, [%rd27];
	mov.f32 	%f44, 0f42FE0000;
	div.approx.ftz.f32 	%f45, %f43, %f44;
	ld.global.nc.u8 	%rs9, [%rd8];
	cvt.s16.s8 	%rs10, %rs9;
	cvt.rn.f32.s16 	%f46, %rs10;
	mul.ftz.f32 	%f47, %f45, %f46;
	shl.b32 	%r69, %r81, 2;
	add.s32 	%r29, %r44, %r69;
	ld.shared.f32 	%f48, [%r29];
	fma.rn.ftz.f32 	%f67, %f48, %f47, %f67;
	setp.eq.s32 	%p18, %r17, 1;
	@%p18 bra 	$L__BB5_15;

	add.s32 	%r70, %r28, 1;
	div.s32 	%r71, %r70, %r35;
	mul.wide.s32 	%rd28, %r71, 4;
	add.s64 	%rd29, %rd1, %rd28;
	ld.global.nc.f32 	%f49, [%rd29];
	div.approx.ftz.f32 	%f51, %f49, %f44;
	ld.global.nc.u8 	%rs11, [%rd8+1];
	cvt.s16.s8 	%rs12, %rs11;
	cvt.rn.f32.s16 	%f52, %rs12;
	mul.ftz.f32 	%f53, %f51, %f52;
	ld.shared.f32 	%f54, [%r29+4];
	fma.rn.ftz.f32 	%f67, %f54, %f53, %f67;
	setp.eq.s32 	%p19, %r17, 2;
	@%p19 bra 	$L__BB5_15;

	add.s32 	%r72, %r28, 2;
	div.s32 	%r73, %r72, %r35;
	mul.wide.s32 	%rd30, %r73, 4;
	add.s64 	%rd31, %rd1, %rd30;
	ld.global.nc.f32 	%f55, [%rd31];
	mov.f32 	%f56, 0f42FE0000;
	div.approx.ftz.f32 	%f57, %f55, %f56;
	ld.global.nc.u8 	%rs13, [%rd8+2];
	cvt.s16.s8 	%rs14, %rs13;
	cvt.rn.f32.s16 	%f58, %rs14;
	mul.ftz.f32 	%f59, %f57, %f58;
	ld.shared.f32 	%f60, [%r29+8];
	fma.rn.ftz.f32 	%f67, %f60, %f59, %f67;

$L__BB5_15:
	bar.sync 	0;
	add.s32 	%r76, %r76, 32;
	setp.lt.s32 	%p20, %r76, %r34;
	add.s32 	%r75, %r75, 32;
	@%p20 bra 	$L__BB5_2;

$L__BB5_16:
	setp.ge.s32 	%p21, %r4, %r33;
	setp.ge.s32 	%p22, %r2, %r32;
	or.pred  	%p23, %p22, %p21;
	@%p23 bra 	$L__BB5_20;

	setp.eq.s32 	%p24, %r36, 0;
	setp.eq.s64 	%p25, %rd11, 0;
	or.pred  	%p26, %p25, %p24;
	@%p26 bra 	$L__BB5_19;

	cvta.to.global.u64 	%rd32, %rd11;
	mul.wide.s32 	%rd33, %r4, 4;
	add.s64 	%rd34, %rd32, %rd33;
	ld.global.nc.f32 	%f61, [%rd34];
	add.ftz.f32 	%f67, %f67, %f61;

$L__BB5_19:
	mad.lo.s32 	%r74, %r2, %r33, %r4;
	cvta.to.global.u64 	%rd35, %rd9;
	mul.wide.s32 	%rd36, %r74, 4;
	add.s64 	%rd37, %rd35, %rd36;
	st.global.f32 	[%rd37], %f67;

$L__BB5_20:
	ret;

}

