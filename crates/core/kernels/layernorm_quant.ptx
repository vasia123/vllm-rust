//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32267302
// Cuda compilation tools, release 12.0, V12.0.140
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_89
.address_size 64

	// .globl	fused_rmsnorm_fp8
.extern .shared .align 16 .b8 smem[];

.visible .entry fused_rmsnorm_fp8(
	.param .u64 fused_rmsnorm_fp8_param_0,
	.param .u64 fused_rmsnorm_fp8_param_1,
	.param .u64 fused_rmsnorm_fp8_param_2,
	.param .u64 fused_rmsnorm_fp8_param_3,
	.param .u32 fused_rmsnorm_fp8_param_4,
	.param .u32 fused_rmsnorm_fp8_param_5,
	.param .f32 fused_rmsnorm_fp8_param_6
)
{
	.reg .pred 	%p<52>;
	.reg .b16 	%rs<9>;
	.reg .f32 	%f<86>;
	.reg .b32 	%r<127>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<43>;


	ld.param.u64 	%rd10, [fused_rmsnorm_fp8_param_0];
	ld.param.u64 	%rd11, [fused_rmsnorm_fp8_param_1];
	ld.param.u64 	%rd12, [fused_rmsnorm_fp8_param_2];
	ld.param.u64 	%rd13, [fused_rmsnorm_fp8_param_3];
	ld.param.u32 	%r24, [fused_rmsnorm_fp8_param_4];
	ld.param.u32 	%r23, [fused_rmsnorm_fp8_param_5];
	ld.param.f32 	%f18, [fused_rmsnorm_fp8_param_6];
	cvta.to.global.u64 	%rd1, %rd13;
	cvta.to.global.u64 	%rd2, %rd12;
	mov.u32 	%r1, %ctaid.x;
	setp.ge.s32 	%p1, %r1, %r24;
	@%p1 bra 	$L__BB0_35;

	mov.u32 	%r2, %tid.x;
	mul.lo.s32 	%r25, %r1, %r23;
	cvt.s64.s32 	%rd3, %r25;
	mov.f32 	%f81, 0f00000000;
	setp.lt.s32 	%p2, %r2, %r23;
	@%p2 bra 	$L__BB0_3;
	bra.uni 	$L__BB0_5;

$L__BB0_3:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r123, %r2;

$L__BB0_4:
	cvt.s64.s32 	%rd14, %r123;
	add.s64 	%rd15, %rd14, %rd3;
	shl.b64 	%rd16, %rd15, 1;
	add.s64 	%rd17, %rd2, %rd16;
	ld.global.nc.u16 	%rs1, [%rd17];
	// begin inline asm
	{ mov.b32 %f21, {0,%rs1};}

	// end inline asm
	fma.rn.ftz.f32 	%f81, %f21, %f21, %f81;
	add.s32 	%r123, %r123, %r3;
	setp.lt.s32 	%p3, %r123, %r23;
	@%p3 bra 	$L__BB0_4;

$L__BB0_5:
	mov.b32 	%r26, %f81;
	mov.u32 	%r27, 31;
	mov.u32 	%r28, 16;
	mov.u32 	%r29, -1;
	shfl.sync.bfly.b32 	%r30|%p4, %r26, %r28, %r27, %r29;
	mov.b32 	%f22, %r30;
	add.ftz.f32 	%f23, %f81, %f22;
	mov.b32 	%r31, %f23;
	mov.u32 	%r32, 8;
	shfl.sync.bfly.b32 	%r33|%p5, %r31, %r32, %r27, %r29;
	mov.b32 	%f24, %r33;
	add.ftz.f32 	%f25, %f23, %f24;
	mov.b32 	%r34, %f25;
	mov.u32 	%r35, 4;
	shfl.sync.bfly.b32 	%r36|%p6, %r34, %r35, %r27, %r29;
	mov.b32 	%f26, %r36;
	add.ftz.f32 	%f27, %f25, %f26;
	mov.b32 	%r37, %f27;
	mov.u32 	%r38, 2;
	shfl.sync.bfly.b32 	%r39|%p7, %r37, %r38, %r27, %r29;
	mov.b32 	%f28, %r39;
	add.ftz.f32 	%f29, %f27, %f28;
	mov.b32 	%r40, %f29;
	mov.u32 	%r41, 1;
	shfl.sync.bfly.b32 	%r42|%p8, %r40, %r41, %r27, %r29;
	mov.b32 	%f30, %r42;
	add.ftz.f32 	%f4, %f29, %f30;
	mov.u32 	%r6, %ntid.x;
	shr.u32 	%r7, %r6, 5;
	shr.s32 	%r43, %r2, 31;
	shr.u32 	%r44, %r43, 27;
	add.s32 	%r45, %r2, %r44;
	and.b32  	%r46, %r45, -32;
	sub.s32 	%r8, %r2, %r46;
	setp.ne.s32 	%p9, %r8, 0;
	shr.s32 	%r47, %r45, 5;
	shl.b32 	%r48, %r47, 2;
	mov.u32 	%r49, smem;
	add.s32 	%r9, %r49, %r48;
	@%p9 bra 	$L__BB0_7;

	st.shared.f32 	[%r9], %f4;

$L__BB0_7:
	bar.sync 	0;
	add.s32 	%r10, %r2, 31;
	setp.gt.u32 	%p10, %r10, 62;
	shl.b32 	%r50, %r8, 2;
	add.s32 	%r11, %r49, %r50;
	@%p10 bra 	$L__BB0_12;

	setp.ge.s32 	%p11, %r8, %r7;
	mov.f32 	%f82, 0f00000000;
	@%p11 bra 	$L__BB0_10;

	ld.shared.f32 	%f82, [%r11];

$L__BB0_10:
	mov.b32 	%r52, %f82;
	mov.u32 	%r53, 31;
	mov.u32 	%r54, 16;
	mov.u32 	%r55, -1;
	shfl.sync.bfly.b32 	%r56|%p12, %r52, %r54, %r53, %r55;
	mov.b32 	%f32, %r56;
	add.ftz.f32 	%f33, %f82, %f32;
	mov.b32 	%r57, %f33;
	mov.u32 	%r58, 8;
	shfl.sync.bfly.b32 	%r59|%p13, %r57, %r58, %r53, %r55;
	mov.b32 	%f34, %r59;
	add.ftz.f32 	%f35, %f33, %f34;
	mov.b32 	%r60, %f35;
	mov.u32 	%r61, 4;
	shfl.sync.bfly.b32 	%r62|%p14, %r60, %r61, %r53, %r55;
	mov.b32 	%f36, %r62;
	add.ftz.f32 	%f37, %f35, %f36;
	mov.b32 	%r63, %f37;
	mov.u32 	%r64, 2;
	shfl.sync.bfly.b32 	%r65|%p15, %r63, %r64, %r53, %r55;
	mov.b32 	%f38, %r65;
	add.ftz.f32 	%f39, %f37, %f38;
	mov.b32 	%r66, %f39;
	mov.u32 	%r67, 1;
	shfl.sync.bfly.b32 	%r68|%p16, %r66, %r67, %r53, %r55;
	mov.b32 	%f40, %r68;
	add.ftz.f32 	%f7, %f39, %f40;
	@%p9 bra 	$L__BB0_12;

	st.shared.f32 	[smem], %f7;

$L__BB0_12:
	setp.ge.s32 	%p18, %r2, %r23;
	bar.sync 	0;
	cvt.rn.f32.s32 	%f42, %r23;
	ld.shared.f32 	%f43, [smem];
	div.approx.ftz.f32 	%f44, %f43, %f42;
	add.ftz.f32 	%f45, %f44, %f18;
	rsqrt.approx.ftz.f32 	%f8, %f45;
	mov.f32 	%f84, 0f00000000;
	@%p18 bra 	$L__BB0_15;

	mov.u32 	%r124, %r2;

$L__BB0_14:
	cvt.s64.s32 	%rd18, %r124;
	add.s64 	%rd19, %rd18, %rd3;
	shl.b64 	%rd20, %rd19, 1;
	add.s64 	%rd21, %rd2, %rd20;
	ld.global.nc.u16 	%rs2, [%rd21];
	// begin inline asm
	{ mov.b32 %f47, {0,%rs2};}

	// end inline asm
	mul.wide.s32 	%rd22, %r124, 2;
	add.s64 	%rd23, %rd1, %rd22;
	ld.global.nc.u16 	%rs3, [%rd23];
	// begin inline asm
	{ mov.b32 %f48, {0,%rs3};}

	// end inline asm
	mul.ftz.f32 	%f49, %f8, %f47;
	mul.ftz.f32 	%f50, %f49, %f48;
	abs.ftz.f32 	%f51, %f50;
	max.ftz.f32 	%f84, %f84, %f51;
	add.s32 	%r124, %r124, %r6;
	setp.lt.s32 	%p19, %r124, %r23;
	@%p19 bra 	$L__BB0_14;

$L__BB0_15:
	mov.b32 	%r69, %f84;
	mov.u32 	%r70, 31;
	mov.u32 	%r71, 16;
	mov.u32 	%r72, -1;
	shfl.sync.bfly.b32 	%r73|%p20, %r69, %r71, %r70, %r72;
	mov.b32 	%f52, %r73;
	max.ftz.f32 	%f53, %f84, %f52;
	mov.b32 	%r74, %f53;
	mov.u32 	%r75, 8;
	shfl.sync.bfly.b32 	%r76|%p21, %r74, %r75, %r70, %r72;
	mov.b32 	%f54, %r76;
	max.ftz.f32 	%f55, %f53, %f54;
	mov.b32 	%r77, %f55;
	mov.u32 	%r78, 4;
	shfl.sync.bfly.b32 	%r79|%p22, %r77, %r78, %r70, %r72;
	mov.b32 	%f56, %r79;
	max.ftz.f32 	%f57, %f55, %f56;
	mov.b32 	%r80, %f57;
	mov.u32 	%r81, 2;
	shfl.sync.bfly.b32 	%r82|%p23, %r80, %r81, %r70, %r72;
	mov.b32 	%f58, %r82;
	max.ftz.f32 	%f59, %f57, %f58;
	mov.b32 	%r83, %f59;
	mov.u32 	%r84, 1;
	shfl.sync.bfly.b32 	%r85|%p24, %r83, %r84, %r70, %r72;
	mov.b32 	%f60, %r85;
	max.ftz.f32 	%f12, %f59, %f60;
	@%p9 bra 	$L__BB0_17;

	st.shared.f32 	[%r9], %f12;

$L__BB0_17:
	bar.sync 	0;
	@%p10 bra 	$L__BB0_22;

	setp.ge.s32 	%p27, %r8, %r7;
	mov.f32 	%f85, 0f00000000;
	@%p27 bra 	$L__BB0_20;

	ld.shared.f32 	%f85, [%r11];

$L__BB0_20:
	mov.b32 	%r86, %f85;
	mov.u32 	%r87, 31;
	mov.u32 	%r88, 16;
	mov.u32 	%r89, -1;
	shfl.sync.bfly.b32 	%r90|%p28, %r86, %r88, %r87, %r89;
	mov.b32 	%f62, %r90;
	max.ftz.f32 	%f63, %f85, %f62;
	mov.b32 	%r91, %f63;
	mov.u32 	%r92, 8;
	shfl.sync.bfly.b32 	%r93|%p29, %r91, %r92, %r87, %r89;
	mov.b32 	%f64, %r93;
	max.ftz.f32 	%f65, %f63, %f64;
	mov.b32 	%r94, %f65;
	mov.u32 	%r95, 4;
	shfl.sync.bfly.b32 	%r96|%p30, %r94, %r95, %r87, %r89;
	mov.b32 	%f66, %r96;
	max.ftz.f32 	%f67, %f65, %f66;
	mov.b32 	%r97, %f67;
	mov.u32 	%r98, 2;
	shfl.sync.bfly.b32 	%r99|%p31, %r97, %r98, %r87, %r89;
	mov.b32 	%f68, %r99;
	max.ftz.f32 	%f69, %f67, %f68;
	mov.b32 	%r100, %f69;
	mov.u32 	%r101, 1;
	shfl.sync.bfly.b32 	%r102|%p32, %r100, %r101, %r87, %r89;
	mov.b32 	%f70, %r102;
	max.ftz.f32 	%f15, %f69, %f70;
	@%p9 bra 	$L__BB0_22;

	st.shared.f32 	[smem], %f15;

$L__BB0_22:
	bar.sync 	0;
	ld.shared.f32 	%f71, [smem];
	mov.f32 	%f72, 0f43E00000;
	div.approx.ftz.f32 	%f73, %f71, %f72;
	mov.f32 	%f74, 0f2B8CBCCC;
	max.ftz.f32 	%f16, %f73, %f74;
	@%p18 bra 	$L__BB0_33;

	cvta.to.global.u64 	%rd4, %rd10;
	rcp.approx.ftz.f32 	%f17, %f16;
	mov.u32 	%r125, %r2;

$L__BB0_24:
	cvt.s64.s32 	%rd24, %r125;
	add.s64 	%rd5, %rd24, %rd3;
	shl.b64 	%rd25, %rd5, 1;
	add.s64 	%rd26, %rd2, %rd25;
	ld.global.nc.u16 	%rs4, [%rd26];
	// begin inline asm
	{ mov.b32 %f75, {0,%rs4};}

	// end inline asm
	mul.wide.s32 	%rd27, %r125, 2;
	add.s64 	%rd28, %rd1, %rd27;
	ld.global.nc.u16 	%rs5, [%rd28];
	// begin inline asm
	{ mov.b32 %f76, {0,%rs5};}

	// end inline asm
	mul.ftz.f32 	%f77, %f8, %f75;
	mul.ftz.f32 	%f78, %f77, %f76;
	mul.ftz.f32 	%f79, %f17, %f78;
	mov.b32 	%r104, %f79;
	and.b32  	%r105, %r104, 2147483647;
	setp.gt.u32 	%p35, %r105, 2139095040;
	cvt.ftz.f64.f32 	%fd1, %f79;
	mov.b64 	%rd29, %fd1;
	selp.b64 	%rd6, 9223372036317904896, %rd29, %p35;
	shr.u64 	%rd30, %rd6, 52;
	cvt.u32.u64 	%r106, %rd30;
	add.s32 	%r15, %r106, 8;
	shr.u64 	%rd31, %rd6, 49;
	cvt.u32.u64 	%r16, %rd31;
	and.b32  	%r17, %r16, 7;
	and.b64  	%rd7, %rd6, 9223372036854775807;
	setp.lt.u64 	%p36, %rd7, 4562146422526312449;
	mov.u32 	%r126, 0;
	@%p36 bra 	$L__BB0_32;

	setp.gt.u64 	%p37, %rd7, 9218868437227405312;
	mov.u32 	%r126, 127;
	@%p37 bra 	$L__BB0_32;

	setp.gt.u64 	%p38, %rd7, 4646870390516219904;
	mov.u32 	%r126, 126;
	@%p38 bra 	$L__BB0_32;

	setp.lt.u64 	%p39, %rd7, 4580160821035794432;
	@%p39 bra 	$L__BB0_29;
	bra.uni 	$L__BB0_28;

$L__BB0_29:
	cvt.u16.u32 	%rs6, %r15;
	mov.u16 	%rs7, 1;
	sub.s16 	%rs8, %rs7, %rs6;
	cvt.u32.u16 	%r115, %rs8;
	and.b32  	%r116, %r115, 255;
	or.b32  	%r117, %r17, 8;
	shr.u32 	%r126, %r117, %r116;
	mov.u64 	%rd33, 562949953421312;
	shl.b64 	%rd34, %rd33, %r116;
	add.s64 	%rd35, %rd34, -1;
	or.b64  	%rd36, %rd6, 4503599627370496;
	and.b64  	%rd8, %rd35, %rd36;
	mov.u64 	%rd37, 281474976710656;
	shl.b64 	%rd9, %rd37, %r116;
	setp.gt.u64 	%p45, %rd8, %rd9;
	@%p45 bra 	$L__BB0_31;

	setp.ne.s64 	%p46, %rd8, %rd9;
	and.b32  	%r118, %r126, 1;
	setp.eq.b32 	%p47, %r118, 1;
	not.pred 	%p48, %p47;
	or.pred  	%p49, %p48, %p46;
	@%p49 bra 	$L__BB0_32;

$L__BB0_31:
	and.b32  	%r119, %r126, 255;
	add.s32 	%r126, %r119, 1;
	bra.uni 	$L__BB0_32;

$L__BB0_28:
	shl.b32 	%r109, %r15, 3;
	and.b32  	%r110, %r109, 2040;
	or.b32  	%r111, %r110, %r17;
	and.b64  	%rd32, %rd6, 562949953421311;
	setp.gt.u64 	%p40, %rd32, 281474976710656;
	setp.eq.s64 	%p41, %rd32, 281474976710656;
	and.b32  	%r112, %r16, 1;
	setp.eq.b32 	%p42, %r112, 1;
	and.pred  	%p43, %p41, %p42;
	or.pred  	%p44, %p40, %p43;
	and.b32  	%r113, %r111, 255;
	add.s32 	%r114, %r113, 1;
	selp.b32 	%r126, %r114, %r111, %p44;

$L__BB0_32:
	shr.u64 	%rd38, %rd6, 63;
	cvt.u32.u64 	%r120, %rd38;
	shl.b32 	%r121, %r120, 7;
	or.b32  	%r122, %r126, %r121;
	add.s64 	%rd39, %rd4, %rd5;
	st.global.u8 	[%rd39], %r122;
	add.s32 	%r125, %r125, %r6;
	setp.lt.s32 	%p50, %r125, %r23;
	@%p50 bra 	$L__BB0_24;

$L__BB0_33:
	setp.ne.s32 	%p51, %r2, 0;
	@%p51 bra 	$L__BB0_35;

	cvta.to.global.u64 	%rd40, %rd11;
	mul.wide.s32 	%rd41, %r1, 4;
	add.s64 	%rd42, %rd40, %rd41;
	st.global.f32 	[%rd42], %f16;

$L__BB0_35:
	ret;

}
	// .globl	fused_layernorm_fp8
.visible .entry fused_layernorm_fp8(
	.param .u64 fused_layernorm_fp8_param_0,
	.param .u64 fused_layernorm_fp8_param_1,
	.param .u64 fused_layernorm_fp8_param_2,
	.param .u64 fused_layernorm_fp8_param_3,
	.param .u64 fused_layernorm_fp8_param_4,
	.param .u32 fused_layernorm_fp8_param_5,
	.param .u32 fused_layernorm_fp8_param_6,
	.param .f32 fused_layernorm_fp8_param_7,
	.param .u32 fused_layernorm_fp8_param_8
)
{
	.reg .pred 	%p<91>;
	.reg .b16 	%rs<19>;
	.reg .f32 	%f<141>;
	.reg .b32 	%r<201>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<78>;


	ld.param.u64 	%rd18, [fused_layernorm_fp8_param_0];
	ld.param.u64 	%rd16, [fused_layernorm_fp8_param_1];
	ld.param.u64 	%rd19, [fused_layernorm_fp8_param_2];
	ld.param.u64 	%rd20, [fused_layernorm_fp8_param_3];
	ld.param.u64 	%rd17, [fused_layernorm_fp8_param_4];
	ld.param.u32 	%r37, [fused_layernorm_fp8_param_5];
	ld.param.u32 	%r35, [fused_layernorm_fp8_param_6];
	ld.param.f32 	%f29, [fused_layernorm_fp8_param_7];
	ld.param.u32 	%r36, [fused_layernorm_fp8_param_8];
	cvta.to.global.u64 	%rd1, %rd18;
	cvta.to.global.u64 	%rd2, %rd17;
	cvta.to.global.u64 	%rd3, %rd20;
	cvta.to.global.u64 	%rd4, %rd19;
	mov.u32 	%r1, %ctaid.x;
	setp.ge.s32 	%p1, %r1, %r37;
	@%p1 bra 	$L__BB1_58;

	mov.u32 	%r2, %tid.x;
	mul.lo.s32 	%r38, %r1, %r35;
	cvt.s64.s32 	%rd5, %r38;
	shr.s32 	%r39, %r2, 31;
	shr.u32 	%r40, %r39, 27;
	add.s32 	%r41, %r2, %r40;
	and.b32  	%r42, %r41, -32;
	sub.s32 	%r3, %r2, %r42;
	mov.u32 	%r4, %ntid.x;
	shr.u32 	%r5, %r4, 5;
	setp.ge.s32 	%p2, %r2, %r35;
	mov.f32 	%f132, 0f00000000;
	@%p2 bra 	$L__BB1_4;

	mov.u32 	%r193, %r2;

$L__BB1_3:
	cvt.s64.s32 	%rd21, %r193;
	add.s64 	%rd22, %rd21, %rd5;
	shl.b64 	%rd23, %rd22, 1;
	add.s64 	%rd24, %rd4, %rd23;
	ld.global.nc.u16 	%rs1, [%rd24];
	// begin inline asm
	{ mov.b32 %f32, {0,%rs1};}

	// end inline asm
	add.ftz.f32 	%f132, %f132, %f32;
	add.s32 	%r193, %r193, %r4;
	setp.lt.s32 	%p3, %r193, %r35;
	@%p3 bra 	$L__BB1_3;

$L__BB1_4:
	mov.b32 	%r43, %f132;
	mov.u32 	%r44, 31;
	mov.u32 	%r45, 16;
	mov.u32 	%r46, -1;
	shfl.sync.bfly.b32 	%r47|%p4, %r43, %r45, %r44, %r46;
	mov.b32 	%f33, %r47;
	add.ftz.f32 	%f34, %f132, %f33;
	mov.b32 	%r48, %f34;
	mov.u32 	%r49, 8;
	shfl.sync.bfly.b32 	%r50|%p5, %r48, %r49, %r44, %r46;
	mov.b32 	%f35, %r50;
	add.ftz.f32 	%f36, %f34, %f35;
	mov.b32 	%r51, %f36;
	mov.u32 	%r52, 4;
	shfl.sync.bfly.b32 	%r53|%p6, %r51, %r52, %r44, %r46;
	mov.b32 	%f37, %r53;
	add.ftz.f32 	%f38, %f36, %f37;
	mov.b32 	%r54, %f38;
	mov.u32 	%r55, 2;
	shfl.sync.bfly.b32 	%r56|%p7, %r54, %r55, %r44, %r46;
	mov.b32 	%f39, %r56;
	add.ftz.f32 	%f40, %f38, %f39;
	mov.b32 	%r57, %f40;
	mov.u32 	%r58, 1;
	shfl.sync.bfly.b32 	%r59|%p8, %r57, %r58, %r44, %r46;
	mov.b32 	%f41, %r59;
	add.ftz.f32 	%f4, %f40, %f41;
	shr.s32 	%r63, %r41, 5;
	shl.b32 	%r64, %r63, 2;
	mov.u32 	%r65, smem;
	add.s32 	%r8, %r65, %r64;
	setp.ne.s32 	%p9, %r3, 0;
	@%p9 bra 	$L__BB1_6;

	st.shared.f32 	[%r8], %f4;

$L__BB1_6:
	bar.sync 	0;
	add.s32 	%r9, %r2, 31;
	setp.gt.u32 	%p10, %r9, 62;
	shl.b32 	%r66, %r3, 2;
	add.s32 	%r10, %r65, %r66;
	@%p10 bra 	$L__BB1_11;

	setp.ge.s32 	%p11, %r3, %r5;
	mov.f32 	%f133, 0f00000000;
	@%p11 bra 	$L__BB1_9;

	ld.shared.f32 	%f133, [%r10];

$L__BB1_9:
	mov.b32 	%r68, %f133;
	mov.u32 	%r69, 31;
	mov.u32 	%r70, 16;
	mov.u32 	%r71, -1;
	shfl.sync.bfly.b32 	%r72|%p12, %r68, %r70, %r69, %r71;
	mov.b32 	%f43, %r72;
	add.ftz.f32 	%f44, %f133, %f43;
	mov.b32 	%r73, %f44;
	mov.u32 	%r74, 8;
	shfl.sync.bfly.b32 	%r75|%p13, %r73, %r74, %r69, %r71;
	mov.b32 	%f45, %r75;
	add.ftz.f32 	%f46, %f44, %f45;
	mov.b32 	%r76, %f46;
	mov.u32 	%r77, 4;
	shfl.sync.bfly.b32 	%r78|%p14, %r76, %r77, %r69, %r71;
	mov.b32 	%f47, %r78;
	add.ftz.f32 	%f48, %f46, %f47;
	mov.b32 	%r79, %f48;
	mov.u32 	%r80, 2;
	shfl.sync.bfly.b32 	%r81|%p15, %r79, %r80, %r69, %r71;
	mov.b32 	%f49, %r81;
	add.ftz.f32 	%f50, %f48, %f49;
	mov.b32 	%r82, %f50;
	mov.u32 	%r83, 1;
	shfl.sync.bfly.b32 	%r84|%p16, %r82, %r83, %r69, %r71;
	mov.b32 	%f51, %r84;
	add.ftz.f32 	%f7, %f50, %f51;
	@%p9 bra 	$L__BB1_11;

	st.shared.f32 	[smem], %f7;

$L__BB1_11:
	bar.sync 	0;
	cvt.rn.f32.s32 	%f8, %r35;
	ld.shared.f32 	%f53, [smem];
	div.approx.ftz.f32 	%f9, %f53, %f8;
	mov.f32 	%f135, 0f00000000;
	@%p2 bra 	$L__BB1_14;

	mov.u32 	%r194, %r2;

$L__BB1_13:
	cvt.s64.s32 	%rd25, %r194;
	add.s64 	%rd26, %rd25, %rd5;
	shl.b64 	%rd27, %rd26, 1;
	add.s64 	%rd28, %rd4, %rd27;
	ld.global.nc.u16 	%rs2, [%rd28];
	// begin inline asm
	{ mov.b32 %f55, {0,%rs2};}

	// end inline asm
	sub.ftz.f32 	%f56, %f55, %f9;
	fma.rn.ftz.f32 	%f135, %f56, %f56, %f135;
	add.s32 	%r194, %r194, %r4;
	setp.lt.s32 	%p19, %r194, %r35;
	@%p19 bra 	$L__BB1_13;

$L__BB1_14:
	mov.b32 	%r85, %f135;
	mov.u32 	%r86, 31;
	mov.u32 	%r87, 16;
	mov.u32 	%r88, -1;
	shfl.sync.bfly.b32 	%r89|%p20, %r85, %r87, %r86, %r88;
	mov.b32 	%f57, %r89;
	add.ftz.f32 	%f58, %f135, %f57;
	mov.b32 	%r90, %f58;
	mov.u32 	%r91, 8;
	shfl.sync.bfly.b32 	%r92|%p21, %r90, %r91, %r86, %r88;
	mov.b32 	%f59, %r92;
	add.ftz.f32 	%f60, %f58, %f59;
	mov.b32 	%r93, %f60;
	mov.u32 	%r94, 4;
	shfl.sync.bfly.b32 	%r95|%p22, %r93, %r94, %r86, %r88;
	mov.b32 	%f61, %r95;
	add.ftz.f32 	%f62, %f60, %f61;
	mov.b32 	%r96, %f62;
	mov.u32 	%r97, 2;
	shfl.sync.bfly.b32 	%r98|%p23, %r96, %r97, %r86, %r88;
	mov.b32 	%f63, %r98;
	add.ftz.f32 	%f64, %f62, %f63;
	mov.b32 	%r99, %f64;
	mov.u32 	%r100, 1;
	shfl.sync.bfly.b32 	%r101|%p24, %r99, %r100, %r86, %r88;
	mov.b32 	%f65, %r101;
	add.ftz.f32 	%f13, %f64, %f65;
	@%p9 bra 	$L__BB1_16;

	st.shared.f32 	[%r8], %f13;

$L__BB1_16:
	bar.sync 	0;
	@%p10 bra 	$L__BB1_21;

	setp.ge.s32 	%p27, %r3, %r5;
	mov.f32 	%f136, 0f00000000;
	@%p27 bra 	$L__BB1_19;

	ld.shared.f32 	%f136, [%r10];

$L__BB1_19:
	mov.b32 	%r102, %f136;
	mov.u32 	%r103, 31;
	mov.u32 	%r104, 16;
	mov.u32 	%r105, -1;
	shfl.sync.bfly.b32 	%r106|%p28, %r102, %r104, %r103, %r105;
	mov.b32 	%f67, %r106;
	add.ftz.f32 	%f68, %f136, %f67;
	mov.b32 	%r107, %f68;
	mov.u32 	%r108, 8;
	shfl.sync.bfly.b32 	%r109|%p29, %r107, %r108, %r103, %r105;
	mov.b32 	%f69, %r109;
	add.ftz.f32 	%f70, %f68, %f69;
	mov.b32 	%r110, %f70;
	mov.u32 	%r111, 4;
	shfl.sync.bfly.b32 	%r112|%p30, %r110, %r111, %r103, %r105;
	mov.b32 	%f71, %r112;
	add.ftz.f32 	%f72, %f70, %f71;
	mov.b32 	%r113, %f72;
	mov.u32 	%r114, 2;
	shfl.sync.bfly.b32 	%r115|%p31, %r113, %r114, %r103, %r105;
	mov.b32 	%f73, %r115;
	add.ftz.f32 	%f74, %f72, %f73;
	mov.b32 	%r116, %f74;
	mov.u32 	%r117, 1;
	shfl.sync.bfly.b32 	%r118|%p32, %r116, %r117, %r103, %r105;
	mov.b32 	%f75, %r118;
	add.ftz.f32 	%f16, %f74, %f75;
	@%p9 bra 	$L__BB1_21;

	st.shared.f32 	[smem], %f16;

$L__BB1_21:
	bar.sync 	0;
	ld.shared.f32 	%f77, [smem];
	div.approx.ftz.f32 	%f78, %f77, %f8;
	add.ftz.f32 	%f79, %f78, %f29;
	rsqrt.approx.ftz.f32 	%f17, %f79;
	mov.f32 	%f139, 0f00000000;
	@%p2 bra 	$L__BB1_27;

	setp.ne.s32 	%p35, %r36, 0;
	setp.ne.s64 	%p36, %rd17, 0;
	mov.f32 	%f139, 0f00000000;
	and.pred  	%p37, %p36, %p35;
	@%p37 bra 	$L__BB1_25;
	bra.uni 	$L__BB1_23;

$L__BB1_25:
	mov.u32 	%r196, %r2;

$L__BB1_26:
	cvt.s64.s32 	%rd35, %r196;
	add.s64 	%rd36, %rd35, %rd5;
	shl.b64 	%rd37, %rd36, 1;
	add.s64 	%rd38, %rd4, %rd37;
	ld.global.nc.u16 	%rs5, [%rd38];
	// begin inline asm
	{ mov.b32 %f88, {0,%rs5};}

	// end inline asm
	mul.wide.s32 	%rd39, %r196, 2;
	add.s64 	%rd40, %rd3, %rd39;
	ld.global.nc.u16 	%rs6, [%rd40];
	// begin inline asm
	{ mov.b32 %f89, {0,%rs6};}

	// end inline asm
	sub.ftz.f32 	%f91, %f88, %f9;
	mul.ftz.f32 	%f92, %f17, %f91;
	add.s64 	%rd41, %rd2, %rd39;
	ld.global.nc.u16 	%rs7, [%rd41];
	// begin inline asm
	{ mov.b32 %f90, {0,%rs7};}

	// end inline asm
	fma.rn.ftz.f32 	%f93, %f92, %f89, %f90;
	abs.ftz.f32 	%f94, %f93;
	max.ftz.f32 	%f139, %f139, %f94;
	add.s32 	%r196, %r196, %r4;
	setp.lt.s32 	%p39, %r196, %r35;
	@%p39 bra 	$L__BB1_26;
	bra.uni 	$L__BB1_27;

$L__BB1_23:
	mov.u32 	%r195, %r2;

$L__BB1_24:
	cvt.s64.s32 	%rd29, %r195;
	add.s64 	%rd30, %rd29, %rd5;
	shl.b64 	%rd31, %rd30, 1;
	add.s64 	%rd32, %rd4, %rd31;
	ld.global.nc.u16 	%rs3, [%rd32];
	// begin inline asm
	{ mov.b32 %f81, {0,%rs3};}

	// end inline asm
	mul.wide.s32 	%rd33, %r195, 2;
	add.s64 	%rd34, %rd3, %rd33;
	ld.global.nc.u16 	%rs4, [%rd34];
	// begin inline asm
	{ mov.b32 %f82, {0,%rs4};}

	// end inline asm
	sub.ftz.f32 	%f83, %f81, %f9;
	mul.ftz.f32 	%f84, %f17, %f83;
	mul.ftz.f32 	%f85, %f84, %f82;
	abs.ftz.f32 	%f86, %f85;
	max.ftz.f32 	%f139, %f139, %f86;
	add.s32 	%r195, %r195, %r4;
	setp.lt.s32 	%p38, %r195, %r35;
	@%p38 bra 	$L__BB1_24;

$L__BB1_27:
	mov.b32 	%r119, %f139;
	mov.u32 	%r120, 31;
	mov.u32 	%r121, 16;
	mov.u32 	%r122, -1;
	shfl.sync.bfly.b32 	%r123|%p40, %r119, %r121, %r120, %r122;
	mov.b32 	%f95, %r123;
	max.ftz.f32 	%f96, %f139, %f95;
	mov.b32 	%r124, %f96;
	mov.u32 	%r125, 8;
	shfl.sync.bfly.b32 	%r126|%p41, %r124, %r125, %r120, %r122;
	mov.b32 	%f97, %r126;
	max.ftz.f32 	%f98, %f96, %f97;
	mov.b32 	%r127, %f98;
	mov.u32 	%r128, 4;
	shfl.sync.bfly.b32 	%r129|%p42, %r127, %r128, %r120, %r122;
	mov.b32 	%f99, %r129;
	max.ftz.f32 	%f100, %f98, %f99;
	mov.b32 	%r130, %f100;
	mov.u32 	%r131, 2;
	shfl.sync.bfly.b32 	%r132|%p43, %r130, %r131, %r120, %r122;
	mov.b32 	%f101, %r132;
	max.ftz.f32 	%f102, %f100, %f101;
	mov.b32 	%r133, %f102;
	mov.u32 	%r134, 1;
	shfl.sync.bfly.b32 	%r135|%p44, %r133, %r134, %r120, %r122;
	mov.b32 	%f103, %r135;
	max.ftz.f32 	%f23, %f102, %f103;
	@%p9 bra 	$L__BB1_29;

	st.shared.f32 	[%r8], %f23;

$L__BB1_29:
	bar.sync 	0;
	@%p10 bra 	$L__BB1_34;

	setp.ge.s32 	%p47, %r3, %r5;
	mov.f32 	%f140, 0f00000000;
	@%p47 bra 	$L__BB1_32;

	ld.shared.f32 	%f140, [%r10];

$L__BB1_32:
	mov.b32 	%r136, %f140;
	mov.u32 	%r137, 31;
	mov.u32 	%r138, 16;
	mov.u32 	%r139, -1;
	shfl.sync.bfly.b32 	%r140|%p48, %r136, %r138, %r137, %r139;
	mov.b32 	%f105, %r140;
	max.ftz.f32 	%f106, %f140, %f105;
	mov.b32 	%r141, %f106;
	mov.u32 	%r142, 8;
	shfl.sync.bfly.b32 	%r143|%p49, %r141, %r142, %r137, %r139;
	mov.b32 	%f107, %r143;
	max.ftz.f32 	%f108, %f106, %f107;
	mov.b32 	%r144, %f108;
	mov.u32 	%r145, 4;
	shfl.sync.bfly.b32 	%r146|%p50, %r144, %r145, %r137, %r139;
	mov.b32 	%f109, %r146;
	max.ftz.f32 	%f110, %f108, %f109;
	mov.b32 	%r147, %f110;
	mov.u32 	%r148, 2;
	shfl.sync.bfly.b32 	%r149|%p51, %r147, %r148, %r137, %r139;
	mov.b32 	%f111, %r149;
	max.ftz.f32 	%f112, %f110, %f111;
	mov.b32 	%r150, %f112;
	mov.u32 	%r151, 1;
	shfl.sync.bfly.b32 	%r152|%p52, %r150, %r151, %r137, %r139;
	mov.b32 	%f113, %r152;
	max.ftz.f32 	%f26, %f112, %f113;
	@%p9 bra 	$L__BB1_34;

	st.shared.f32 	[smem], %f26;

$L__BB1_34:
	bar.sync 	0;
	ld.shared.f32 	%f114, [smem];
	mov.f32 	%f115, 0f43E00000;
	div.approx.ftz.f32 	%f116, %f114, %f115;
	mov.f32 	%f117, 0f2B8CBCCC;
	max.ftz.f32 	%f27, %f116, %f117;
	rcp.approx.ftz.f32 	%f28, %f27;
	@%p2 bra 	$L__BB1_56;

	setp.ne.s32 	%p55, %r36, 0;
	setp.ne.s64 	%p56, %rd17, 0;
	and.pred  	%p57, %p56, %p55;
	@%p57 bra 	$L__BB1_46;
	bra.uni 	$L__BB1_36;

$L__BB1_46:
	mov.u32 	%r199, %r2;

$L__BB1_47:
	cvt.s64.s32 	%rd58, %r199;
	add.s64 	%rd11, %rd58, %rd5;
	shl.b64 	%rd59, %rd11, 1;
	add.s64 	%rd60, %rd4, %rd59;
	ld.global.nc.u16 	%rs13, [%rd60];
	// begin inline asm
	{ mov.b32 %f124, {0,%rs13};}

	// end inline asm
	mul.wide.s32 	%rd61, %r199, 2;
	add.s64 	%rd62, %rd3, %rd61;
	ld.global.nc.u16 	%rs14, [%rd62];
	// begin inline asm
	{ mov.b32 %f125, {0,%rs14};}

	// end inline asm
	sub.ftz.f32 	%f127, %f124, %f9;
	mul.ftz.f32 	%f128, %f17, %f127;
	add.s64 	%rd63, %rd2, %rd61;
	ld.global.nc.u16 	%rs15, [%rd63];
	// begin inline asm
	{ mov.b32 %f126, {0,%rs15};}

	// end inline asm
	fma.rn.ftz.f32 	%f129, %f128, %f125, %f126;
	mul.ftz.f32 	%f130, %f28, %f129;
	mov.b32 	%r174, %f130;
	and.b32  	%r175, %r174, 2147483647;
	setp.gt.u32 	%p74, %r175, 2139095040;
	cvt.ftz.f64.f32 	%fd2, %f130;
	mov.b64 	%rd64, %fd2;
	selp.b64 	%rd12, 9223372036317904896, %rd64, %p74;
	shr.u64 	%rd65, %rd12, 52;
	cvt.u32.u64 	%r176, %rd65;
	add.s32 	%r27, %r176, 8;
	shr.u64 	%rd66, %rd12, 49;
	cvt.u32.u64 	%r28, %rd66;
	and.b32  	%r29, %r28, 7;
	and.b64  	%rd13, %rd12, 9223372036854775807;
	setp.lt.u64 	%p75, %rd13, 4562146422526312449;
	mov.u32 	%r200, 0;
	@%p75 bra 	$L__BB1_55;

	setp.gt.u64 	%p76, %rd13, 9218868437227405312;
	mov.u32 	%r200, 127;
	@%p76 bra 	$L__BB1_55;

	setp.gt.u64 	%p77, %rd13, 4646870390516219904;
	mov.u32 	%r200, 126;
	@%p77 bra 	$L__BB1_55;

	setp.lt.u64 	%p78, %rd13, 4580160821035794432;
	@%p78 bra 	$L__BB1_52;
	bra.uni 	$L__BB1_51;

$L__BB1_52:
	cvt.u16.u32 	%rs16, %r27;
	mov.u16 	%rs17, 1;
	sub.s16 	%rs18, %rs17, %rs16;
	cvt.u32.u16 	%r185, %rs18;
	and.b32  	%r186, %r185, 255;
	or.b32  	%r187, %r29, 8;
	shr.u32 	%r200, %r187, %r186;
	mov.u64 	%rd68, 562949953421312;
	shl.b64 	%rd69, %rd68, %r186;
	add.s64 	%rd70, %rd69, -1;
	or.b64  	%rd71, %rd12, 4503599627370496;
	and.b64  	%rd14, %rd70, %rd71;
	mov.u64 	%rd72, 281474976710656;
	shl.b64 	%rd15, %rd72, %r186;
	setp.gt.u64 	%p84, %rd14, %rd15;
	@%p84 bra 	$L__BB1_54;

	setp.ne.s64 	%p85, %rd14, %rd15;
	and.b32  	%r188, %r200, 1;
	setp.eq.b32 	%p86, %r188, 1;
	not.pred 	%p87, %p86;
	or.pred  	%p88, %p87, %p85;
	@%p88 bra 	$L__BB1_55;

$L__BB1_54:
	and.b32  	%r189, %r200, 255;
	add.s32 	%r200, %r189, 1;
	bra.uni 	$L__BB1_55;

$L__BB1_51:
	shl.b32 	%r179, %r27, 3;
	and.b32  	%r180, %r179, 2040;
	or.b32  	%r181, %r180, %r29;
	and.b64  	%rd67, %rd12, 562949953421311;
	setp.gt.u64 	%p79, %rd67, 281474976710656;
	setp.eq.s64 	%p80, %rd67, 281474976710656;
	and.b32  	%r182, %r28, 1;
	setp.eq.b32 	%p81, %r182, 1;
	and.pred  	%p82, %p80, %p81;
	or.pred  	%p83, %p79, %p82;
	and.b32  	%r183, %r181, 255;
	add.s32 	%r184, %r183, 1;
	selp.b32 	%r200, %r184, %r181, %p83;

$L__BB1_55:
	shr.u64 	%rd73, %rd12, 63;
	cvt.u32.u64 	%r190, %rd73;
	shl.b32 	%r191, %r190, 7;
	or.b32  	%r192, %r200, %r191;
	add.s64 	%rd74, %rd1, %rd11;
	st.global.u8 	[%rd74], %r192;
	add.s32 	%r199, %r199, %r4;
	setp.lt.s32 	%p89, %r199, %r35;
	@%p89 bra 	$L__BB1_47;
	bra.uni 	$L__BB1_56;

$L__BB1_36:
	mov.u32 	%r197, %r2;

$L__BB1_37:
	cvt.s64.s32 	%rd42, %r197;
	add.s64 	%rd6, %rd42, %rd5;
	shl.b64 	%rd43, %rd6, 1;
	add.s64 	%rd44, %rd4, %rd43;
	ld.global.nc.u16 	%rs8, [%rd44];
	// begin inline asm
	{ mov.b32 %f118, {0,%rs8};}

	// end inline asm
	mul.wide.s32 	%rd45, %r197, 2;
	add.s64 	%rd46, %rd3, %rd45;
	ld.global.nc.u16 	%rs9, [%rd46];
	// begin inline asm
	{ mov.b32 %f119, {0,%rs9};}

	// end inline asm
	sub.ftz.f32 	%f120, %f118, %f9;
	mul.ftz.f32 	%f121, %f17, %f120;
	mul.ftz.f32 	%f122, %f121, %f119;
	mul.ftz.f32 	%f123, %f28, %f122;
	mov.b32 	%r154, %f123;
	and.b32  	%r155, %r154, 2147483647;
	setp.gt.u32 	%p58, %r155, 2139095040;
	cvt.ftz.f64.f32 	%fd1, %f123;
	mov.b64 	%rd47, %fd1;
	selp.b64 	%rd7, 9223372036317904896, %rd47, %p58;
	shr.u64 	%rd48, %rd7, 52;
	cvt.u32.u64 	%r156, %rd48;
	add.s32 	%r18, %r156, 8;
	shr.u64 	%rd49, %rd7, 49;
	cvt.u32.u64 	%r19, %rd49;
	and.b32  	%r20, %r19, 7;
	and.b64  	%rd8, %rd7, 9223372036854775807;
	setp.lt.u64 	%p59, %rd8, 4562146422526312449;
	mov.u32 	%r198, 0;
	@%p59 bra 	$L__BB1_45;

	setp.gt.u64 	%p60, %rd8, 9218868437227405312;
	mov.u32 	%r198, 127;
	@%p60 bra 	$L__BB1_45;

	setp.gt.u64 	%p61, %rd8, 4646870390516219904;
	mov.u32 	%r198, 126;
	@%p61 bra 	$L__BB1_45;

	setp.lt.u64 	%p62, %rd8, 4580160821035794432;
	@%p62 bra 	$L__BB1_42;
	bra.uni 	$L__BB1_41;

$L__BB1_42:
	cvt.u16.u32 	%rs10, %r18;
	mov.u16 	%rs11, 1;
	sub.s16 	%rs12, %rs11, %rs10;
	cvt.u32.u16 	%r165, %rs12;
	and.b32  	%r166, %r165, 255;
	or.b32  	%r167, %r20, 8;
	shr.u32 	%r198, %r167, %r166;
	mov.u64 	%rd51, 562949953421312;
	shl.b64 	%rd52, %rd51, %r166;
	add.s64 	%rd53, %rd52, -1;
	or.b64  	%rd54, %rd7, 4503599627370496;
	and.b64  	%rd9, %rd53, %rd54;
	mov.u64 	%rd55, 281474976710656;
	shl.b64 	%rd10, %rd55, %r166;
	setp.gt.u64 	%p68, %rd9, %rd10;
	@%p68 bra 	$L__BB1_44;

	setp.ne.s64 	%p69, %rd9, %rd10;
	and.b32  	%r168, %r198, 1;
	setp.eq.b32 	%p70, %r168, 1;
	not.pred 	%p71, %p70;
	or.pred  	%p72, %p71, %p69;
	@%p72 bra 	$L__BB1_45;

$L__BB1_44:
	and.b32  	%r169, %r198, 255;
	add.s32 	%r198, %r169, 1;
	bra.uni 	$L__BB1_45;

$L__BB1_41:
	shl.b32 	%r159, %r18, 3;
	and.b32  	%r160, %r159, 2040;
	or.b32  	%r161, %r160, %r20;
	and.b64  	%rd50, %rd7, 562949953421311;
	setp.gt.u64 	%p63, %rd50, 281474976710656;
	setp.eq.s64 	%p64, %rd50, 281474976710656;
	and.b32  	%r162, %r19, 1;
	setp.eq.b32 	%p65, %r162, 1;
	and.pred  	%p66, %p64, %p65;
	or.pred  	%p67, %p63, %p66;
	and.b32  	%r163, %r161, 255;
	add.s32 	%r164, %r163, 1;
	selp.b32 	%r198, %r164, %r161, %p67;

$L__BB1_45:
	shr.u64 	%rd56, %rd7, 63;
	cvt.u32.u64 	%r170, %rd56;
	shl.b32 	%r171, %r170, 7;
	or.b32  	%r172, %r198, %r171;
	add.s64 	%rd57, %rd1, %rd6;
	st.global.u8 	[%rd57], %r172;
	add.s32 	%r197, %r197, %r4;
	setp.lt.s32 	%p73, %r197, %r35;
	@%p73 bra 	$L__BB1_37;

$L__BB1_56:
	setp.ne.s32 	%p90, %r2, 0;
	@%p90 bra 	$L__BB1_58;

	cvta.to.global.u64 	%rd75, %rd16;
	mul.wide.s32 	%rd76, %r1, 4;
	add.s64 	%rd77, %rd75, %rd76;
	st.global.f32 	[%rd77], %f27;

$L__BB1_58:
	ret;

}

