//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32267302
// Cuda compilation tools, release 12.0, V12.0.140
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_89
.address_size 64

	// .globl	paged_attention_v1_bf16
.extern .shared .align 16 .b8 smem[];

.visible .entry paged_attention_v1_bf16(
	.param .u64 paged_attention_v1_bf16_param_0,
	.param .u64 paged_attention_v1_bf16_param_1,
	.param .u64 paged_attention_v1_bf16_param_2,
	.param .u64 paged_attention_v1_bf16_param_3,
	.param .u64 paged_attention_v1_bf16_param_4,
	.param .u64 paged_attention_v1_bf16_param_5,
	.param .f32 paged_attention_v1_bf16_param_6,
	.param .u32 paged_attention_v1_bf16_param_7,
	.param .u32 paged_attention_v1_bf16_param_8,
	.param .u32 paged_attention_v1_bf16_param_9
)
{
	.reg .pred 	%p<89>;
	.reg .b16 	%rs<13>;
	.reg .f32 	%f<235>;
	.reg .b32 	%r<330>;
	.reg .b64 	%rd<44>;


	ld.param.u64 	%rd7, [paged_attention_v1_bf16_param_1];
	ld.param.u64 	%rd8, [paged_attention_v1_bf16_param_2];
	ld.param.u64 	%rd9, [paged_attention_v1_bf16_param_3];
	ld.param.u64 	%rd10, [paged_attention_v1_bf16_param_4];
	ld.param.u64 	%rd11, [paged_attention_v1_bf16_param_5];
	ld.param.f32 	%f48, [paged_attention_v1_bf16_param_6];
	ld.param.u32 	%r85, [paged_attention_v1_bf16_param_7];
	ld.param.u32 	%r86, [paged_attention_v1_bf16_param_8];
	ld.param.u32 	%r87, [paged_attention_v1_bf16_param_9];
	cvta.to.global.u64 	%rd1, %rd8;
	cvta.to.global.u64 	%rd2, %rd9;
	cvta.to.global.u64 	%rd3, %rd10;
	mov.u32 	%r1, %tid.x;
	div.s32 	%r88, %r85, %r86;
	mov.u32 	%r2, %ctaid.x;
	div.s32 	%r3, %r2, %r88;
	mov.u32 	%r4, %ctaid.y;
	cvta.to.global.u64 	%rd12, %rd11;
	mul.wide.s32 	%rd13, %r4, 4;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.nc.u32 	%r5, [%rd14];
	setp.eq.s32 	%p3, %r5, 0;
	@%p3 bra 	$L__BB0_72;

	cvta.to.global.u64 	%rd15, %rd7;
	shl.b32 	%r6, %r86, 11;
	shl.b32 	%r7, %r86, 7;
	mad.lo.s32 	%r89, %r4, %r85, %r2;
	shl.b32 	%r90, %r89, 7;
	add.s32 	%r91, %r90, %r1;
	mul.wide.s32 	%rd16, %r91, 2;
	add.s64 	%rd17, %rd15, %rd16;
	ld.global.nc.u16 	%rs1, [%rd17];
	// begin inline asm
	{ mov.b32 %f49, {0,%rs1};}

	// end inline asm
	shl.b32 	%r92, %r1, 2;
	mov.u32 	%r93, smem;
	add.s32 	%r8, %r93, %r92;
	st.shared.f32 	[%r8], %f49;
	bar.sync 	0;
	add.s32 	%r94, %r5, 15;
	shr.s32 	%r95, %r94, 31;
	shr.u32 	%r96, %r95, 28;
	add.s32 	%r97, %r94, %r96;
	shr.s32 	%r9, %r97, 4;
	shr.u32 	%r10, %r1, 5;
	shl.b32 	%r98, %r10, 2;
	add.s32 	%r11, %r93, %r98;
	and.b32  	%r12, %r1, 31;
	and.b32  	%r99, %r92, 124;
	add.s32 	%r13, %r93, %r99;
	setp.lt.s32 	%p4, %r5, 1;
	mov.f32 	%f214, 0fFF7FFFFF;
	@%p4 bra 	$L__BB0_37;

	mul.lo.s32 	%r14, %r4, %r87;
	shl.b32 	%r101, %r3, 7;
	add.s32 	%r15, %r101, %r1;
	mov.u32 	%r102, 1;
	sub.s32 	%r16, %r102, %r5;
	not.b32 	%r17, %r5;
	mov.u32 	%r310, 0;

$L__BB0_3:
	shl.b32 	%r19, %r310, 4;
	add.s32 	%r103, %r19, %r17;
	max.s32 	%r20, %r103, -17;
	not.b32 	%r104, %r20;
	max.s32 	%r21, %r104, 1;
	setp.le.s32 	%p5, %r5, %r19;
	@%p5 bra 	$L__BB0_36;

	add.s32 	%r106, %r310, %r14;
	mul.wide.s32 	%rd18, %r106, 4;
	add.s64 	%rd19, %rd3, %rd18;
	ld.global.nc.u32 	%r107, [%rd19];
	mad.lo.s32 	%r22, %r107, %r6, %r15;
	and.b32  	%r23, %r21, 1;
	setp.gt.s32 	%p6, %r20, -3;
	mov.u32 	%r313, 0;
	@%p6 bra 	$L__BB0_25;

	sub.s32 	%r312, %r21, %r23;

$L__BB0_6:
	mad.lo.s32 	%r27, %r313, %r7, %r22;
	mul.wide.s32 	%rd20, %r27, 2;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.nc.u16 	%rs2, [%rd21];
	mov.u32 	%r109, 2;
	// begin inline asm
	{ mov.b32 %f53, {0,%rs2};}

	// end inline asm
	ld.shared.f32 	%f54, [%r8];
	mul.ftz.f32 	%f55, %f53, %f54;
	mov.b32 	%r110, %f55;
	mov.u32 	%r111, 31;
	mov.u32 	%r112, 16;
	mov.u32 	%r113, -1;
	shfl.sync.bfly.b32 	%r114|%p7, %r110, %r112, %r111, %r113;
	mov.b32 	%f56, %r114;
	add.ftz.f32 	%f57, %f55, %f56;
	mov.b32 	%r115, %f57;
	mov.u32 	%r116, 8;
	shfl.sync.bfly.b32 	%r117|%p8, %r115, %r116, %r111, %r113;
	mov.b32 	%f58, %r117;
	add.ftz.f32 	%f59, %f57, %f58;
	mov.b32 	%r118, %f59;
	mov.u32 	%r119, 4;
	shfl.sync.bfly.b32 	%r120|%p9, %r118, %r119, %r111, %r113;
	mov.b32 	%f60, %r120;
	add.ftz.f32 	%f61, %f59, %f60;
	mov.b32 	%r121, %f61;
	shfl.sync.bfly.b32 	%r122|%p10, %r121, %r109, %r111, %r113;
	mov.b32 	%f62, %r122;
	add.ftz.f32 	%f63, %f61, %f62;
	mov.b32 	%r123, %f63;
	mov.u32 	%r124, 1;
	shfl.sync.bfly.b32 	%r125|%p11, %r123, %r124, %r111, %r113;
	mov.b32 	%f64, %r125;
	add.ftz.f32 	%f3, %f63, %f64;
	setp.ne.s32 	%p12, %r12, 0;
	@%p12 bra 	$L__BB0_8;

	st.shared.f32 	[%r11+512], %f3;

$L__BB0_8:
	setp.ne.s32 	%p13, %r10, 0;
	bar.sync 	0;
	@%p13 bra 	$L__BB0_13;

	setp.gt.u32 	%p14, %r12, 3;
	mov.f32 	%f213, 0f00000000;
	@%p14 bra 	$L__BB0_11;

	ld.shared.f32 	%f213, [%r13+512];

$L__BB0_11:
	mov.b32 	%r126, %f213;
	mov.u32 	%r127, 31;
	mov.u32 	%r128, 16;
	mov.u32 	%r129, -1;
	shfl.sync.bfly.b32 	%r130|%p15, %r126, %r128, %r127, %r129;
	mov.b32 	%f66, %r130;
	add.ftz.f32 	%f67, %f213, %f66;
	mov.b32 	%r131, %f67;
	mov.u32 	%r132, 8;
	shfl.sync.bfly.b32 	%r133|%p16, %r131, %r132, %r127, %r129;
	mov.b32 	%f68, %r133;
	add.ftz.f32 	%f69, %f67, %f68;
	mov.b32 	%r134, %f69;
	mov.u32 	%r135, 4;
	shfl.sync.bfly.b32 	%r136|%p17, %r134, %r135, %r127, %r129;
	mov.b32 	%f70, %r136;
	add.ftz.f32 	%f71, %f69, %f70;
	mov.b32 	%r137, %f71;
	mov.u32 	%r138, 2;
	shfl.sync.bfly.b32 	%r139|%p18, %r137, %r138, %r127, %r129;
	mov.b32 	%f72, %r139;
	add.ftz.f32 	%f73, %f71, %f72;
	mov.b32 	%r140, %f73;
	mov.u32 	%r141, 1;
	shfl.sync.bfly.b32 	%r142|%p19, %r140, %r141, %r127, %r129;
	mov.b32 	%f74, %r142;
	add.ftz.f32 	%f6, %f73, %f74;
	@%p12 bra 	$L__BB0_13;

	st.shared.f32 	[smem+512], %f6;

$L__BB0_13:
	shl.b32 	%r298, %r310, 4;
	setp.ne.s32 	%p21, %r1, 0;
	bar.sync 	0;
	add.s32 	%r28, %r313, %r298;
	shl.b32 	%r143, %r28, 2;
	add.s32 	%r29, %r93, %r143;
	@%p21 bra 	$L__BB0_15;

	ld.shared.f32 	%f75, [smem+512];
	mul.ftz.f32 	%f76, %f75, %f48;
	add.s32 	%r145, %r16, %r28;
	cvt.rn.f32.s32 	%f77, %r145;
	fma.rn.ftz.f32 	%f78, %f77, 0f00000000, %f76;
	st.shared.f32 	[%r29+528], %f78;
	max.ftz.f32 	%f214, %f214, %f78;

$L__BB0_15:
	bar.sync 	0;
	add.s32 	%r146, %r27, %r7;
	mul.wide.s32 	%rd22, %r146, 2;
	add.s64 	%rd23, %rd1, %rd22;
	ld.global.nc.u16 	%rs3, [%rd23];
	mov.u32 	%r147, 2;
	// begin inline asm
	{ mov.b32 %f79, {0,%rs3};}

	// end inline asm
	ld.shared.f32 	%f80, [%r8];
	mul.ftz.f32 	%f81, %f79, %f80;
	mov.b32 	%r148, %f81;
	mov.u32 	%r149, 31;
	mov.u32 	%r150, 16;
	mov.u32 	%r151, -1;
	shfl.sync.bfly.b32 	%r152|%p22, %r148, %r150, %r149, %r151;
	mov.b32 	%f82, %r152;
	add.ftz.f32 	%f83, %f81, %f82;
	mov.b32 	%r153, %f83;
	mov.u32 	%r154, 8;
	shfl.sync.bfly.b32 	%r155|%p23, %r153, %r154, %r149, %r151;
	mov.b32 	%f84, %r155;
	add.ftz.f32 	%f85, %f83, %f84;
	mov.b32 	%r156, %f85;
	mov.u32 	%r157, 4;
	shfl.sync.bfly.b32 	%r158|%p24, %r156, %r157, %r149, %r151;
	mov.b32 	%f86, %r158;
	add.ftz.f32 	%f87, %f85, %f86;
	mov.b32 	%r159, %f87;
	shfl.sync.bfly.b32 	%r160|%p25, %r159, %r147, %r149, %r151;
	mov.b32 	%f88, %r160;
	add.ftz.f32 	%f9, %f87, %f88;
	mov.b32 	%r161, %f9;
	mov.u32 	%r162, 1;
	shfl.sync.bfly.b32 	%r30|%p1, %r161, %r162, %r149, %r151;
	@%p12 bra 	$L__BB0_17;

	mov.b32 	%f89, %r30;
	add.ftz.f32 	%f90, %f9, %f89;
	st.shared.f32 	[%r11+512], %f90;

$L__BB0_17:
	setp.ne.s32 	%p87, %r10, 0;
	bar.sync 	0;
	@%p87 bra 	$L__BB0_22;

	setp.gt.u32 	%p28, %r12, 3;
	mov.f32 	%f215, 0f00000000;
	@%p28 bra 	$L__BB0_20;

	ld.shared.f32 	%f215, [%r13+512];

$L__BB0_20:
	mov.b32 	%r163, %f215;
	mov.u32 	%r164, 31;
	mov.u32 	%r165, 16;
	mov.u32 	%r166, -1;
	shfl.sync.bfly.b32 	%r167|%p29, %r163, %r165, %r164, %r166;
	mov.b32 	%f92, %r167;
	add.ftz.f32 	%f93, %f215, %f92;
	mov.b32 	%r168, %f93;
	mov.u32 	%r169, 8;
	shfl.sync.bfly.b32 	%r170|%p30, %r168, %r169, %r164, %r166;
	mov.b32 	%f94, %r170;
	add.ftz.f32 	%f95, %f93, %f94;
	mov.b32 	%r171, %f95;
	mov.u32 	%r172, 4;
	shfl.sync.bfly.b32 	%r173|%p31, %r171, %r172, %r164, %r166;
	mov.b32 	%f96, %r173;
	add.ftz.f32 	%f97, %f95, %f96;
	mov.b32 	%r174, %f97;
	mov.u32 	%r175, 2;
	shfl.sync.bfly.b32 	%r176|%p32, %r174, %r175, %r164, %r166;
	mov.b32 	%f98, %r176;
	add.ftz.f32 	%f99, %f97, %f98;
	mov.b32 	%r177, %f99;
	mov.u32 	%r178, 1;
	shfl.sync.bfly.b32 	%r179|%p33, %r177, %r178, %r164, %r166;
	mov.b32 	%f100, %r179;
	add.ftz.f32 	%f12, %f99, %f100;
	@%p12 bra 	$L__BB0_22;

	st.shared.f32 	[smem+512], %f12;

$L__BB0_22:
	setp.ne.s32 	%p88, %r1, 0;
	bar.sync 	0;
	@%p88 bra 	$L__BB0_24;

	shl.b32 	%r309, %r310, 4;
	add.s32 	%r308, %r313, %r309;
	ld.shared.f32 	%f101, [smem+512];
	mul.ftz.f32 	%f102, %f101, %f48;
	add.s32 	%r181, %r16, %r308;
	add.s32 	%r182, %r181, 1;
	cvt.rn.f32.s32 	%f103, %r182;
	fma.rn.ftz.f32 	%f104, %f103, 0f00000000, %f102;
	st.shared.f32 	[%r29+532], %f104;
	max.ftz.f32 	%f214, %f214, %f104;

$L__BB0_24:
	bar.sync 	0;
	add.s32 	%r313, %r313, 2;
	add.s32 	%r312, %r312, -2;
	setp.ne.s32 	%p36, %r312, 0;
	@%p36 bra 	$L__BB0_6;

$L__BB0_25:
	setp.eq.s32 	%p37, %r23, 0;
	@%p37 bra 	$L__BB0_36;

	setp.ne.s32 	%p38, %r12, 0;
	mad.lo.s32 	%r183, %r313, %r7, %r22;
	mul.wide.s32 	%rd24, %r183, 2;
	add.s64 	%rd25, %rd1, %rd24;
	ld.global.nc.u16 	%rs4, [%rd25];
	mov.u32 	%r184, 2;
	// begin inline asm
	{ mov.b32 %f105, {0,%rs4};}

	// end inline asm
	ld.shared.f32 	%f106, [%r8];
	mul.ftz.f32 	%f107, %f105, %f106;
	mov.b32 	%r185, %f107;
	mov.u32 	%r186, 31;
	mov.u32 	%r187, 16;
	mov.u32 	%r188, -1;
	shfl.sync.bfly.b32 	%r189|%p39, %r185, %r187, %r186, %r188;
	mov.b32 	%f108, %r189;
	add.ftz.f32 	%f109, %f107, %f108;
	mov.b32 	%r190, %f109;
	mov.u32 	%r191, 8;
	shfl.sync.bfly.b32 	%r192|%p40, %r190, %r191, %r186, %r188;
	mov.b32 	%f110, %r192;
	add.ftz.f32 	%f111, %f109, %f110;
	mov.b32 	%r193, %f111;
	mov.u32 	%r194, 4;
	shfl.sync.bfly.b32 	%r195|%p41, %r193, %r194, %r186, %r188;
	mov.b32 	%f112, %r195;
	add.ftz.f32 	%f113, %f111, %f112;
	mov.b32 	%r196, %f113;
	shfl.sync.bfly.b32 	%r197|%p42, %r196, %r184, %r186, %r188;
	mov.b32 	%f114, %r197;
	add.ftz.f32 	%f115, %f113, %f114;
	mov.b32 	%r198, %f115;
	mov.u32 	%r199, 1;
	shfl.sync.bfly.b32 	%r200|%p43, %r198, %r199, %r186, %r188;
	mov.b32 	%f116, %r200;
	add.ftz.f32 	%f17, %f115, %f116;
	@%p38 bra 	$L__BB0_28;

	st.shared.f32 	[%r11+512], %f17;

$L__BB0_28:
	setp.ne.s32 	%p44, %r10, 0;
	bar.sync 	0;
	@%p44 bra 	$L__BB0_33;

	setp.gt.u32 	%p45, %r12, 3;
	mov.f32 	%f219, 0f00000000;
	@%p45 bra 	$L__BB0_31;

	ld.shared.f32 	%f219, [%r13+512];

$L__BB0_31:
	mov.b32 	%r201, %f219;
	mov.u32 	%r202, 31;
	mov.u32 	%r203, 16;
	mov.u32 	%r204, -1;
	shfl.sync.bfly.b32 	%r205|%p46, %r201, %r203, %r202, %r204;
	mov.b32 	%f118, %r205;
	add.ftz.f32 	%f119, %f219, %f118;
	mov.b32 	%r206, %f119;
	mov.u32 	%r207, 8;
	shfl.sync.bfly.b32 	%r208|%p47, %r206, %r207, %r202, %r204;
	mov.b32 	%f120, %r208;
	add.ftz.f32 	%f121, %f119, %f120;
	mov.b32 	%r209, %f121;
	mov.u32 	%r210, 4;
	shfl.sync.bfly.b32 	%r211|%p48, %r209, %r210, %r202, %r204;
	mov.b32 	%f122, %r211;
	add.ftz.f32 	%f123, %f121, %f122;
	mov.b32 	%r212, %f123;
	mov.u32 	%r213, 2;
	shfl.sync.bfly.b32 	%r214|%p49, %r212, %r213, %r202, %r204;
	mov.b32 	%f124, %r214;
	add.ftz.f32 	%f125, %f123, %f124;
	mov.b32 	%r215, %f125;
	mov.u32 	%r216, 1;
	shfl.sync.bfly.b32 	%r217|%p50, %r215, %r216, %r202, %r204;
	mov.b32 	%f126, %r217;
	add.ftz.f32 	%f20, %f125, %f126;
	@%p38 bra 	$L__BB0_33;

	st.shared.f32 	[smem+512], %f20;

$L__BB0_33:
	setp.ne.s32 	%p52, %r1, 0;
	bar.sync 	0;
	@%p52 bra 	$L__BB0_35;

	shl.b32 	%r299, %r310, 4;
	ld.shared.f32 	%f127, [smem+512];
	mul.ftz.f32 	%f128, %f127, %f48;
	add.s32 	%r219, %r313, %r299;
	add.s32 	%r220, %r16, %r219;
	cvt.rn.f32.s32 	%f129, %r220;
	fma.rn.ftz.f32 	%f130, %f129, 0f00000000, %f128;
	shl.b32 	%r221, %r219, 2;
	add.s32 	%r222, %r93, %r221;
	st.shared.f32 	[%r222+528], %f130;
	max.ftz.f32 	%f214, %f214, %f130;

$L__BB0_35:
	bar.sync 	0;

$L__BB0_36:
	add.s32 	%r310, %r310, 1;
	setp.lt.s32 	%p53, %r310, %r9;
	@%p53 bra 	$L__BB0_3;

$L__BB0_37:
	setp.ne.s32 	%p54, %r1, 0;
	@%p54 bra 	$L__BB0_39;

	st.shared.f32 	[smem+512], %f214;

$L__BB0_39:
	bar.sync 	0;
	ld.shared.f32 	%f25, [smem+512];
	setp.ge.s32 	%p55, %r1, %r5;
	mov.f32 	%f227, 0f00000000;
	@%p55 bra 	$L__BB0_46;

	not.b32 	%r223, %r1;
	add.s32 	%r35, %r5, %r223;
	shr.u32 	%r224, %r35, 7;
	add.s32 	%r225, %r224, 1;
	and.b32  	%r316, %r225, 3;
	setp.eq.s32 	%p56, %r316, 0;
	mov.f32 	%f227, 0f00000000;
	mov.u32 	%r317, %r1;
	@%p56 bra 	$L__BB0_43;

	add.s32 	%r314, %r8, 528;
	mov.u32 	%r317, %r1;

$L__BB0_42:
	.pragma "nounroll";
	ld.shared.f32 	%f135, [%r314];
	sub.ftz.f32 	%f136, %f135, %f25;
	mul.ftz.f32 	%f137, %f136, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f138, %f137;
	st.shared.f32 	[%r314], %f138;
	add.ftz.f32 	%f227, %f227, %f138;
	add.s32 	%r317, %r317, 128;
	add.s32 	%r314, %r314, 512;
	add.s32 	%r316, %r316, -1;
	setp.ne.s32 	%p57, %r316, 0;
	@%p57 bra 	$L__BB0_42;

$L__BB0_43:
	setp.lt.u32 	%p58, %r35, 384;
	@%p58 bra 	$L__BB0_46;

	shl.b32 	%r229, %r317, 2;
	add.s32 	%r231, %r93, %r229;
	add.s32 	%r318, %r231, 2064;

$L__BB0_45:
	ld.shared.f32 	%f139, [%r318+-1536];
	sub.ftz.f32 	%f140, %f139, %f25;
	mul.ftz.f32 	%f141, %f140, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f142, %f141;
	st.shared.f32 	[%r318+-1536], %f142;
	add.ftz.f32 	%f143, %f227, %f142;
	ld.shared.f32 	%f144, [%r318+-1024];
	sub.ftz.f32 	%f145, %f144, %f25;
	mul.ftz.f32 	%f146, %f145, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f147, %f146;
	st.shared.f32 	[%r318+-1024], %f147;
	add.ftz.f32 	%f148, %f143, %f147;
	ld.shared.f32 	%f149, [%r318+-512];
	sub.ftz.f32 	%f150, %f149, %f25;
	mul.ftz.f32 	%f151, %f150, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f152, %f151;
	st.shared.f32 	[%r318+-512], %f152;
	add.ftz.f32 	%f153, %f148, %f152;
	ld.shared.f32 	%f154, [%r318];
	sub.ftz.f32 	%f155, %f154, %f25;
	mul.ftz.f32 	%f156, %f155, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f157, %f156;
	st.shared.f32 	[%r318], %f157;
	add.ftz.f32 	%f227, %f153, %f157;
	add.s32 	%r318, %r318, 2048;
	add.s32 	%r317, %r317, 512;
	setp.lt.s32 	%p59, %r317, %r5;
	@%p59 bra 	$L__BB0_45;

$L__BB0_46:
	bar.sync 	0;
	mov.b32 	%r232, %f227;
	mov.u32 	%r233, 31;
	mov.u32 	%r234, 16;
	mov.u32 	%r235, -1;
	shfl.sync.bfly.b32 	%r236|%p60, %r232, %r234, %r233, %r235;
	mov.b32 	%f158, %r236;
	add.ftz.f32 	%f159, %f227, %f158;
	mov.b32 	%r237, %f159;
	mov.u32 	%r238, 8;
	shfl.sync.bfly.b32 	%r239|%p61, %r237, %r238, %r233, %r235;
	mov.b32 	%f160, %r239;
	add.ftz.f32 	%f161, %f159, %f160;
	mov.b32 	%r240, %f161;
	mov.u32 	%r241, 4;
	shfl.sync.bfly.b32 	%r242|%p62, %r240, %r241, %r233, %r235;
	mov.b32 	%f162, %r242;
	add.ftz.f32 	%f163, %f161, %f162;
	mov.b32 	%r243, %f163;
	mov.u32 	%r244, 2;
	shfl.sync.bfly.b32 	%r245|%p63, %r243, %r244, %r233, %r235;
	mov.b32 	%f164, %r245;
	add.ftz.f32 	%f33, %f163, %f164;
	mov.b32 	%r246, %f33;
	mov.u32 	%r247, 1;
	shfl.sync.bfly.b32 	%r50|%p2, %r246, %r247, %r233, %r235;
	setp.ne.s32 	%p64, %r12, 0;
	@%p64 bra 	$L__BB0_48;

	mov.b32 	%f165, %r50;
	add.ftz.f32 	%f166, %f33, %f165;
	st.shared.f32 	[%r11+512], %f166;

$L__BB0_48:
	bar.sync 	0;
	setp.ne.s32 	%p65, %r10, 0;
	@%p65 bra 	$L__BB0_53;

	setp.gt.u32 	%p66, %r12, 3;
	mov.f32 	%f228, 0f00000000;
	@%p66 bra 	$L__BB0_51;

	ld.shared.f32 	%f228, [%r13+512];

$L__BB0_51:
	mov.b32 	%r249, %f228;
	mov.u32 	%r250, 31;
	mov.u32 	%r251, 16;
	mov.u32 	%r252, -1;
	shfl.sync.bfly.b32 	%r253|%p67, %r249, %r251, %r250, %r252;
	mov.b32 	%f168, %r253;
	add.ftz.f32 	%f169, %f228, %f168;
	mov.b32 	%r254, %f169;
	mov.u32 	%r255, 8;
	shfl.sync.bfly.b32 	%r256|%p68, %r254, %r255, %r250, %r252;
	mov.b32 	%f170, %r256;
	add.ftz.f32 	%f171, %f169, %f170;
	mov.b32 	%r257, %f171;
	mov.u32 	%r258, 4;
	shfl.sync.bfly.b32 	%r259|%p69, %r257, %r258, %r250, %r252;
	mov.b32 	%f172, %r259;
	add.ftz.f32 	%f173, %f171, %f172;
	mov.b32 	%r260, %f173;
	mov.u32 	%r261, 2;
	shfl.sync.bfly.b32 	%r262|%p70, %r260, %r261, %r250, %r252;
	mov.b32 	%f174, %r262;
	add.ftz.f32 	%f175, %f173, %f174;
	mov.b32 	%r263, %f175;
	mov.u32 	%r264, 1;
	shfl.sync.bfly.b32 	%r265|%p71, %r263, %r264, %r250, %r252;
	mov.b32 	%f176, %r265;
	add.ftz.f32 	%f36, %f175, %f176;
	@%p64 bra 	$L__BB0_53;

	st.shared.f32 	[smem+512], %f36;

$L__BB0_53:
	bar.sync 	0;
	ld.shared.f32 	%f177, [smem+512];
	add.ftz.f32 	%f178, %f177, 0f358637BD;
	mov.f32 	%f179, 0f3F800000;
	div.approx.ftz.f32 	%f37, %f179, %f178;
	@%p55 bra 	$L__BB0_60;

	not.b32 	%r266, %r1;
	add.s32 	%r52, %r5, %r266;
	shr.u32 	%r267, %r52, 7;
	add.s32 	%r268, %r267, 1;
	and.b32  	%r322, %r268, 3;
	setp.eq.s32 	%p74, %r322, 0;
	mov.u32 	%r323, %r1;
	@%p74 bra 	$L__BB0_57;

	add.s32 	%r320, %r8, 528;
	mov.u32 	%r323, %r1;

$L__BB0_56:
	.pragma "nounroll";
	ld.shared.f32 	%f180, [%r320];
	mul.ftz.f32 	%f181, %f37, %f180;
	st.shared.f32 	[%r320], %f181;
	add.s32 	%r323, %r323, 128;
	add.s32 	%r320, %r320, 512;
	add.s32 	%r322, %r322, -1;
	setp.ne.s32 	%p75, %r322, 0;
	@%p75 bra 	$L__BB0_56;

$L__BB0_57:
	setp.lt.u32 	%p76, %r52, 384;
	@%p76 bra 	$L__BB0_60;

	shl.b32 	%r272, %r323, 2;
	add.s32 	%r274, %r93, %r272;
	add.s32 	%r324, %r274, 2064;

$L__BB0_59:
	ld.shared.f32 	%f182, [%r324+-1536];
	mul.ftz.f32 	%f183, %f37, %f182;
	st.shared.f32 	[%r324+-1536], %f183;
	ld.shared.f32 	%f184, [%r324+-1024];
	mul.ftz.f32 	%f185, %f37, %f184;
	st.shared.f32 	[%r324+-1024], %f185;
	ld.shared.f32 	%f186, [%r324+-512];
	mul.ftz.f32 	%f187, %f37, %f186;
	st.shared.f32 	[%r324+-512], %f187;
	ld.shared.f32 	%f188, [%r324];
	mul.ftz.f32 	%f189, %f37, %f188;
	st.shared.f32 	[%r324], %f189;
	add.s32 	%r324, %r324, 2048;
	add.s32 	%r323, %r323, 512;
	setp.lt.s32 	%p77, %r323, %r5;
	@%p77 bra 	$L__BB0_59;

$L__BB0_60:
	setp.lt.s32 	%p86, %r5, 1;
	bar.sync 	0;
	mov.f32 	%f233, 0f00000000;
	@%p86 bra 	$L__BB0_71;

	ld.param.u32 	%r307, [paged_attention_v1_bf16_param_9];
	mov.u32 	%r306, %ctaid.y;
	mul.lo.s32 	%r67, %r306, %r307;
	shl.b32 	%r276, %r3, 7;
	add.s32 	%r68, %r276, %r1;
	not.b32 	%r69, %r5;
	mul.wide.s32 	%rd5, %r7, 2;
	mov.u32 	%r326, 0;

$L__BB0_62:
	shl.b32 	%r71, %r326, 4;
	add.s32 	%r278, %r71, %r69;
	max.s32 	%r279, %r278, -17;
	not.b32 	%r280, %r279;
	max.s32 	%r72, %r280, 1;
	setp.le.s32 	%p79, %r5, %r71;
	@%p79 bra 	$L__BB0_70;

	add.s32 	%r282, %r72, -1;
	add.s32 	%r283, %r326, %r67;
	mul.wide.s32 	%rd26, %r283, 4;
	add.s64 	%rd27, %rd3, %rd26;
	ld.global.nc.u32 	%r284, [%rd27];
	mad.lo.s32 	%r73, %r284, %r6, %r68;
	and.b32  	%r74, %r72, 3;
	setp.lt.u32 	%p80, %r282, 3;
	mov.u32 	%r329, 0;
	@%p80 bra 	$L__BB0_66;

	sub.s32 	%r328, %r72, %r74;

$L__BB0_65:
	mad.lo.s32 	%r286, %r329, %r7, %r73;
	mul.wide.s32 	%rd28, %r286, 2;
	add.s64 	%rd29, %rd2, %rd28;
	ld.global.nc.u16 	%rs5, [%rd29];
	// begin inline asm
	{ mov.b32 %f193, {0,%rs5};}

	// end inline asm
	add.s32 	%r287, %r71, %r329;
	shl.b32 	%r288, %r287, 2;
	add.s32 	%r290, %r93, %r288;
	ld.shared.f32 	%f197, [%r290+528];
	fma.rn.ftz.f32 	%f198, %f193, %f197, %f233;
	add.s64 	%rd30, %rd29, %rd5;
	ld.global.nc.u16 	%rs6, [%rd30];
	// begin inline asm
	{ mov.b32 %f194, {0,%rs6};}

	// end inline asm
	ld.shared.f32 	%f199, [%r290+532];
	fma.rn.ftz.f32 	%f200, %f194, %f199, %f198;
	add.s64 	%rd31, %rd30, %rd5;
	ld.global.nc.u16 	%rs7, [%rd31];
	// begin inline asm
	{ mov.b32 %f195, {0,%rs7};}

	// end inline asm
	ld.shared.f32 	%f201, [%r290+536];
	fma.rn.ftz.f32 	%f202, %f195, %f201, %f200;
	add.s64 	%rd32, %rd31, %rd5;
	ld.global.nc.u16 	%rs8, [%rd32];
	// begin inline asm
	{ mov.b32 %f196, {0,%rs8};}

	// end inline asm
	ld.shared.f32 	%f203, [%r290+540];
	fma.rn.ftz.f32 	%f233, %f196, %f203, %f202;
	add.s32 	%r329, %r329, 4;
	add.s32 	%r328, %r328, -4;
	setp.ne.s32 	%p81, %r328, 0;
	@%p81 bra 	$L__BB0_65;

$L__BB0_66:
	setp.eq.s32 	%p82, %r74, 0;
	@%p82 bra 	$L__BB0_70;

	mad.lo.s32 	%r81, %r329, %r7, %r73;
	mul.wide.s32 	%rd33, %r81, 2;
	add.s64 	%rd34, %rd2, %rd33;
	ld.global.nc.u16 	%rs9, [%rd34];
	// begin inline asm
	{ mov.b32 %f204, {0,%rs9};}

	// end inline asm
	add.s32 	%r291, %r329, %r71;
	shl.b32 	%r292, %r291, 2;
	add.s32 	%r294, %r93, %r292;
	ld.shared.f32 	%f205, [%r294+528];
	fma.rn.ftz.f32 	%f233, %f204, %f205, %f233;
	setp.eq.s32 	%p83, %r74, 1;
	@%p83 bra 	$L__BB0_70;

	add.s32 	%r83, %r81, %r7;
	mul.wide.s32 	%rd35, %r83, 2;
	add.s64 	%rd36, %rd2, %rd35;
	ld.global.nc.u16 	%rs10, [%rd36];
	// begin inline asm
	{ mov.b32 %f206, {0,%rs10};}

	// end inline asm
	add.s32 	%r296, %r294, 528;
	ld.shared.f32 	%f207, [%r296+4];
	fma.rn.ftz.f32 	%f233, %f206, %f207, %f233;
	setp.eq.s32 	%p84, %r74, 2;
	@%p84 bra 	$L__BB0_70;

	add.s32 	%r295, %r83, %r7;
	mul.wide.s32 	%rd37, %r295, 2;
	add.s64 	%rd38, %rd2, %rd37;
	ld.global.nc.u16 	%rs11, [%rd38];
	// begin inline asm
	{ mov.b32 %f208, {0,%rs11};}

	// end inline asm
	add.s32 	%r297, %r294, 528;
	ld.shared.f32 	%f209, [%r297+8];
	fma.rn.ftz.f32 	%f233, %f208, %f209, %f233;

$L__BB0_70:
	add.s32 	%r326, %r326, 1;
	setp.lt.s32 	%p85, %r326, %r9;
	@%p85 bra 	$L__BB0_62;

$L__BB0_71:
	mov.u32 	%r305, %ctaid.x;
	ld.param.u32 	%r304, [paged_attention_v1_bf16_param_7];
	mov.u32 	%r303, %ctaid.y;
	mad.lo.s32 	%r302, %r303, %r304, %r305;
	shl.b32 	%r301, %r302, 7;
	add.s32 	%r300, %r301, %r1;
	cvt.s64.s32 	%rd43, %r300;
	ld.param.u64 	%rd42, [paged_attention_v1_bf16_param_0];
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs12, %f233;}

	// end inline asm
	cvta.to.global.u64 	%rd39, %rd42;
	shl.b64 	%rd40, %rd43, 1;
	add.s64 	%rd41, %rd39, %rd40;
	st.global.u16 	[%rd41], %rs12;

$L__BB0_72:
	ret;

}
	// .globl	paged_attention_v1_bf16_alibi
.visible .entry paged_attention_v1_bf16_alibi(
	.param .u64 paged_attention_v1_bf16_alibi_param_0,
	.param .u64 paged_attention_v1_bf16_alibi_param_1,
	.param .u64 paged_attention_v1_bf16_alibi_param_2,
	.param .u64 paged_attention_v1_bf16_alibi_param_3,
	.param .u64 paged_attention_v1_bf16_alibi_param_4,
	.param .u64 paged_attention_v1_bf16_alibi_param_5,
	.param .f32 paged_attention_v1_bf16_alibi_param_6,
	.param .u32 paged_attention_v1_bf16_alibi_param_7,
	.param .u32 paged_attention_v1_bf16_alibi_param_8,
	.param .u32 paged_attention_v1_bf16_alibi_param_9,
	.param .u64 paged_attention_v1_bf16_alibi_param_10
)
{
	.reg .pred 	%p<90>;
	.reg .b16 	%rs<13>;
	.reg .f32 	%f<239>;
	.reg .b32 	%r<332>;
	.reg .b64 	%rd<48>;


	ld.param.u64 	%rd7, [paged_attention_v1_bf16_alibi_param_1];
	ld.param.u64 	%rd9, [paged_attention_v1_bf16_alibi_param_2];
	ld.param.u64 	%rd10, [paged_attention_v1_bf16_alibi_param_3];
	ld.param.u64 	%rd11, [paged_attention_v1_bf16_alibi_param_4];
	ld.param.u64 	%rd12, [paged_attention_v1_bf16_alibi_param_5];
	ld.param.f32 	%f50, [paged_attention_v1_bf16_alibi_param_6];
	ld.param.u32 	%r85, [paged_attention_v1_bf16_alibi_param_7];
	ld.param.u32 	%r86, [paged_attention_v1_bf16_alibi_param_8];
	ld.param.u32 	%r87, [paged_attention_v1_bf16_alibi_param_9];
	ld.param.u64 	%rd8, [paged_attention_v1_bf16_alibi_param_10];
	cvta.to.global.u64 	%rd1, %rd9;
	cvta.to.global.u64 	%rd2, %rd10;
	cvta.to.global.u64 	%rd3, %rd11;
	mov.u32 	%r1, %tid.x;
	div.s32 	%r88, %r85, %r86;
	mov.u32 	%r2, %ctaid.x;
	div.s32 	%r3, %r2, %r88;
	mov.u32 	%r4, %ctaid.y;
	cvta.to.global.u64 	%rd13, %rd12;
	mul.wide.s32 	%rd14, %r4, 4;
	add.s64 	%rd15, %rd13, %rd14;
	ld.global.nc.u32 	%r5, [%rd15];
	setp.eq.s32 	%p3, %r5, 0;
	@%p3 bra 	$L__BB1_74;

	setp.eq.s64 	%p4, %rd8, 0;
	mov.f32 	%f214, 0f00000000;
	@%p4 bra 	$L__BB1_3;

	cvta.to.global.u64 	%rd16, %rd8;
	mul.wide.s32 	%rd17, %r2, 4;
	add.s64 	%rd18, %rd16, %rd17;
	ld.global.nc.f32 	%f214, [%rd18];

$L__BB1_3:
	shl.b32 	%r6, %r86, 11;
	shl.b32 	%r7, %r86, 7;
	mad.lo.s32 	%r89, %r4, %r85, %r2;
	shl.b32 	%r90, %r89, 7;
	add.s32 	%r91, %r90, %r1;
	cvta.to.global.u64 	%rd19, %rd7;
	mul.wide.s32 	%rd20, %r91, 2;
	add.s64 	%rd21, %rd19, %rd20;
	ld.global.nc.u16 	%rs1, [%rd21];
	// begin inline asm
	{ mov.b32 %f52, {0,%rs1};}

	// end inline asm
	shl.b32 	%r92, %r1, 2;
	mov.u32 	%r93, smem;
	add.s32 	%r8, %r93, %r92;
	st.shared.f32 	[%r8], %f52;
	bar.sync 	0;
	add.s32 	%r94, %r5, 15;
	shr.s32 	%r95, %r94, 31;
	shr.u32 	%r96, %r95, 28;
	add.s32 	%r97, %r94, %r96;
	shr.s32 	%r9, %r97, 4;
	shr.u32 	%r10, %r1, 5;
	shl.b32 	%r98, %r10, 2;
	add.s32 	%r11, %r93, %r98;
	and.b32  	%r12, %r1, 31;
	and.b32  	%r99, %r92, 124;
	add.s32 	%r13, %r93, %r99;
	setp.lt.s32 	%p5, %r5, 1;
	mov.f32 	%f218, 0fFF7FFFFF;
	@%p5 bra 	$L__BB1_39;

	mul.lo.s32 	%r14, %r4, %r87;
	shl.b32 	%r101, %r3, 7;
	add.s32 	%r15, %r101, %r1;
	mov.u32 	%r102, 1;
	sub.s32 	%r16, %r102, %r5;
	not.b32 	%r17, %r5;
	mov.u32 	%r312, 0;

$L__BB1_5:
	shl.b32 	%r19, %r312, 4;
	add.s32 	%r103, %r19, %r17;
	max.s32 	%r20, %r103, -17;
	not.b32 	%r104, %r20;
	max.s32 	%r21, %r104, 1;
	setp.le.s32 	%p6, %r5, %r19;
	@%p6 bra 	$L__BB1_38;

	add.s32 	%r106, %r312, %r14;
	mul.wide.s32 	%rd22, %r106, 4;
	add.s64 	%rd23, %rd3, %rd22;
	ld.global.nc.u32 	%r107, [%rd23];
	mad.lo.s32 	%r22, %r107, %r6, %r15;
	and.b32  	%r23, %r21, 1;
	setp.gt.s32 	%p7, %r20, -3;
	mov.u32 	%r315, 0;
	@%p7 bra 	$L__BB1_27;

	sub.s32 	%r314, %r21, %r23;

$L__BB1_8:
	mad.lo.s32 	%r27, %r315, %r7, %r22;
	mul.wide.s32 	%rd24, %r27, 2;
	add.s64 	%rd25, %rd1, %rd24;
	ld.global.nc.u16 	%rs2, [%rd25];
	mov.u32 	%r109, 2;
	// begin inline asm
	{ mov.b32 %f56, {0,%rs2};}

	// end inline asm
	ld.shared.f32 	%f57, [%r8];
	mul.ftz.f32 	%f58, %f56, %f57;
	mov.b32 	%r110, %f58;
	mov.u32 	%r111, 31;
	mov.u32 	%r112, 16;
	mov.u32 	%r113, -1;
	shfl.sync.bfly.b32 	%r114|%p8, %r110, %r112, %r111, %r113;
	mov.b32 	%f59, %r114;
	add.ftz.f32 	%f60, %f58, %f59;
	mov.b32 	%r115, %f60;
	mov.u32 	%r116, 8;
	shfl.sync.bfly.b32 	%r117|%p9, %r115, %r116, %r111, %r113;
	mov.b32 	%f61, %r117;
	add.ftz.f32 	%f62, %f60, %f61;
	mov.b32 	%r118, %f62;
	mov.u32 	%r119, 4;
	shfl.sync.bfly.b32 	%r120|%p10, %r118, %r119, %r111, %r113;
	mov.b32 	%f63, %r120;
	add.ftz.f32 	%f64, %f62, %f63;
	mov.b32 	%r121, %f64;
	shfl.sync.bfly.b32 	%r122|%p11, %r121, %r109, %r111, %r113;
	mov.b32 	%f65, %r122;
	add.ftz.f32 	%f66, %f64, %f65;
	mov.b32 	%r123, %f66;
	mov.u32 	%r124, 1;
	shfl.sync.bfly.b32 	%r125|%p12, %r123, %r124, %r111, %r113;
	mov.b32 	%f67, %r125;
	add.ftz.f32 	%f5, %f66, %f67;
	setp.ne.s32 	%p13, %r12, 0;
	@%p13 bra 	$L__BB1_10;

	st.shared.f32 	[%r11+512], %f5;

$L__BB1_10:
	setp.ne.s32 	%p14, %r10, 0;
	bar.sync 	0;
	@%p14 bra 	$L__BB1_15;

	setp.gt.u32 	%p15, %r12, 3;
	mov.f32 	%f217, 0f00000000;
	@%p15 bra 	$L__BB1_13;

	ld.shared.f32 	%f217, [%r13+512];

$L__BB1_13:
	mov.b32 	%r126, %f217;
	mov.u32 	%r127, 31;
	mov.u32 	%r128, 16;
	mov.u32 	%r129, -1;
	shfl.sync.bfly.b32 	%r130|%p16, %r126, %r128, %r127, %r129;
	mov.b32 	%f69, %r130;
	add.ftz.f32 	%f70, %f217, %f69;
	mov.b32 	%r131, %f70;
	mov.u32 	%r132, 8;
	shfl.sync.bfly.b32 	%r133|%p17, %r131, %r132, %r127, %r129;
	mov.b32 	%f71, %r133;
	add.ftz.f32 	%f72, %f70, %f71;
	mov.b32 	%r134, %f72;
	mov.u32 	%r135, 4;
	shfl.sync.bfly.b32 	%r136|%p18, %r134, %r135, %r127, %r129;
	mov.b32 	%f73, %r136;
	add.ftz.f32 	%f74, %f72, %f73;
	mov.b32 	%r137, %f74;
	mov.u32 	%r138, 2;
	shfl.sync.bfly.b32 	%r139|%p19, %r137, %r138, %r127, %r129;
	mov.b32 	%f75, %r139;
	add.ftz.f32 	%f76, %f74, %f75;
	mov.b32 	%r140, %f76;
	mov.u32 	%r141, 1;
	shfl.sync.bfly.b32 	%r142|%p20, %r140, %r141, %r127, %r129;
	mov.b32 	%f77, %r142;
	add.ftz.f32 	%f8, %f76, %f77;
	@%p13 bra 	$L__BB1_15;

	st.shared.f32 	[smem+512], %f8;

$L__BB1_15:
	shl.b32 	%r298, %r312, 4;
	setp.ne.s32 	%p22, %r1, 0;
	bar.sync 	0;
	add.s32 	%r28, %r315, %r298;
	shl.b32 	%r143, %r28, 2;
	add.s32 	%r29, %r93, %r143;
	@%p22 bra 	$L__BB1_17;

	ld.shared.f32 	%f78, [smem+512];
	mul.ftz.f32 	%f79, %f78, %f50;
	add.s32 	%r145, %r16, %r28;
	cvt.rn.f32.s32 	%f80, %r145;
	fma.rn.ftz.f32 	%f81, %f214, %f80, %f79;
	st.shared.f32 	[%r29+528], %f81;
	max.ftz.f32 	%f218, %f218, %f81;

$L__BB1_17:
	bar.sync 	0;
	add.s32 	%r146, %r27, %r7;
	mul.wide.s32 	%rd26, %r146, 2;
	add.s64 	%rd27, %rd1, %rd26;
	ld.global.nc.u16 	%rs3, [%rd27];
	mov.u32 	%r147, 2;
	// begin inline asm
	{ mov.b32 %f82, {0,%rs3};}

	// end inline asm
	ld.shared.f32 	%f83, [%r8];
	mul.ftz.f32 	%f84, %f82, %f83;
	mov.b32 	%r148, %f84;
	mov.u32 	%r149, 31;
	mov.u32 	%r150, 16;
	mov.u32 	%r151, -1;
	shfl.sync.bfly.b32 	%r152|%p23, %r148, %r150, %r149, %r151;
	mov.b32 	%f85, %r152;
	add.ftz.f32 	%f86, %f84, %f85;
	mov.b32 	%r153, %f86;
	mov.u32 	%r154, 8;
	shfl.sync.bfly.b32 	%r155|%p24, %r153, %r154, %r149, %r151;
	mov.b32 	%f87, %r155;
	add.ftz.f32 	%f88, %f86, %f87;
	mov.b32 	%r156, %f88;
	mov.u32 	%r157, 4;
	shfl.sync.bfly.b32 	%r158|%p25, %r156, %r157, %r149, %r151;
	mov.b32 	%f89, %r158;
	add.ftz.f32 	%f90, %f88, %f89;
	mov.b32 	%r159, %f90;
	shfl.sync.bfly.b32 	%r160|%p26, %r159, %r147, %r149, %r151;
	mov.b32 	%f91, %r160;
	add.ftz.f32 	%f11, %f90, %f91;
	mov.b32 	%r161, %f11;
	mov.u32 	%r162, 1;
	shfl.sync.bfly.b32 	%r30|%p1, %r161, %r162, %r149, %r151;
	@%p13 bra 	$L__BB1_19;

	mov.b32 	%f92, %r30;
	add.ftz.f32 	%f93, %f11, %f92;
	st.shared.f32 	[%r11+512], %f93;

$L__BB1_19:
	setp.ne.s32 	%p88, %r10, 0;
	bar.sync 	0;
	@%p88 bra 	$L__BB1_24;

	setp.gt.u32 	%p29, %r12, 3;
	mov.f32 	%f219, 0f00000000;
	@%p29 bra 	$L__BB1_22;

	ld.shared.f32 	%f219, [%r13+512];

$L__BB1_22:
	mov.b32 	%r163, %f219;
	mov.u32 	%r164, 31;
	mov.u32 	%r165, 16;
	mov.u32 	%r166, -1;
	shfl.sync.bfly.b32 	%r167|%p30, %r163, %r165, %r164, %r166;
	mov.b32 	%f95, %r167;
	add.ftz.f32 	%f96, %f219, %f95;
	mov.b32 	%r168, %f96;
	mov.u32 	%r169, 8;
	shfl.sync.bfly.b32 	%r170|%p31, %r168, %r169, %r164, %r166;
	mov.b32 	%f97, %r170;
	add.ftz.f32 	%f98, %f96, %f97;
	mov.b32 	%r171, %f98;
	mov.u32 	%r172, 4;
	shfl.sync.bfly.b32 	%r173|%p32, %r171, %r172, %r164, %r166;
	mov.b32 	%f99, %r173;
	add.ftz.f32 	%f100, %f98, %f99;
	mov.b32 	%r174, %f100;
	mov.u32 	%r175, 2;
	shfl.sync.bfly.b32 	%r176|%p33, %r174, %r175, %r164, %r166;
	mov.b32 	%f101, %r176;
	add.ftz.f32 	%f102, %f100, %f101;
	mov.b32 	%r177, %f102;
	mov.u32 	%r178, 1;
	shfl.sync.bfly.b32 	%r179|%p34, %r177, %r178, %r164, %r166;
	mov.b32 	%f103, %r179;
	add.ftz.f32 	%f14, %f102, %f103;
	@%p13 bra 	$L__BB1_24;

	st.shared.f32 	[smem+512], %f14;

$L__BB1_24:
	setp.ne.s32 	%p89, %r1, 0;
	bar.sync 	0;
	@%p89 bra 	$L__BB1_26;

	shl.b32 	%r311, %r312, 4;
	add.s32 	%r310, %r315, %r311;
	shl.b32 	%r309, %r310, 2;
	add.s32 	%r308, %r93, %r309;
	ld.shared.f32 	%f104, [smem+512];
	mul.ftz.f32 	%f105, %f104, %f50;
	add.s32 	%r181, %r16, %r310;
	add.s32 	%r182, %r181, 1;
	cvt.rn.f32.s32 	%f106, %r182;
	fma.rn.ftz.f32 	%f107, %f214, %f106, %f105;
	st.shared.f32 	[%r308+532], %f107;
	max.ftz.f32 	%f218, %f218, %f107;

$L__BB1_26:
	bar.sync 	0;
	add.s32 	%r315, %r315, 2;
	add.s32 	%r314, %r314, -2;
	setp.ne.s32 	%p37, %r314, 0;
	@%p37 bra 	$L__BB1_8;

$L__BB1_27:
	setp.eq.s32 	%p38, %r23, 0;
	@%p38 bra 	$L__BB1_38;

	setp.ne.s32 	%p39, %r12, 0;
	mad.lo.s32 	%r183, %r315, %r7, %r22;
	mul.wide.s32 	%rd28, %r183, 2;
	add.s64 	%rd29, %rd1, %rd28;
	ld.global.nc.u16 	%rs4, [%rd29];
	mov.u32 	%r184, 2;
	// begin inline asm
	{ mov.b32 %f108, {0,%rs4};}

	// end inline asm
	ld.shared.f32 	%f109, [%r8];
	mul.ftz.f32 	%f110, %f108, %f109;
	mov.b32 	%r185, %f110;
	mov.u32 	%r186, 31;
	mov.u32 	%r187, 16;
	mov.u32 	%r188, -1;
	shfl.sync.bfly.b32 	%r189|%p40, %r185, %r187, %r186, %r188;
	mov.b32 	%f111, %r189;
	add.ftz.f32 	%f112, %f110, %f111;
	mov.b32 	%r190, %f112;
	mov.u32 	%r191, 8;
	shfl.sync.bfly.b32 	%r192|%p41, %r190, %r191, %r186, %r188;
	mov.b32 	%f113, %r192;
	add.ftz.f32 	%f114, %f112, %f113;
	mov.b32 	%r193, %f114;
	mov.u32 	%r194, 4;
	shfl.sync.bfly.b32 	%r195|%p42, %r193, %r194, %r186, %r188;
	mov.b32 	%f115, %r195;
	add.ftz.f32 	%f116, %f114, %f115;
	mov.b32 	%r196, %f116;
	shfl.sync.bfly.b32 	%r197|%p43, %r196, %r184, %r186, %r188;
	mov.b32 	%f117, %r197;
	add.ftz.f32 	%f118, %f116, %f117;
	mov.b32 	%r198, %f118;
	mov.u32 	%r199, 1;
	shfl.sync.bfly.b32 	%r200|%p44, %r198, %r199, %r186, %r188;
	mov.b32 	%f119, %r200;
	add.ftz.f32 	%f19, %f118, %f119;
	@%p39 bra 	$L__BB1_30;

	st.shared.f32 	[%r11+512], %f19;

$L__BB1_30:
	setp.ne.s32 	%p45, %r10, 0;
	bar.sync 	0;
	@%p45 bra 	$L__BB1_35;

	setp.gt.u32 	%p46, %r12, 3;
	mov.f32 	%f223, 0f00000000;
	@%p46 bra 	$L__BB1_33;

	ld.shared.f32 	%f223, [%r13+512];

$L__BB1_33:
	mov.b32 	%r201, %f223;
	mov.u32 	%r202, 31;
	mov.u32 	%r203, 16;
	mov.u32 	%r204, -1;
	shfl.sync.bfly.b32 	%r205|%p47, %r201, %r203, %r202, %r204;
	mov.b32 	%f121, %r205;
	add.ftz.f32 	%f122, %f223, %f121;
	mov.b32 	%r206, %f122;
	mov.u32 	%r207, 8;
	shfl.sync.bfly.b32 	%r208|%p48, %r206, %r207, %r202, %r204;
	mov.b32 	%f123, %r208;
	add.ftz.f32 	%f124, %f122, %f123;
	mov.b32 	%r209, %f124;
	mov.u32 	%r210, 4;
	shfl.sync.bfly.b32 	%r211|%p49, %r209, %r210, %r202, %r204;
	mov.b32 	%f125, %r211;
	add.ftz.f32 	%f126, %f124, %f125;
	mov.b32 	%r212, %f126;
	mov.u32 	%r213, 2;
	shfl.sync.bfly.b32 	%r214|%p50, %r212, %r213, %r202, %r204;
	mov.b32 	%f127, %r214;
	add.ftz.f32 	%f128, %f126, %f127;
	mov.b32 	%r215, %f128;
	mov.u32 	%r216, 1;
	shfl.sync.bfly.b32 	%r217|%p51, %r215, %r216, %r202, %r204;
	mov.b32 	%f129, %r217;
	add.ftz.f32 	%f22, %f128, %f129;
	@%p39 bra 	$L__BB1_35;

	st.shared.f32 	[smem+512], %f22;

$L__BB1_35:
	setp.ne.s32 	%p53, %r1, 0;
	bar.sync 	0;
	@%p53 bra 	$L__BB1_37;

	shl.b32 	%r299, %r312, 4;
	ld.shared.f32 	%f130, [smem+512];
	mul.ftz.f32 	%f131, %f130, %f50;
	add.s32 	%r219, %r315, %r299;
	add.s32 	%r220, %r16, %r219;
	cvt.rn.f32.s32 	%f132, %r220;
	fma.rn.ftz.f32 	%f133, %f214, %f132, %f131;
	shl.b32 	%r221, %r219, 2;
	add.s32 	%r222, %r93, %r221;
	st.shared.f32 	[%r222+528], %f133;
	max.ftz.f32 	%f218, %f218, %f133;

$L__BB1_37:
	bar.sync 	0;

$L__BB1_38:
	add.s32 	%r312, %r312, 1;
	setp.lt.s32 	%p54, %r312, %r9;
	@%p54 bra 	$L__BB1_5;

$L__BB1_39:
	setp.ne.s32 	%p55, %r1, 0;
	@%p55 bra 	$L__BB1_41;

	st.shared.f32 	[smem+512], %f218;

$L__BB1_41:
	bar.sync 	0;
	ld.shared.f32 	%f27, [smem+512];
	setp.ge.s32 	%p56, %r1, %r5;
	mov.f32 	%f231, 0f00000000;
	@%p56 bra 	$L__BB1_48;

	not.b32 	%r223, %r1;
	add.s32 	%r35, %r5, %r223;
	shr.u32 	%r224, %r35, 7;
	add.s32 	%r225, %r224, 1;
	and.b32  	%r318, %r225, 3;
	setp.eq.s32 	%p57, %r318, 0;
	mov.f32 	%f231, 0f00000000;
	mov.u32 	%r319, %r1;
	@%p57 bra 	$L__BB1_45;

	add.s32 	%r316, %r8, 528;
	mov.u32 	%r319, %r1;

$L__BB1_44:
	.pragma "nounroll";
	ld.shared.f32 	%f138, [%r316];
	sub.ftz.f32 	%f139, %f138, %f27;
	mul.ftz.f32 	%f140, %f139, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f141, %f140;
	st.shared.f32 	[%r316], %f141;
	add.ftz.f32 	%f231, %f231, %f141;
	add.s32 	%r319, %r319, 128;
	add.s32 	%r316, %r316, 512;
	add.s32 	%r318, %r318, -1;
	setp.ne.s32 	%p58, %r318, 0;
	@%p58 bra 	$L__BB1_44;

$L__BB1_45:
	setp.lt.u32 	%p59, %r35, 384;
	@%p59 bra 	$L__BB1_48;

	shl.b32 	%r229, %r319, 2;
	add.s32 	%r231, %r93, %r229;
	add.s32 	%r320, %r231, 2064;

$L__BB1_47:
	ld.shared.f32 	%f142, [%r320+-1536];
	sub.ftz.f32 	%f143, %f142, %f27;
	mul.ftz.f32 	%f144, %f143, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f145, %f144;
	st.shared.f32 	[%r320+-1536], %f145;
	add.ftz.f32 	%f146, %f231, %f145;
	ld.shared.f32 	%f147, [%r320+-1024];
	sub.ftz.f32 	%f148, %f147, %f27;
	mul.ftz.f32 	%f149, %f148, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f150, %f149;
	st.shared.f32 	[%r320+-1024], %f150;
	add.ftz.f32 	%f151, %f146, %f150;
	ld.shared.f32 	%f152, [%r320+-512];
	sub.ftz.f32 	%f153, %f152, %f27;
	mul.ftz.f32 	%f154, %f153, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f155, %f154;
	st.shared.f32 	[%r320+-512], %f155;
	add.ftz.f32 	%f156, %f151, %f155;
	ld.shared.f32 	%f157, [%r320];
	sub.ftz.f32 	%f158, %f157, %f27;
	mul.ftz.f32 	%f159, %f158, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f160, %f159;
	st.shared.f32 	[%r320], %f160;
	add.ftz.f32 	%f231, %f156, %f160;
	add.s32 	%r320, %r320, 2048;
	add.s32 	%r319, %r319, 512;
	setp.lt.s32 	%p60, %r319, %r5;
	@%p60 bra 	$L__BB1_47;

$L__BB1_48:
	bar.sync 	0;
	mov.b32 	%r232, %f231;
	mov.u32 	%r233, 31;
	mov.u32 	%r234, 16;
	mov.u32 	%r235, -1;
	shfl.sync.bfly.b32 	%r236|%p61, %r232, %r234, %r233, %r235;
	mov.b32 	%f161, %r236;
	add.ftz.f32 	%f162, %f231, %f161;
	mov.b32 	%r237, %f162;
	mov.u32 	%r238, 8;
	shfl.sync.bfly.b32 	%r239|%p62, %r237, %r238, %r233, %r235;
	mov.b32 	%f163, %r239;
	add.ftz.f32 	%f164, %f162, %f163;
	mov.b32 	%r240, %f164;
	mov.u32 	%r241, 4;
	shfl.sync.bfly.b32 	%r242|%p63, %r240, %r241, %r233, %r235;
	mov.b32 	%f165, %r242;
	add.ftz.f32 	%f166, %f164, %f165;
	mov.b32 	%r243, %f166;
	mov.u32 	%r244, 2;
	shfl.sync.bfly.b32 	%r245|%p64, %r243, %r244, %r233, %r235;
	mov.b32 	%f167, %r245;
	add.ftz.f32 	%f35, %f166, %f167;
	mov.b32 	%r246, %f35;
	mov.u32 	%r247, 1;
	shfl.sync.bfly.b32 	%r50|%p2, %r246, %r247, %r233, %r235;
	setp.ne.s32 	%p65, %r12, 0;
	@%p65 bra 	$L__BB1_50;

	mov.b32 	%f168, %r50;
	add.ftz.f32 	%f169, %f35, %f168;
	st.shared.f32 	[%r11+512], %f169;

$L__BB1_50:
	bar.sync 	0;
	setp.ne.s32 	%p66, %r10, 0;
	@%p66 bra 	$L__BB1_55;

	setp.gt.u32 	%p67, %r12, 3;
	mov.f32 	%f232, 0f00000000;
	@%p67 bra 	$L__BB1_53;

	ld.shared.f32 	%f232, [%r13+512];

$L__BB1_53:
	mov.b32 	%r249, %f232;
	mov.u32 	%r250, 31;
	mov.u32 	%r251, 16;
	mov.u32 	%r252, -1;
	shfl.sync.bfly.b32 	%r253|%p68, %r249, %r251, %r250, %r252;
	mov.b32 	%f171, %r253;
	add.ftz.f32 	%f172, %f232, %f171;
	mov.b32 	%r254, %f172;
	mov.u32 	%r255, 8;
	shfl.sync.bfly.b32 	%r256|%p69, %r254, %r255, %r250, %r252;
	mov.b32 	%f173, %r256;
	add.ftz.f32 	%f174, %f172, %f173;
	mov.b32 	%r257, %f174;
	mov.u32 	%r258, 4;
	shfl.sync.bfly.b32 	%r259|%p70, %r257, %r258, %r250, %r252;
	mov.b32 	%f175, %r259;
	add.ftz.f32 	%f176, %f174, %f175;
	mov.b32 	%r260, %f176;
	mov.u32 	%r261, 2;
	shfl.sync.bfly.b32 	%r262|%p71, %r260, %r261, %r250, %r252;
	mov.b32 	%f177, %r262;
	add.ftz.f32 	%f178, %f176, %f177;
	mov.b32 	%r263, %f178;
	mov.u32 	%r264, 1;
	shfl.sync.bfly.b32 	%r265|%p72, %r263, %r264, %r250, %r252;
	mov.b32 	%f179, %r265;
	add.ftz.f32 	%f38, %f178, %f179;
	@%p65 bra 	$L__BB1_55;

	st.shared.f32 	[smem+512], %f38;

$L__BB1_55:
	bar.sync 	0;
	ld.shared.f32 	%f180, [smem+512];
	add.ftz.f32 	%f181, %f180, 0f358637BD;
	mov.f32 	%f182, 0f3F800000;
	div.approx.ftz.f32 	%f39, %f182, %f181;
	@%p56 bra 	$L__BB1_62;

	not.b32 	%r266, %r1;
	add.s32 	%r52, %r5, %r266;
	shr.u32 	%r267, %r52, 7;
	add.s32 	%r268, %r267, 1;
	and.b32  	%r324, %r268, 3;
	setp.eq.s32 	%p75, %r324, 0;
	mov.u32 	%r325, %r1;
	@%p75 bra 	$L__BB1_59;

	add.s32 	%r322, %r8, 528;
	mov.u32 	%r325, %r1;

$L__BB1_58:
	.pragma "nounroll";
	ld.shared.f32 	%f183, [%r322];
	mul.ftz.f32 	%f184, %f39, %f183;
	st.shared.f32 	[%r322], %f184;
	add.s32 	%r325, %r325, 128;
	add.s32 	%r322, %r322, 512;
	add.s32 	%r324, %r324, -1;
	setp.ne.s32 	%p76, %r324, 0;
	@%p76 bra 	$L__BB1_58;

$L__BB1_59:
	setp.lt.u32 	%p77, %r52, 384;
	@%p77 bra 	$L__BB1_62;

	shl.b32 	%r272, %r325, 2;
	add.s32 	%r274, %r93, %r272;
	add.s32 	%r326, %r274, 2064;

$L__BB1_61:
	ld.shared.f32 	%f185, [%r326+-1536];
	mul.ftz.f32 	%f186, %f39, %f185;
	st.shared.f32 	[%r326+-1536], %f186;
	ld.shared.f32 	%f187, [%r326+-1024];
	mul.ftz.f32 	%f188, %f39, %f187;
	st.shared.f32 	[%r326+-1024], %f188;
	ld.shared.f32 	%f189, [%r326+-512];
	mul.ftz.f32 	%f190, %f39, %f189;
	st.shared.f32 	[%r326+-512], %f190;
	ld.shared.f32 	%f191, [%r326];
	mul.ftz.f32 	%f192, %f39, %f191;
	st.shared.f32 	[%r326], %f192;
	add.s32 	%r326, %r326, 2048;
	add.s32 	%r325, %r325, 512;
	setp.lt.s32 	%p78, %r325, %r5;
	@%p78 bra 	$L__BB1_61;

$L__BB1_62:
	setp.lt.s32 	%p87, %r5, 1;
	bar.sync 	0;
	mov.f32 	%f237, 0f00000000;
	@%p87 bra 	$L__BB1_73;

	ld.param.u32 	%r307, [paged_attention_v1_bf16_alibi_param_9];
	mov.u32 	%r306, %ctaid.y;
	mul.lo.s32 	%r67, %r306, %r307;
	shl.b32 	%r276, %r3, 7;
	add.s32 	%r68, %r276, %r1;
	not.b32 	%r69, %r5;
	mul.wide.s32 	%rd5, %r7, 2;
	mov.u32 	%r328, 0;

$L__BB1_64:
	shl.b32 	%r71, %r328, 4;
	add.s32 	%r278, %r71, %r69;
	max.s32 	%r279, %r278, -17;
	not.b32 	%r280, %r279;
	max.s32 	%r72, %r280, 1;
	setp.le.s32 	%p80, %r5, %r71;
	@%p80 bra 	$L__BB1_72;

	add.s32 	%r282, %r72, -1;
	add.s32 	%r283, %r328, %r67;
	mul.wide.s32 	%rd30, %r283, 4;
	add.s64 	%rd31, %rd3, %rd30;
	ld.global.nc.u32 	%r284, [%rd31];
	mad.lo.s32 	%r73, %r284, %r6, %r68;
	and.b32  	%r74, %r72, 3;
	setp.lt.u32 	%p81, %r282, 3;
	mov.u32 	%r331, 0;
	@%p81 bra 	$L__BB1_68;

	sub.s32 	%r330, %r72, %r74;

$L__BB1_67:
	mad.lo.s32 	%r286, %r331, %r7, %r73;
	mul.wide.s32 	%rd32, %r286, 2;
	add.s64 	%rd33, %rd2, %rd32;
	ld.global.nc.u16 	%rs5, [%rd33];
	// begin inline asm
	{ mov.b32 %f196, {0,%rs5};}

	// end inline asm
	add.s32 	%r287, %r71, %r331;
	shl.b32 	%r288, %r287, 2;
	add.s32 	%r290, %r93, %r288;
	ld.shared.f32 	%f200, [%r290+528];
	fma.rn.ftz.f32 	%f201, %f196, %f200, %f237;
	add.s64 	%rd34, %rd33, %rd5;
	ld.global.nc.u16 	%rs6, [%rd34];
	// begin inline asm
	{ mov.b32 %f197, {0,%rs6};}

	// end inline asm
	ld.shared.f32 	%f202, [%r290+532];
	fma.rn.ftz.f32 	%f203, %f197, %f202, %f201;
	add.s64 	%rd35, %rd34, %rd5;
	ld.global.nc.u16 	%rs7, [%rd35];
	// begin inline asm
	{ mov.b32 %f198, {0,%rs7};}

	// end inline asm
	ld.shared.f32 	%f204, [%r290+536];
	fma.rn.ftz.f32 	%f205, %f198, %f204, %f203;
	add.s64 	%rd36, %rd35, %rd5;
	ld.global.nc.u16 	%rs8, [%rd36];
	// begin inline asm
	{ mov.b32 %f199, {0,%rs8};}

	// end inline asm
	ld.shared.f32 	%f206, [%r290+540];
	fma.rn.ftz.f32 	%f237, %f199, %f206, %f205;
	add.s32 	%r331, %r331, 4;
	add.s32 	%r330, %r330, -4;
	setp.ne.s32 	%p82, %r330, 0;
	@%p82 bra 	$L__BB1_67;

$L__BB1_68:
	setp.eq.s32 	%p83, %r74, 0;
	@%p83 bra 	$L__BB1_72;

	mad.lo.s32 	%r81, %r331, %r7, %r73;
	mul.wide.s32 	%rd37, %r81, 2;
	add.s64 	%rd38, %rd2, %rd37;
	ld.global.nc.u16 	%rs9, [%rd38];
	// begin inline asm
	{ mov.b32 %f207, {0,%rs9};}

	// end inline asm
	add.s32 	%r291, %r331, %r71;
	shl.b32 	%r292, %r291, 2;
	add.s32 	%r294, %r93, %r292;
	ld.shared.f32 	%f208, [%r294+528];
	fma.rn.ftz.f32 	%f237, %f207, %f208, %f237;
	setp.eq.s32 	%p84, %r74, 1;
	@%p84 bra 	$L__BB1_72;

	add.s32 	%r83, %r81, %r7;
	mul.wide.s32 	%rd39, %r83, 2;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.nc.u16 	%rs10, [%rd40];
	// begin inline asm
	{ mov.b32 %f209, {0,%rs10};}

	// end inline asm
	add.s32 	%r296, %r294, 528;
	ld.shared.f32 	%f210, [%r296+4];
	fma.rn.ftz.f32 	%f237, %f209, %f210, %f237;
	setp.eq.s32 	%p85, %r74, 2;
	@%p85 bra 	$L__BB1_72;

	add.s32 	%r295, %r83, %r7;
	mul.wide.s32 	%rd41, %r295, 2;
	add.s64 	%rd42, %rd2, %rd41;
	ld.global.nc.u16 	%rs11, [%rd42];
	// begin inline asm
	{ mov.b32 %f211, {0,%rs11};}

	// end inline asm
	add.s32 	%r297, %r294, 528;
	ld.shared.f32 	%f212, [%r297+8];
	fma.rn.ftz.f32 	%f237, %f211, %f212, %f237;

$L__BB1_72:
	add.s32 	%r328, %r328, 1;
	setp.lt.s32 	%p86, %r328, %r9;
	@%p86 bra 	$L__BB1_64;

$L__BB1_73:
	mov.u32 	%r305, %ctaid.x;
	ld.param.u32 	%r304, [paged_attention_v1_bf16_alibi_param_7];
	mov.u32 	%r303, %ctaid.y;
	mad.lo.s32 	%r302, %r303, %r304, %r305;
	shl.b32 	%r301, %r302, 7;
	add.s32 	%r300, %r301, %r1;
	cvt.s64.s32 	%rd47, %r300;
	ld.param.u64 	%rd46, [paged_attention_v1_bf16_alibi_param_0];
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs12, %f237;}

	// end inline asm
	cvta.to.global.u64 	%rd43, %rd46;
	shl.b64 	%rd44, %rd47, 1;
	add.s64 	%rd45, %rd43, %rd44;
	st.global.u16 	[%rd45], %rs12;

$L__BB1_74:
	ret;

}

