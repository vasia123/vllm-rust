//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32267302
// Cuda compilation tools, release 12.0, V12.0.140
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_89
.address_size 64

	// .globl	moe_align_block_size_kernel
// _ZZ27moe_align_block_size_kernelE9scan_temp has been demoted
.global .align 1 .b8 _ZN49_INTERNAL_f57a15dc_18_fused_moe_align_cu_a7df09966thrust6system6detail10sequential3seqE[1];
.global .align 1 .b8 _ZN49_INTERNAL_f57a15dc_18_fused_moe_align_cu_a7df09966thrust12placeholders2_1E[1];
.global .align 1 .b8 _ZN49_INTERNAL_f57a15dc_18_fused_moe_align_cu_a7df09966thrust12placeholders2_2E[1];
.global .align 1 .b8 _ZN49_INTERNAL_f57a15dc_18_fused_moe_align_cu_a7df09966thrust12placeholders2_3E[1];
.global .align 1 .b8 _ZN49_INTERNAL_f57a15dc_18_fused_moe_align_cu_a7df09966thrust12placeholders2_4E[1];
.global .align 1 .b8 _ZN49_INTERNAL_f57a15dc_18_fused_moe_align_cu_a7df09966thrust12placeholders2_5E[1];
.global .align 1 .b8 _ZN49_INTERNAL_f57a15dc_18_fused_moe_align_cu_a7df09966thrust12placeholders2_6E[1];
.global .align 1 .b8 _ZN49_INTERNAL_f57a15dc_18_fused_moe_align_cu_a7df09966thrust12placeholders2_7E[1];
.global .align 1 .b8 _ZN49_INTERNAL_f57a15dc_18_fused_moe_align_cu_a7df09966thrust12placeholders2_8E[1];
.global .align 1 .b8 _ZN49_INTERNAL_f57a15dc_18_fused_moe_align_cu_a7df09966thrust12placeholders2_9E[1];
.global .align 1 .b8 _ZN49_INTERNAL_f57a15dc_18_fused_moe_align_cu_a7df09966thrust12placeholders3_10E[1];
.extern .shared .align 16 .b8 shared_mem[];

.visible .entry moe_align_block_size_kernel(
	.param .u64 moe_align_block_size_kernel_param_0,
	.param .u64 moe_align_block_size_kernel_param_1,
	.param .u64 moe_align_block_size_kernel_param_2,
	.param .u64 moe_align_block_size_kernel_param_3,
	.param .u32 moe_align_block_size_kernel_param_4,
	.param .u32 moe_align_block_size_kernel_param_5,
	.param .u32 moe_align_block_size_kernel_param_6,
	.param .u32 moe_align_block_size_kernel_param_7,
	.param .u64 moe_align_block_size_kernel_param_8
)
{
	.reg .pred 	%p<28>;
	.reg .b32 	%r<257>;
	.reg .b64 	%rd<52>;
	// demoted variable
	.shared .align 16 .b8 _ZZ27moe_align_block_size_kernelE9scan_temp[4256];

	ld.param.u64 	%rd22, [moe_align_block_size_kernel_param_0];
	ld.param.u64 	%rd24, [moe_align_block_size_kernel_param_1];
	ld.param.u64 	%rd25, [moe_align_block_size_kernel_param_2];
	ld.param.u64 	%rd23, [moe_align_block_size_kernel_param_3];
	ld.param.u32 	%r42, [moe_align_block_size_kernel_param_4];
	ld.param.u32 	%r43, [moe_align_block_size_kernel_param_5];
	ld.param.u32 	%r44, [moe_align_block_size_kernel_param_6];
	ld.param.u32 	%r45, [moe_align_block_size_kernel_param_7];
	ld.param.u64 	%rd26, [moe_align_block_size_kernel_param_8];
	cvta.to.global.u64 	%rd1, %rd24;
	cvta.to.global.u64 	%rd2, %rd25;
	cvta.to.global.u64 	%rd3, %rd26;
	mov.u32 	%r255, %tid.x;
	mov.u32 	%r2, %ntid.x;
	add.s32 	%r3, %r43, -1;
	add.s32 	%r46, %r3, %r45;
	div.s32 	%r4, %r46, %r43;
	mov.u32 	%r47, %ctaid.x;
	setp.eq.s32 	%p1, %r47, 1;
	@%p1 bra 	$L__BB0_32;
	bra.uni 	$L__BB0_1;

$L__BB0_32:
	setp.ge.s32 	%p23, %r255, %r45;
	@%p23 bra 	$L__BB0_39;

	not.b32 	%r237, %r255;
	add.s32 	%r238, %r237, %r45;
	div.u32 	%r33, %r238, %r2;
	add.s32 	%r239, %r33, 1;
	and.b32  	%r254, %r239, 3;
	setp.eq.s32 	%p24, %r254, 0;
	@%p24 bra 	$L__BB0_36;

	mul.wide.s32 	%rd42, %r255, 4;
	add.s64 	%rd51, %rd1, %rd42;
	mul.wide.s32 	%rd18, %r2, 4;

$L__BB0_35:
	.pragma "nounroll";
	st.global.u32 	[%rd51], %r44;
	add.s32 	%r255, %r255, %r2;
	add.s64 	%rd51, %rd51, %rd18;
	add.s32 	%r254, %r254, -1;
	setp.ne.s32 	%p25, %r254, 0;
	@%p25 bra 	$L__BB0_35;

$L__BB0_36:
	setp.lt.u32 	%p26, %r33, 3;
	@%p26 bra 	$L__BB0_39;

	mul.wide.s32 	%rd21, %r2, 4;

$L__BB0_38:
	mul.wide.s32 	%rd43, %r255, 4;
	add.s64 	%rd44, %rd1, %rd43;
	st.global.u32 	[%rd44], %r44;
	add.s64 	%rd45, %rd44, %rd21;
	st.global.u32 	[%rd45], %r44;
	add.s32 	%r240, %r255, %r2;
	add.s32 	%r241, %r240, %r2;
	add.s64 	%rd46, %rd45, %rd21;
	st.global.u32 	[%rd46], %r44;
	add.s32 	%r242, %r241, %r2;
	add.s64 	%rd47, %rd46, %rd21;
	st.global.u32 	[%rd47], %r44;
	add.s32 	%r255, %r242, %r2;
	setp.lt.s32 	%p27, %r255, %r45;
	@%p27 bra 	$L__BB0_38;
	bra.uni 	$L__BB0_39;

$L__BB0_1:
	setp.ge.s32 	%p2, %r255, %r42;
	shl.b32 	%r48, %r255, 2;
	mov.u32 	%r49, shared_mem;
	add.s32 	%r5, %r49, %r48;
	@%p2 bra 	$L__BB0_3;

	mov.u32 	%r50, 0;
	st.shared.u32 	[%r5], %r50;

$L__BB0_3:
	bar.sync 	0;
	setp.ge.s32 	%p3, %r255, %r44;
	@%p3 bra 	$L__BB0_8;

	cvta.to.global.u64 	%rd4, %rd22;
	mov.u32 	%r243, %r255;

$L__BB0_5:
	mul.wide.s32 	%rd27, %r243, 4;
	add.s64 	%rd28, %rd4, %rd27;
	ld.global.nc.u32 	%r7, [%rd28];
	setp.lt.s32 	%p4, %r7, 0;
	setp.ge.s32 	%p5, %r7, %r42;
	or.pred  	%p6, %p4, %p5;
	@%p6 bra 	$L__BB0_7;

	shl.b32 	%r51, %r7, 2;
	add.s32 	%r53, %r49, %r51;
	atom.shared.add.u32 	%r54, [%r53], 1;

$L__BB0_7:
	add.s32 	%r243, %r243, %r2;
	setp.lt.s32 	%p7, %r243, %r44;
	@%p7 bra 	$L__BB0_5;

$L__BB0_8:
	bar.sync 	0;
	mov.u32 	%r244, 0;
	@%p2 bra 	$L__BB0_10;

	ld.shared.u32 	%r56, [%r5];
	add.s32 	%r57, %r3, %r56;
	rem.s32 	%r58, %r57, %r43;
	sub.s32 	%r244, %r57, %r58;

$L__BB0_10:
	shr.u32 	%r59, %r255, 5;
	add.s32 	%r60, %r59, %r255;
	shl.b32 	%r61, %r60, 2;
	mov.u32 	%r62, _ZZ27moe_align_block_size_kernelE9scan_temp;
	add.s32 	%r11, %r62, %r61;
	st.shared.u32 	[%r11+16], %r244;
	bar.sync 	0;
	setp.gt.u32 	%p9, %r255, 31;
	@%p9 bra 	$L__BB0_12;

	mad.lo.s32 	%r96, %r255, 132, %r62;
	mov.u32 	%r73, 2;
	mov.u32 	%r91, 16;
	ld.shared.u32 	%r97, [%r96+20];
	mov.u32 	%r92, 0;
	ld.shared.u32 	%r98, [%r96+16];
	add.s32 	%r99, %r97, %r98;
	ld.shared.u32 	%r100, [%r96+24];
	add.s32 	%r101, %r99, %r100;
	ld.shared.u32 	%r102, [%r96+28];
	add.s32 	%r103, %r101, %r102;
	ld.shared.u32 	%r104, [%r96+32];
	add.s32 	%r105, %r103, %r104;
	ld.shared.u32 	%r106, [%r96+36];
	add.s32 	%r107, %r105, %r106;
	ld.shared.u32 	%r108, [%r96+40];
	add.s32 	%r109, %r107, %r108;
	ld.shared.u32 	%r110, [%r96+44];
	add.s32 	%r111, %r109, %r110;
	ld.shared.u32 	%r112, [%r96+48];
	add.s32 	%r113, %r111, %r112;
	ld.shared.u32 	%r114, [%r96+52];
	add.s32 	%r115, %r113, %r114;
	ld.shared.u32 	%r116, [%r96+56];
	add.s32 	%r117, %r115, %r116;
	ld.shared.u32 	%r118, [%r96+60];
	add.s32 	%r119, %r117, %r118;
	ld.shared.u32 	%r120, [%r96+64];
	add.s32 	%r121, %r119, %r120;
	ld.shared.u32 	%r122, [%r96+68];
	add.s32 	%r123, %r121, %r122;
	ld.shared.u32 	%r124, [%r96+72];
	add.s32 	%r125, %r123, %r124;
	ld.shared.u32 	%r126, [%r96+76];
	add.s32 	%r127, %r125, %r126;
	ld.shared.u32 	%r128, [%r96+80];
	add.s32 	%r129, %r127, %r128;
	ld.shared.u32 	%r130, [%r96+84];
	add.s32 	%r131, %r129, %r130;
	ld.shared.u32 	%r132, [%r96+88];
	add.s32 	%r133, %r131, %r132;
	ld.shared.u32 	%r134, [%r96+92];
	add.s32 	%r135, %r133, %r134;
	ld.shared.u32 	%r136, [%r96+96];
	add.s32 	%r137, %r135, %r136;
	ld.shared.u32 	%r138, [%r96+100];
	add.s32 	%r139, %r137, %r138;
	ld.shared.u32 	%r140, [%r96+104];
	add.s32 	%r141, %r139, %r140;
	ld.shared.u32 	%r142, [%r96+108];
	add.s32 	%r143, %r141, %r142;
	ld.shared.u32 	%r144, [%r96+112];
	add.s32 	%r145, %r143, %r144;
	ld.shared.u32 	%r146, [%r96+116];
	add.s32 	%r147, %r145, %r146;
	ld.shared.u32 	%r148, [%r96+120];
	add.s32 	%r149, %r147, %r148;
	ld.shared.u32 	%r150, [%r96+124];
	add.s32 	%r151, %r149, %r150;
	ld.shared.u32 	%r152, [%r96+128];
	add.s32 	%r153, %r151, %r152;
	ld.shared.u32 	%r154, [%r96+132];
	add.s32 	%r155, %r153, %r154;
	ld.shared.u32 	%r156, [%r96+136];
	add.s32 	%r157, %r155, %r156;
	ld.shared.u32 	%r158, [%r96+140];
	add.s32 	%r69, %r157, %r158;
	mov.u32 	%r67, 1;
	mov.u32 	%r94, -1;
	// begin inline asm
	{  .reg .s32 r0;  .reg .pred p;  shfl.sync.up.b32 r0|p, %r69, %r67, %r92, %r94;  @p add.s32 r0, r0, %r69;  mov.s32 %r65, r0;}
	// end inline asm
	// begin inline asm
	{  .reg .s32 r0;  .reg .pred p;  shfl.sync.up.b32 r0|p, %r65, %r73, %r92, %r94;  @p add.s32 r0, r0, %r65;  mov.s32 %r71, r0;}
	// end inline asm
	mov.u32 	%r79, 4;
	// begin inline asm
	{  .reg .s32 r0;  .reg .pred p;  shfl.sync.up.b32 r0|p, %r71, %r79, %r92, %r94;  @p add.s32 r0, r0, %r71;  mov.s32 %r77, r0;}
	// end inline asm
	mov.u32 	%r85, 8;
	// begin inline asm
	{  .reg .s32 r0;  .reg .pred p;  shfl.sync.up.b32 r0|p, %r77, %r85, %r92, %r94;  @p add.s32 r0, r0, %r77;  mov.s32 %r83, r0;}
	// end inline asm
	// begin inline asm
	{  .reg .s32 r0;  .reg .pred p;  shfl.sync.up.b32 r0|p, %r83, %r91, %r92, %r94;  @p add.s32 r0, r0, %r83;  mov.s32 %r89, r0;}
	// end inline asm
	sub.s32 	%r159, %r89, %r69;
	ld.shared.u32 	%r160, [%r96+16];
	add.s32 	%r161, %r160, %r159;
	ld.shared.u32 	%r162, [%r96+20];
	add.s32 	%r163, %r162, %r161;
	ld.shared.u32 	%r164, [%r96+24];
	add.s32 	%r165, %r164, %r163;
	ld.shared.u32 	%r166, [%r96+28];
	add.s32 	%r167, %r166, %r165;
	ld.shared.u32 	%r168, [%r96+32];
	add.s32 	%r169, %r168, %r167;
	ld.shared.u32 	%r170, [%r96+36];
	add.s32 	%r171, %r170, %r169;
	ld.shared.u32 	%r172, [%r96+40];
	add.s32 	%r173, %r172, %r171;
	ld.shared.u32 	%r174, [%r96+44];
	add.s32 	%r175, %r174, %r173;
	ld.shared.u32 	%r176, [%r96+48];
	add.s32 	%r177, %r176, %r175;
	ld.shared.u32 	%r178, [%r96+52];
	add.s32 	%r179, %r178, %r177;
	ld.shared.u32 	%r180, [%r96+56];
	add.s32 	%r181, %r180, %r179;
	ld.shared.u32 	%r182, [%r96+60];
	add.s32 	%r183, %r182, %r181;
	ld.shared.u32 	%r184, [%r96+64];
	add.s32 	%r185, %r184, %r183;
	ld.shared.u32 	%r186, [%r96+68];
	add.s32 	%r187, %r186, %r185;
	ld.shared.u32 	%r188, [%r96+72];
	add.s32 	%r189, %r188, %r187;
	ld.shared.u32 	%r190, [%r96+76];
	add.s32 	%r191, %r190, %r189;
	ld.shared.u32 	%r192, [%r96+80];
	add.s32 	%r193, %r192, %r191;
	ld.shared.u32 	%r194, [%r96+84];
	add.s32 	%r195, %r194, %r193;
	ld.shared.u32 	%r196, [%r96+88];
	add.s32 	%r197, %r196, %r195;
	ld.shared.u32 	%r198, [%r96+92];
	add.s32 	%r199, %r198, %r197;
	ld.shared.u32 	%r200, [%r96+96];
	add.s32 	%r201, %r200, %r199;
	ld.shared.u32 	%r202, [%r96+100];
	add.s32 	%r203, %r202, %r201;
	ld.shared.u32 	%r204, [%r96+104];
	add.s32 	%r205, %r204, %r203;
	ld.shared.u32 	%r206, [%r96+108];
	add.s32 	%r207, %r206, %r205;
	ld.shared.u32 	%r208, [%r96+112];
	add.s32 	%r209, %r208, %r207;
	ld.shared.u32 	%r210, [%r96+116];
	add.s32 	%r211, %r210, %r209;
	ld.shared.u32 	%r212, [%r96+120];
	add.s32 	%r213, %r212, %r211;
	ld.shared.u32 	%r214, [%r96+124];
	add.s32 	%r215, %r214, %r213;
	ld.shared.u32 	%r216, [%r96+128];
	add.s32 	%r217, %r216, %r215;
	ld.shared.u32 	%r218, [%r96+132];
	add.s32 	%r219, %r218, %r217;
	ld.shared.u32 	%r220, [%r96+136];
	add.s32 	%r221, %r220, %r219;
	st.shared.u32 	[%r96+16], %r159;
	st.shared.u32 	[%r96+20], %r161;
	st.shared.u32 	[%r96+24], %r163;
	st.shared.u32 	[%r96+28], %r165;
	st.shared.u32 	[%r96+32], %r167;
	st.shared.u32 	[%r96+36], %r169;
	st.shared.u32 	[%r96+40], %r171;
	st.shared.u32 	[%r96+44], %r173;
	st.shared.u32 	[%r96+48], %r175;
	st.shared.u32 	[%r96+52], %r177;
	st.shared.u32 	[%r96+56], %r179;
	st.shared.u32 	[%r96+60], %r181;
	st.shared.u32 	[%r96+64], %r183;
	st.shared.u32 	[%r96+68], %r185;
	st.shared.u32 	[%r96+72], %r187;
	st.shared.u32 	[%r96+76], %r189;
	st.shared.u32 	[%r96+80], %r191;
	st.shared.u32 	[%r96+84], %r193;
	st.shared.u32 	[%r96+88], %r195;
	st.shared.u32 	[%r96+92], %r197;
	st.shared.u32 	[%r96+96], %r199;
	st.shared.u32 	[%r96+100], %r201;
	st.shared.u32 	[%r96+104], %r203;
	st.shared.u32 	[%r96+108], %r205;
	st.shared.u32 	[%r96+112], %r207;
	st.shared.u32 	[%r96+116], %r209;
	st.shared.u32 	[%r96+120], %r211;
	st.shared.u32 	[%r96+124], %r213;
	st.shared.u32 	[%r96+128], %r215;
	st.shared.u32 	[%r96+132], %r217;
	st.shared.u32 	[%r96+136], %r219;
	st.shared.u32 	[%r96+140], %r221;

$L__BB0_12:
	bar.sync 	0;
	ld.shared.u32 	%r12, [%r11+16];
	mul.wide.s32 	%rd29, %r255, 4;
	add.s64 	%rd5, %rd3, %rd29;
	setp.gt.s32 	%p10, %r255, %r42;
	@%p10 bra 	$L__BB0_14;

	st.global.u32 	[%rd5], %r12;

$L__BB0_14:
	setp.ne.s32 	%p11, %r255, %r42;
	@%p11 bra 	$L__BB0_16;

	cvta.to.global.u64 	%rd30, %rd23;
	st.global.u32 	[%rd30], %r12;

$L__BB0_16:
	bar.sync 	0;
	@%p2 bra 	$L__BB0_24;

	ld.global.u32 	%r222, [%rd5];
	div.s32 	%r13, %r222, %r43;
	ld.global.u32 	%r223, [%rd5+4];
	div.s32 	%r14, %r223, %r43;
	setp.le.s32 	%p13, %r14, %r13;
	@%p13 bra 	$L__BB0_24;

	sub.s32 	%r224, %r14, %r13;
	and.b32  	%r246, %r224, 3;
	setp.eq.s32 	%p14, %r246, 0;
	mov.u32 	%r247, %r13;
	@%p14 bra 	$L__BB0_21;

	mul.wide.s32 	%rd31, %r13, 4;
	add.s64 	%rd48, %rd2, %rd31;
	mov.u32 	%r247, %r13;

$L__BB0_20:
	.pragma "nounroll";
	st.global.u32 	[%rd48], %r255;
	add.s32 	%r247, %r247, 1;
	add.s64 	%rd48, %rd48, 4;
	add.s32 	%r246, %r246, -1;
	setp.ne.s32 	%p15, %r246, 0;
	@%p15 bra 	$L__BB0_20;

$L__BB0_21:
	not.b32 	%r225, %r13;
	add.s32 	%r226, %r14, %r225;
	setp.lt.u32 	%p16, %r226, 3;
	@%p16 bra 	$L__BB0_24;

	mul.wide.s32 	%rd32, %r247, 4;
	add.s64 	%rd33, %rd2, %rd32;
	add.s64 	%rd49, %rd33, 8;

$L__BB0_23:
	st.global.u32 	[%rd49+-8], %r255;
	st.global.u32 	[%rd49+-4], %r255;
	st.global.u32 	[%rd49], %r255;
	st.global.u32 	[%rd49+4], %r255;
	add.s64 	%rd49, %rd49, 16;
	add.s32 	%r247, %r247, 4;
	setp.lt.s32 	%p17, %r247, %r14;
	@%p17 bra 	$L__BB0_23;

$L__BB0_24:
	mul.wide.s32 	%rd34, %r42, 4;
	add.s64 	%rd35, %rd3, %rd34;
	ld.global.u32 	%r227, [%rd35];
	div.s32 	%r228, %r227, %r43;
	add.s32 	%r251, %r228, %r255;
	setp.ge.s32 	%p18, %r251, %r4;
	@%p18 bra 	$L__BB0_31;

	not.b32 	%r229, %r251;
	add.s32 	%r230, %r4, %r229;
	div.u32 	%r24, %r230, %r2;
	add.s32 	%r231, %r24, 1;
	and.b32  	%r250, %r231, 3;
	setp.eq.s32 	%p19, %r250, 0;
	@%p19 bra 	$L__BB0_28;

	mul.wide.s32 	%rd36, %r251, 4;
	add.s64 	%rd50, %rd2, %rd36;
	mul.wide.s32 	%rd13, %r2, 4;

$L__BB0_27:
	.pragma "nounroll";
	mov.u32 	%r232, -1;
	st.global.u32 	[%rd50], %r232;
	add.s32 	%r251, %r251, %r2;
	add.s64 	%rd50, %rd50, %rd13;
	add.s32 	%r250, %r250, -1;
	setp.ne.s32 	%p20, %r250, 0;
	@%p20 bra 	$L__BB0_27;

$L__BB0_28:
	setp.lt.u32 	%p21, %r24, 3;
	@%p21 bra 	$L__BB0_31;

	mul.wide.s32 	%rd16, %r2, 4;

$L__BB0_30:
	mul.wide.s32 	%rd37, %r251, 4;
	add.s64 	%rd38, %rd2, %rd37;
	mov.u32 	%r233, -1;
	st.global.u32 	[%rd38], %r233;
	add.s64 	%rd39, %rd38, %rd16;
	st.global.u32 	[%rd39], %r233;
	add.s32 	%r234, %r251, %r2;
	add.s32 	%r235, %r234, %r2;
	add.s64 	%rd40, %rd39, %rd16;
	st.global.u32 	[%rd40], %r233;
	add.s32 	%r236, %r235, %r2;
	add.s64 	%rd41, %rd40, %rd16;
	st.global.u32 	[%rd41], %r233;
	add.s32 	%r251, %r236, %r2;
	setp.lt.s32 	%p22, %r251, %r4;
	@%p22 bra 	$L__BB0_30;

$L__BB0_31:
	bar.sync 	0;

$L__BB0_39:
	ret;

}
	// .globl	moe_sort_tokens_kernel
.visible .entry moe_sort_tokens_kernel(
	.param .u64 moe_sort_tokens_kernel_param_0,
	.param .u64 moe_sort_tokens_kernel_param_1,
	.param .u64 moe_sort_tokens_kernel_param_2,
	.param .u32 moe_sort_tokens_kernel_param_3,
	.param .u32 moe_sort_tokens_kernel_param_4,
	.param .u32 moe_sort_tokens_kernel_param_5
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd4, [moe_sort_tokens_kernel_param_0];
	ld.param.u64 	%rd5, [moe_sort_tokens_kernel_param_1];
	ld.param.u64 	%rd6, [moe_sort_tokens_kernel_param_2];
	ld.param.u32 	%r8, [moe_sort_tokens_kernel_param_3];
	ld.param.u32 	%r9, [moe_sort_tokens_kernel_param_4];
	ld.param.u32 	%r10, [moe_sort_tokens_kernel_param_5];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r11, %ctaid.y;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r14, %r11, %r1, %r12;
	setp.ge.s32 	%p1, %r14, %r9;
	@%p1 bra 	$L__BB1_6;

	mov.u32 	%r13, %nctaid.y;
	mul.lo.s32 	%r3, %r1, %r13;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd6;
	cvta.to.global.u64 	%rd3, %rd5;

$L__BB1_2:
	mul.wide.s32 	%rd7, %r14, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.nc.u32 	%r5, [%rd8];
	setp.lt.s32 	%p2, %r5, 0;
	setp.ge.s32 	%p3, %r5, %r8;
	or.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB1_5;

	mul.wide.s32 	%rd9, %r5, 4;
	add.s64 	%rd10, %rd2, %rd9;
	atom.global.add.u32 	%r6, [%rd10], 1;
	setp.ge.s32 	%p5, %r6, %r10;
	@%p5 bra 	$L__BB1_5;

	mul.wide.s32 	%rd11, %r6, 4;
	add.s64 	%rd12, %rd3, %rd11;
	st.global.u32 	[%rd12], %r14;

$L__BB1_5:
	add.s32 	%r14, %r14, %r3;
	setp.lt.s32 	%p6, %r14, %r9;
	@%p6 bra 	$L__BB1_2;

$L__BB1_6:
	ret;

}
	// .globl	moe_align_block_size_small_kernel
.visible .entry moe_align_block_size_small_kernel(
	.param .u64 moe_align_block_size_small_kernel_param_0,
	.param .u64 moe_align_block_size_small_kernel_param_1,
	.param .u64 moe_align_block_size_small_kernel_param_2,
	.param .u64 moe_align_block_size_small_kernel_param_3,
	.param .u32 moe_align_block_size_small_kernel_param_4,
	.param .u32 moe_align_block_size_small_kernel_param_5,
	.param .u32 moe_align_block_size_small_kernel_param_6,
	.param .u32 moe_align_block_size_small_kernel_param_7
)
{
	.reg .pred 	%p<75>;
	.reg .b32 	%r<385>;
	.reg .b64 	%rd<85>;


	ld.param.u64 	%rd39, [moe_align_block_size_small_kernel_param_0];
	ld.param.u64 	%rd40, [moe_align_block_size_small_kernel_param_1];
	ld.param.u64 	%rd41, [moe_align_block_size_small_kernel_param_2];
	ld.param.u64 	%rd38, [moe_align_block_size_small_kernel_param_3];
	ld.param.u32 	%r130, [moe_align_block_size_small_kernel_param_4];
	ld.param.u32 	%r131, [moe_align_block_size_small_kernel_param_5];
	ld.param.u32 	%r132, [moe_align_block_size_small_kernel_param_6];
	ld.param.u32 	%r133, [moe_align_block_size_small_kernel_param_7];
	cvta.to.global.u64 	%rd1, %rd41;
	cvta.to.global.u64 	%rd2, %rd40;
	cvta.to.global.u64 	%rd3, %rd39;
	cvt.s64.s32 	%rd42, %r130;
	add.s64 	%rd4, %rd42, 1;
	add.s32 	%r1, %r131, -1;
	add.s32 	%r134, %r1, %r133;
	div.s32 	%r2, %r134, %r131;
	mov.u32 	%r383, %tid.x;
	setp.lt.s32 	%p1, %r383, 256;
	@%p1 bra 	$L__BB2_74;
	bra.uni 	$L__BB2_1;

$L__BB2_74:
	setp.ge.s32 	%p70, %r383, %r133;
	@%p70 bra 	$L__BB2_81;

	not.b32 	%r336, %r383;
	add.s32 	%r121, %r336, %r133;
	shr.u32 	%r337, %r121, 8;
	add.s32 	%r338, %r337, 1;
	and.b32  	%r382, %r338, 3;
	setp.eq.s32 	%p71, %r382, 0;
	@%p71 bra 	$L__BB2_78;

	mul.wide.s32 	%rd75, %r383, 4;
	add.s64 	%rd83, %rd2, %rd75;

$L__BB2_77:
	.pragma "nounroll";
	st.global.u32 	[%rd83], %r132;
	add.s32 	%r383, %r383, 256;
	add.s64 	%rd83, %rd83, 1024;
	add.s32 	%r382, %r382, -1;
	setp.ne.s32 	%p72, %r382, 0;
	@%p72 bra 	$L__BB2_77;

$L__BB2_78:
	setp.lt.u32 	%p73, %r121, 768;
	@%p73 bra 	$L__BB2_81;

	mul.wide.s32 	%rd76, %r383, 4;
	add.s64 	%rd77, %rd2, %rd76;
	add.s64 	%rd84, %rd77, 2048;

$L__BB2_80:
	st.global.u32 	[%rd84+-2048], %r132;
	st.global.u32 	[%rd84+-1024], %r132;
	st.global.u32 	[%rd84], %r132;
	st.global.u32 	[%rd84+1024], %r132;
	add.s64 	%rd84, %rd84, 4096;
	add.s32 	%r383, %r383, 1024;
	setp.lt.s32 	%p74, %r383, %r133;
	@%p74 bra 	$L__BB2_80;

$L__BB2_81:
	bar.sync 	0;
	bar.sync 	0;
	bar.sync 	0;
	bra.uni 	$L__BB2_82;

$L__BB2_1:
	mov.u32 	%r4, %ntid.x;
	add.s32 	%r379, %r383, -256;
	add.s32 	%r6, %r4, -256;
	setp.lt.s32 	%p2, %r130, 1;
	@%p2 bra 	$L__BB2_8;

	add.s32 	%r136, %r130, -1;
	and.b32  	%r349, %r130, 3;
	setp.lt.u32 	%p3, %r136, 3;
	mov.u32 	%r347, 0;
	@%p3 bra 	$L__BB2_5;

	sub.s32 	%r346, %r130, %r349;
	add.s32 	%r138, %r383, -254;
	mul.lo.s32 	%r139, %r130, %r138;
	shl.b32 	%r140, %r139, 2;
	mov.u32 	%r141, shared_mem;
	add.s32 	%r142, %r141, %r140;
	add.s32 	%r344, %r142, 16;

$L__BB2_4:
	mov.u32 	%r143, 0;
	st.shared.u32 	[%r344+-12], %r143;
	st.shared.u32 	[%r344+-8], %r143;
	st.shared.u32 	[%r344+-4], %r143;
	st.shared.u32 	[%r344], %r143;
	add.s32 	%r347, %r347, 4;
	add.s32 	%r344, %r344, 16;
	add.s32 	%r346, %r346, -4;
	setp.ne.s32 	%p4, %r346, 0;
	@%p4 bra 	$L__BB2_4;

$L__BB2_5:
	setp.eq.s32 	%p5, %r349, 0;
	@%p5 bra 	$L__BB2_8;

	add.s32 	%r144, %r383, -254;
	mad.lo.s32 	%r145, %r130, %r144, %r347;
	shl.b32 	%r146, %r145, 2;
	mov.u32 	%r147, shared_mem;
	add.s32 	%r148, %r147, %r146;
	add.s32 	%r348, %r148, 4;

$L__BB2_7:
	.pragma "nounroll";
	mov.u32 	%r149, 0;
	st.shared.u32 	[%r348], %r149;
	add.s32 	%r348, %r348, 4;
	add.s32 	%r349, %r349, -1;
	setp.ne.s32 	%p6, %r349, 0;
	@%p6 bra 	$L__BB2_7;

$L__BB2_8:
	setp.ge.s32 	%p7, %r379, %r132;
	@%p7 bra 	$L__BB2_25;

	add.s32 	%r150, %r383, -255;
	cvt.u32.u64 	%r151, %rd4;
	mad.lo.s32 	%r22, %r150, %r130, %r151;
	add.s32 	%r152, %r132, 255;
	sub.s32 	%r153, %r152, %r383;
	div.u32 	%r23, %r153, %r6;
	add.s32 	%r154, %r23, 1;
	and.b32  	%r351, %r154, 3;
	setp.eq.s32 	%p8, %r351, 0;
	mov.u32 	%r352, %r379;
	@%p8 bra 	$L__BB2_14;

	mul.wide.s32 	%rd43, %r383, 4;
	add.s64 	%rd44, %rd3, %rd43;
	add.s64 	%rd78, %rd44, -1024;
	mul.wide.s32 	%rd6, %r6, 4;
	mov.u32 	%r352, %r379;

$L__BB2_11:
	.pragma "nounroll";
	ld.global.nc.u32 	%r27, [%rd78];
	setp.lt.s32 	%p9, %r27, 0;
	setp.ge.s32 	%p10, %r27, %r130;
	or.pred  	%p11, %p9, %p10;
	@%p11 bra 	$L__BB2_13;

	add.s32 	%r155, %r22, %r27;
	shl.b32 	%r156, %r155, 2;
	mov.u32 	%r157, shared_mem;
	add.s32 	%r158, %r157, %r156;
	ld.shared.u32 	%r159, [%r158];
	add.s32 	%r160, %r159, 1;
	st.shared.u32 	[%r158], %r160;

$L__BB2_13:
	add.s32 	%r352, %r352, %r6;
	add.s64 	%rd78, %rd78, %rd6;
	add.s32 	%r351, %r351, -1;
	setp.ne.s32 	%p12, %r351, 0;
	@%p12 bra 	$L__BB2_11;

$L__BB2_14:
	setp.lt.u32 	%p13, %r23, 3;
	@%p13 bra 	$L__BB2_25;

	mul.wide.s32 	%rd45, %r4, 4;
	add.s64 	%rd9, %rd45, -1024;

$L__BB2_16:
	mul.wide.s32 	%rd46, %r352, 4;
	add.s64 	%rd10, %rd3, %rd46;
	ld.global.nc.u32 	%r32, [%rd10];
	setp.lt.s32 	%p14, %r32, 0;
	setp.ge.s32 	%p15, %r32, %r130;
	or.pred  	%p16, %p14, %p15;
	@%p16 bra 	$L__BB2_18;

	add.s32 	%r161, %r22, %r32;
	shl.b32 	%r162, %r161, 2;
	mov.u32 	%r163, shared_mem;
	add.s32 	%r164, %r163, %r162;
	ld.shared.u32 	%r165, [%r164];
	add.s32 	%r166, %r165, 1;
	st.shared.u32 	[%r164], %r166;

$L__BB2_18:
	add.s64 	%rd11, %rd10, %rd9;
	ld.global.nc.u32 	%r33, [%rd11];
	setp.lt.s32 	%p17, %r33, 0;
	setp.ge.s32 	%p18, %r33, %r130;
	or.pred  	%p19, %p17, %p18;
	@%p19 bra 	$L__BB2_20;

	add.s32 	%r167, %r22, %r33;
	shl.b32 	%r168, %r167, 2;
	mov.u32 	%r169, shared_mem;
	add.s32 	%r170, %r169, %r168;
	ld.shared.u32 	%r171, [%r170];
	add.s32 	%r172, %r171, 1;
	st.shared.u32 	[%r170], %r172;

$L__BB2_20:
	add.s64 	%rd12, %rd11, %rd9;
	ld.global.nc.u32 	%r34, [%rd12];
	setp.lt.s32 	%p20, %r34, 0;
	setp.ge.s32 	%p21, %r34, %r130;
	or.pred  	%p22, %p20, %p21;
	@%p22 bra 	$L__BB2_22;

	add.s32 	%r173, %r22, %r34;
	shl.b32 	%r174, %r173, 2;
	mov.u32 	%r175, shared_mem;
	add.s32 	%r176, %r175, %r174;
	ld.shared.u32 	%r177, [%r176];
	add.s32 	%r178, %r177, 1;
	st.shared.u32 	[%r176], %r178;

$L__BB2_22:
	add.s64 	%rd47, %rd12, %rd9;
	ld.global.nc.u32 	%r35, [%rd47];
	setp.lt.s32 	%p23, %r35, 0;
	setp.ge.s32 	%p24, %r35, %r130;
	or.pred  	%p25, %p23, %p24;
	@%p25 bra 	$L__BB2_24;

	add.s32 	%r179, %r22, %r35;
	shl.b32 	%r180, %r179, 2;
	mov.u32 	%r181, shared_mem;
	add.s32 	%r182, %r181, %r180;
	ld.shared.u32 	%r183, [%r182];
	add.s32 	%r184, %r183, 1;
	st.shared.u32 	[%r182], %r184;

$L__BB2_24:
	add.s32 	%r185, %r352, %r6;
	add.s32 	%r186, %r185, %r6;
	add.s32 	%r187, %r186, %r6;
	add.s32 	%r352, %r187, %r6;
	setp.lt.s32 	%p26, %r352, %r132;
	@%p26 bra 	$L__BB2_16;

$L__BB2_25:
	bar.sync 	0;
	setp.ge.s32 	%p27, %r379, %r130;
	@%p27 bra 	$L__BB2_33;

	cvt.u32.u64 	%r188, %rd4;
	add.s32 	%r189, %r379, %r188;
	shl.b32 	%r190, %r189, 2;
	mov.u32 	%r191, shared_mem;
	add.s32 	%r355, %r191, %r190;
	mov.u32 	%r192, 0;
	st.shared.u32 	[%r355], %r192;
	setp.lt.s32 	%p28, %r4, 257;
	@%p28 bra 	$L__BB2_33;

	add.s32 	%r194, %r4, -257;
	and.b32  	%r360, %r6, 3;
	setp.lt.u32 	%p29, %r194, 3;
	mov.u32 	%r357, 1;
	@%p29 bra 	$L__BB2_30;

	shl.b32 	%r40, %r130, 4;
	sub.s32 	%r354, %r4, %r360;
	shl.b32 	%r42, %r130, 2;

$L__BB2_29:
	add.s32 	%r196, %r355, %r42;
	ld.shared.u32 	%r197, [%r196];
	ld.shared.u32 	%r198, [%r355];
	add.s32 	%r199, %r197, %r198;
	st.shared.u32 	[%r196], %r199;
	add.s32 	%r200, %r196, %r42;
	ld.shared.u32 	%r201, [%r200];
	add.s32 	%r202, %r201, %r199;
	st.shared.u32 	[%r200], %r202;
	add.s32 	%r203, %r200, %r42;
	ld.shared.u32 	%r204, [%r203];
	add.s32 	%r205, %r204, %r202;
	st.shared.u32 	[%r203], %r205;
	add.s32 	%r206, %r203, %r42;
	ld.shared.u32 	%r207, [%r206];
	add.s32 	%r208, %r207, %r205;
	st.shared.u32 	[%r206], %r208;
	add.s32 	%r357, %r357, 4;
	add.s32 	%r355, %r355, %r40;
	add.s32 	%r354, %r354, -4;
	setp.ne.s32 	%p30, %r354, 256;
	@%p30 bra 	$L__BB2_29;

$L__BB2_30:
	setp.eq.s32 	%p31, %r360, 0;
	@%p31 bra 	$L__BB2_33;

	shl.b32 	%r209, %r383, 2;
	add.s32 	%r211, %r191, %r209;
	add.s32 	%r212, %r211, -1020;
	shl.b32 	%r213, %r357, 2;
	add.s32 	%r214, %r213, 4;
	mad.lo.s32 	%r359, %r130, %r214, %r212;
	shl.b32 	%r51, %r130, 2;
	mad.lo.s32 	%r215, %r357, %r130, %r383;
	shl.b32 	%r216, %r215, 2;
	add.s32 	%r217, %r191, %r216;
	add.s32 	%r358, %r217, -1020;

$L__BB2_32:
	.pragma "nounroll";
	ld.shared.u32 	%r218, [%r359];
	ld.shared.u32 	%r219, [%r358];
	add.s32 	%r220, %r218, %r219;
	st.shared.u32 	[%r359], %r220;
	add.s32 	%r359, %r359, %r51;
	add.s32 	%r358, %r358, %r51;
	add.s32 	%r360, %r360, -1;
	setp.ne.s32 	%p32, %r360, 0;
	@%p32 bra 	$L__BB2_32;

$L__BB2_33:
	bar.sync 	0;
	shl.b32 	%r221, %r130, 2;
	mov.u32 	%r222, shared_mem;
	add.s32 	%r59, %r222, %r221;
	setp.ne.s32 	%p33, %r379, 0;
	@%p33 bra 	$L__BB2_42;

	mov.u32 	%r223, 0;
	st.shared.u32 	[shared_mem], %r223;
	@%p2 bra 	$L__BB2_41;

	add.s32 	%r225, %r130, -1;
	and.b32  	%r368, %r130, 3;
	setp.lt.u32 	%p35, %r225, 3;
	mov.u32 	%r366, 1;
	@%p35 bra 	$L__BB2_38;

	sub.s32 	%r365, %r130, %r368;
	add.s32 	%r229, %r4, -255;
	mul.lo.s32 	%r230, %r130, %r229;
	shl.b32 	%r231, %r230, 2;
	add.s32 	%r232, %r222, %r231;
	add.s32 	%r362, %r232, 16;
	mov.u32 	%r363, 0;
	mov.u32 	%r361, %r222;

$L__BB2_37:
	ld.shared.u32 	%r233, [%r362+-12];
	add.s32 	%r234, %r1, %r233;
	rem.s32 	%r235, %r234, %r131;
	add.s32 	%r236, %r363, %r234;
	sub.s32 	%r237, %r236, %r235;
	st.shared.u32 	[%r361+4], %r237;
	ld.shared.u32 	%r238, [%r362+-8];
	add.s32 	%r239, %r1, %r238;
	rem.s32 	%r240, %r239, %r131;
	add.s32 	%r241, %r237, %r239;
	sub.s32 	%r242, %r241, %r240;
	st.shared.u32 	[%r361+8], %r242;
	ld.shared.u32 	%r243, [%r362+-4];
	add.s32 	%r244, %r1, %r243;
	rem.s32 	%r245, %r244, %r131;
	add.s32 	%r246, %r242, %r244;
	sub.s32 	%r247, %r246, %r245;
	st.shared.u32 	[%r361+12], %r247;
	ld.shared.u32 	%r248, [%r362];
	add.s32 	%r249, %r1, %r248;
	rem.s32 	%r250, %r249, %r131;
	add.s32 	%r251, %r247, %r249;
	sub.s32 	%r363, %r251, %r250;
	add.s32 	%r71, %r361, 16;
	st.shared.u32 	[%r361+16], %r363;
	add.s32 	%r366, %r366, 4;
	add.s32 	%r362, %r362, 16;
	add.s32 	%r365, %r365, -4;
	setp.ne.s32 	%p36, %r365, 0;
	mov.u32 	%r361, %r71;
	@%p36 bra 	$L__BB2_37;

$L__BB2_38:
	setp.eq.s32 	%p37, %r368, 0;
	@%p37 bra 	$L__BB2_41;

	shl.b32 	%r252, %r366, 2;
	add.s32 	%r367, %r222, %r252;
	add.s32 	%r254, %r4, -255;
	mul.lo.s32 	%r255, %r130, %r254;
	shl.b32 	%r75, %r255, 2;

$L__BB2_40:
	.pragma "nounroll";
	add.s32 	%r256, %r367, %r75;
	ld.shared.u32 	%r257, [%r256];
	add.s32 	%r258, %r1, %r257;
	rem.s32 	%r259, %r258, %r131;
	ld.shared.u32 	%r260, [%r367+-4];
	add.s32 	%r261, %r260, %r258;
	sub.s32 	%r262, %r261, %r259;
	st.shared.u32 	[%r367], %r262;
	add.s32 	%r367, %r367, 4;
	add.s32 	%r368, %r368, -1;
	setp.ne.s32 	%p38, %r368, 0;
	@%p38 bra 	$L__BB2_40;

$L__BB2_41:
	ld.shared.u32 	%r263, [%r59];
	cvta.to.global.u64 	%rd48, %rd38;
	st.global.u32 	[%rd48], %r263;

$L__BB2_42:
	bar.sync 	0;
	@%p27 bra 	$L__BB2_50;

	shl.b32 	%r264, %r379, 2;
	add.s32 	%r266, %r222, %r264;
	ld.shared.u32 	%r267, [%r266];
	div.s32 	%r80, %r267, %r131;
	ld.shared.u32 	%r268, [%r266+4];
	div.s32 	%r81, %r268, %r131;
	setp.ge.s32 	%p40, %r80, %r81;
	@%p40 bra 	$L__BB2_50;

	add.s32 	%r269, %r80, 1;
	max.s32 	%r82, %r81, %r269;
	sub.s32 	%r270, %r82, %r80;
	and.b32  	%r370, %r270, 3;
	setp.eq.s32 	%p41, %r370, 0;
	mov.u32 	%r371, %r80;
	@%p41 bra 	$L__BB2_47;

	mul.wide.s32 	%rd49, %r80, 4;
	add.s64 	%rd79, %rd1, %rd49;
	mov.u32 	%r371, %r80;

$L__BB2_46:
	.pragma "nounroll";
	add.s32 	%r339, %r383, -256;
	st.global.u32 	[%rd79], %r339;
	add.s32 	%r371, %r371, 1;
	add.s64 	%rd79, %rd79, 4;
	add.s32 	%r370, %r370, -1;
	setp.ne.s32 	%p42, %r370, 0;
	@%p42 bra 	$L__BB2_46;

$L__BB2_47:
	not.b32 	%r271, %r80;
	add.s32 	%r272, %r82, %r271;
	setp.lt.u32 	%p43, %r272, 3;
	@%p43 bra 	$L__BB2_50;

	mul.wide.s32 	%rd50, %r371, 4;
	add.s64 	%rd51, %rd1, %rd50;
	add.s64 	%rd80, %rd51, 8;

$L__BB2_49:
	add.s32 	%r340, %r383, -256;
	st.global.u32 	[%rd80+-8], %r340;
	add.s32 	%r341, %r383, -256;
	st.global.u32 	[%rd80+-4], %r341;
	add.s32 	%r342, %r383, -256;
	st.global.u32 	[%rd80], %r342;
	add.s32 	%r343, %r383, -256;
	st.global.u32 	[%rd80+4], %r343;
	add.s64 	%rd80, %rd80, 16;
	add.s32 	%r371, %r371, 4;
	setp.lt.s32 	%p44, %r371, %r81;
	@%p44 bra 	$L__BB2_49;

$L__BB2_50:
	ld.shared.u32 	%r273, [%r59];
	div.s32 	%r91, %r273, %r131;
	add.s32 	%r375, %r91, %r379;
	setp.ge.s32 	%p45, %r375, %r2;
	@%p45 bra 	$L__BB2_57;

	add.s32 	%r274, %r2, 255;
	add.s32 	%r93, %r383, %r91;
	sub.s32 	%r275, %r274, %r93;
	div.u32 	%r94, %r275, %r6;
	add.s32 	%r276, %r94, 1;
	and.b32  	%r374, %r276, 3;
	setp.eq.s32 	%p46, %r374, 0;
	@%p46 bra 	$L__BB2_54;

	mul.wide.s32 	%rd52, %r93, 4;
	add.s64 	%rd53, %rd1, %rd52;
	add.s64 	%rd81, %rd53, -1024;
	mul.wide.s32 	%rd20, %r6, 4;

$L__BB2_53:
	.pragma "nounroll";
	mov.u32 	%r277, -1;
	st.global.u32 	[%rd81], %r277;
	add.s32 	%r375, %r375, %r6;
	add.s64 	%rd81, %rd81, %rd20;
	add.s32 	%r374, %r374, -1;
	setp.ne.s32 	%p47, %r374, 0;
	@%p47 bra 	$L__BB2_53;

$L__BB2_54:
	setp.lt.u32 	%p48, %r94, 3;
	@%p48 bra 	$L__BB2_57;

	mul.wide.s32 	%rd54, %r4, 4;
	add.s64 	%rd23, %rd54, -1024;

$L__BB2_56:
	mul.wide.s32 	%rd55, %r375, 4;
	add.s64 	%rd56, %rd1, %rd55;
	mov.u32 	%r278, -1;
	st.global.u32 	[%rd56], %r278;
	add.s64 	%rd57, %rd56, %rd23;
	st.global.u32 	[%rd57], %r278;
	add.s32 	%r279, %r375, %r6;
	add.s32 	%r280, %r279, %r6;
	add.s64 	%rd58, %rd57, %rd23;
	st.global.u32 	[%rd58], %r278;
	add.s32 	%r281, %r280, %r6;
	add.s64 	%rd59, %rd58, %rd23;
	st.global.u32 	[%rd59], %r278;
	add.s32 	%r375, %r281, %r6;
	setp.lt.s32 	%p49, %r375, %r2;
	@%p49 bra 	$L__BB2_56;

$L__BB2_57:
	@%p7 bra 	$L__BB2_82;

	cvt.u32.u64 	%r282, %rd4;
	mad.lo.s32 	%r103, %r379, %r130, %r282;
	add.s32 	%r283, %r132, 255;
	sub.s32 	%r284, %r283, %r383;
	div.u32 	%r104, %r284, %r6;
	add.s32 	%r285, %r104, 1;
	and.b32  	%r378, %r285, 3;
	setp.eq.s32 	%p51, %r378, 0;
	@%p51 bra 	$L__BB2_63;

	mul.wide.s32 	%rd60, %r383, 4;
	add.s64 	%rd61, %rd3, %rd60;
	add.s64 	%rd82, %rd61, -1024;
	mul.wide.s32 	%rd25, %r6, 4;

$L__BB2_60:
	.pragma "nounroll";
	ld.global.nc.u32 	%r108, [%rd82];
	setp.lt.s32 	%p52, %r108, 0;
	setp.ge.s32 	%p53, %r108, %r130;
	or.pred  	%p54, %p52, %p53;
	@%p54 bra 	$L__BB2_62;

	add.s32 	%r286, %r103, %r108;
	shl.b32 	%r287, %r286, 2;
	add.s32 	%r289, %r222, %r287;
	shl.b32 	%r290, %r108, 2;
	add.s32 	%r291, %r222, %r290;
	ld.shared.u32 	%r292, [%r291];
	ld.shared.u32 	%r293, [%r289];
	add.s32 	%r294, %r292, %r293;
	mul.wide.s32 	%rd62, %r294, 4;
	add.s64 	%rd63, %rd2, %rd62;
	st.global.u32 	[%rd63], %r379;
	add.s32 	%r295, %r293, 1;
	st.shared.u32 	[%r289], %r295;

$L__BB2_62:
	add.s32 	%r379, %r379, %r6;
	add.s64 	%rd82, %rd82, %rd25;
	add.s32 	%r378, %r378, -1;
	setp.ne.s32 	%p55, %r378, 0;
	@%p55 bra 	$L__BB2_60;

$L__BB2_63:
	setp.lt.u32 	%p56, %r104, 3;
	@%p56 bra 	$L__BB2_82;

	mul.wide.s32 	%rd64, %r4, 4;
	add.s64 	%rd28, %rd64, -1024;

$L__BB2_65:
	mul.wide.s32 	%rd65, %r379, 4;
	add.s64 	%rd29, %rd3, %rd65;
	ld.global.nc.u32 	%r113, [%rd29];
	setp.lt.s32 	%p57, %r113, 0;
	setp.ge.s32 	%p58, %r113, %r130;
	or.pred  	%p59, %p57, %p58;
	@%p59 bra 	$L__BB2_67;

	add.s32 	%r296, %r103, %r113;
	shl.b32 	%r297, %r296, 2;
	add.s32 	%r299, %r222, %r297;
	shl.b32 	%r300, %r113, 2;
	add.s32 	%r301, %r222, %r300;
	ld.shared.u32 	%r302, [%r301];
	ld.shared.u32 	%r303, [%r299];
	add.s32 	%r304, %r302, %r303;
	mul.wide.s32 	%rd66, %r304, 4;
	add.s64 	%rd67, %rd2, %rd66;
	st.global.u32 	[%rd67], %r379;
	add.s32 	%r305, %r303, 1;
	st.shared.u32 	[%r299], %r305;

$L__BB2_67:
	add.s32 	%r114, %r379, %r6;
	add.s64 	%rd30, %rd29, %rd28;
	ld.global.nc.u32 	%r115, [%rd30];
	setp.lt.s32 	%p60, %r115, 0;
	setp.ge.s32 	%p61, %r115, %r130;
	or.pred  	%p62, %p60, %p61;
	@%p62 bra 	$L__BB2_69;

	add.s32 	%r306, %r103, %r115;
	shl.b32 	%r307, %r306, 2;
	add.s32 	%r309, %r222, %r307;
	shl.b32 	%r310, %r115, 2;
	add.s32 	%r311, %r222, %r310;
	ld.shared.u32 	%r312, [%r311];
	ld.shared.u32 	%r313, [%r309];
	add.s32 	%r314, %r312, %r313;
	mul.wide.s32 	%rd68, %r314, 4;
	add.s64 	%rd69, %rd2, %rd68;
	st.global.u32 	[%rd69], %r114;
	add.s32 	%r315, %r313, 1;
	st.shared.u32 	[%r309], %r315;

$L__BB2_69:
	add.s32 	%r116, %r114, %r6;
	add.s64 	%rd31, %rd30, %rd28;
	ld.global.nc.u32 	%r117, [%rd31];
	setp.lt.s32 	%p63, %r117, 0;
	setp.ge.s32 	%p64, %r117, %r130;
	or.pred  	%p65, %p63, %p64;
	@%p65 bra 	$L__BB2_71;

	add.s32 	%r316, %r103, %r117;
	shl.b32 	%r317, %r316, 2;
	add.s32 	%r319, %r222, %r317;
	shl.b32 	%r320, %r117, 2;
	add.s32 	%r321, %r222, %r320;
	ld.shared.u32 	%r322, [%r321];
	ld.shared.u32 	%r323, [%r319];
	add.s32 	%r324, %r322, %r323;
	mul.wide.s32 	%rd70, %r324, 4;
	add.s64 	%rd71, %rd2, %rd70;
	st.global.u32 	[%rd71], %r116;
	add.s32 	%r325, %r323, 1;
	st.shared.u32 	[%r319], %r325;

$L__BB2_71:
	add.s32 	%r118, %r116, %r6;
	add.s64 	%rd72, %rd31, %rd28;
	ld.global.nc.u32 	%r119, [%rd72];
	setp.lt.s32 	%p66, %r119, 0;
	setp.ge.s32 	%p67, %r119, %r130;
	or.pred  	%p68, %p66, %p67;
	@%p68 bra 	$L__BB2_73;

	add.s32 	%r326, %r103, %r119;
	shl.b32 	%r327, %r326, 2;
	add.s32 	%r329, %r222, %r327;
	shl.b32 	%r330, %r119, 2;
	add.s32 	%r331, %r222, %r330;
	ld.shared.u32 	%r332, [%r331];
	ld.shared.u32 	%r333, [%r329];
	add.s32 	%r334, %r332, %r333;
	mul.wide.s32 	%rd73, %r334, 4;
	add.s64 	%rd74, %rd2, %rd73;
	st.global.u32 	[%rd74], %r118;
	add.s32 	%r335, %r333, 1;
	st.shared.u32 	[%r329], %r335;

$L__BB2_73:
	add.s32 	%r379, %r118, %r6;
	setp.lt.s32 	%p69, %r379, %r132;
	@%p69 bra 	$L__BB2_65;

$L__BB2_82:
	ret;

}
	// .globl	moe_sum_kernel
.visible .entry moe_sum_kernel(
	.param .u64 moe_sum_kernel_param_0,
	.param .u64 moe_sum_kernel_param_1,
	.param .u32 moe_sum_kernel_param_2,
	.param .u32 moe_sum_kernel_param_3
)
{
	.reg .pred 	%p<10>;
	.reg .f32 	%f<26>;
	.reg .b32 	%r<40>;
	.reg .b64 	%rd<21>;


	ld.param.u64 	%rd4, [moe_sum_kernel_param_0];
	ld.param.u64 	%rd5, [moe_sum_kernel_param_1];
	ld.param.u32 	%r21, [moe_sum_kernel_param_2];
	ld.param.u32 	%r22, [moe_sum_kernel_param_3];
	cvta.to.global.u64 	%rd1, %rd5;
	cvta.to.global.u64 	%rd2, %rd4;
	mov.u32 	%r35, %tid.x;
	setp.ge.s32 	%p1, %r35, %r21;
	@%p1 bra 	$L__BB3_13;

	mov.u32 	%r23, %ctaid.x;
	setp.gt.s32 	%p2, %r22, 0;
	mul.lo.s32 	%r2, %r23, %r21;
	mov.u32 	%r3, %ntid.x;
	mul.lo.s32 	%r4, %r23, %r22;
	@%p2 bra 	$L__BB3_4;
	bra.uni 	$L__BB3_3;

$L__BB3_4:
	add.s32 	%r7, %r22, -1;
	shl.b32 	%r8, %r21, 2;
	mul.lo.s32 	%r9, %r4, %r21;
	and.b32  	%r10, %r22, 3;
	sub.s32 	%r11, %r10, %r22;
	mul.wide.s32 	%rd3, %r21, 4;

$L__BB3_5:
	setp.lt.u32 	%p4, %r7, 3;
	mov.f32 	%f25, 0f00000000;
	mov.u32 	%r39, 0;
	@%p4 bra 	$L__BB3_8;

	add.s32 	%r37, %r9, %r35;

$L__BB3_7:
	mul.wide.s32 	%rd8, %r37, 4;
	add.s64 	%rd9, %rd1, %rd8;
	ld.global.nc.f32 	%f12, [%rd9];
	add.ftz.f32 	%f13, %f25, %f12;
	add.s64 	%rd10, %rd9, %rd3;
	ld.global.nc.f32 	%f14, [%rd10];
	add.ftz.f32 	%f15, %f13, %f14;
	add.s64 	%rd11, %rd10, %rd3;
	ld.global.nc.f32 	%f16, [%rd11];
	add.ftz.f32 	%f17, %f15, %f16;
	add.s64 	%rd12, %rd11, %rd3;
	ld.global.nc.f32 	%f18, [%rd12];
	add.ftz.f32 	%f25, %f17, %f18;
	add.s32 	%r37, %r37, %r8;
	add.s32 	%r39, %r39, 4;
	add.s32 	%r28, %r11, %r39;
	setp.ne.s32 	%p5, %r28, 0;
	@%p5 bra 	$L__BB3_7;

$L__BB3_8:
	setp.eq.s32 	%p6, %r10, 0;
	@%p6 bra 	$L__BB3_12;

	setp.eq.s32 	%p7, %r10, 1;
	add.s32 	%r19, %r39, %r4;
	mad.lo.s32 	%r29, %r19, %r21, %r35;
	mul.wide.s32 	%rd13, %r29, 4;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.nc.f32 	%f19, [%rd14];
	add.ftz.f32 	%f25, %f25, %f19;
	@%p7 bra 	$L__BB3_12;

	setp.eq.s32 	%p8, %r10, 2;
	add.s32 	%r30, %r19, 1;
	mad.lo.s32 	%r31, %r30, %r21, %r35;
	mul.wide.s32 	%rd15, %r31, 4;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.nc.f32 	%f20, [%rd16];
	add.ftz.f32 	%f25, %f25, %f20;
	@%p8 bra 	$L__BB3_12;

	add.s32 	%r32, %r19, 2;
	mad.lo.s32 	%r33, %r32, %r21, %r35;
	mul.wide.s32 	%rd17, %r33, 4;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.nc.f32 	%f21, [%rd18];
	add.ftz.f32 	%f25, %f25, %f21;

$L__BB3_12:
	add.s32 	%r34, %r35, %r2;
	mul.wide.s32 	%rd19, %r34, 4;
	add.s64 	%rd20, %rd2, %rd19;
	st.global.f32 	[%rd20], %f25;
	add.s32 	%r35, %r35, %r3;
	setp.lt.s32 	%p9, %r35, %r21;
	@%p9 bra 	$L__BB3_5;
	bra.uni 	$L__BB3_13;

$L__BB3_3:
	add.s32 	%r24, %r35, %r2;
	mul.wide.s32 	%rd6, %r24, 4;
	add.s64 	%rd7, %rd2, %rd6;
	mov.u32 	%r25, 0;
	st.global.u32 	[%rd7], %r25;
	add.s32 	%r35, %r35, %r3;
	setp.lt.s32 	%p3, %r35, %r21;
	@%p3 bra 	$L__BB3_3;

$L__BB3_13:
	ret;

}
	// .globl	moe_sum_bf16_kernel
.visible .entry moe_sum_bf16_kernel(
	.param .u64 moe_sum_bf16_kernel_param_0,
	.param .u64 moe_sum_bf16_kernel_param_1,
	.param .u32 moe_sum_bf16_kernel_param_2,
	.param .u32 moe_sum_bf16_kernel_param_3
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<10>;
	.reg .f32 	%f<28>;
	.reg .b32 	%r<39>;
	.reg .b64 	%rd<21>;


	ld.param.u64 	%rd4, [moe_sum_bf16_kernel_param_0];
	ld.param.u64 	%rd5, [moe_sum_bf16_kernel_param_1];
	ld.param.u32 	%r21, [moe_sum_bf16_kernel_param_2];
	ld.param.u32 	%r22, [moe_sum_bf16_kernel_param_3];
	cvta.to.global.u64 	%rd1, %rd5;
	cvta.to.global.u64 	%rd2, %rd4;
	mov.u32 	%r34, %tid.x;
	setp.ge.s32 	%p1, %r34, %r21;
	@%p1 bra 	$L__BB4_13;

	mov.u32 	%r23, %ctaid.x;
	setp.gt.s32 	%p2, %r22, 0;
	mul.lo.s32 	%r2, %r23, %r21;
	mov.u32 	%r3, %ntid.x;
	mul.lo.s32 	%r4, %r23, %r22;
	@%p2 bra 	$L__BB4_4;
	bra.uni 	$L__BB4_3;

$L__BB4_4:
	add.s32 	%r7, %r22, -1;
	shl.b32 	%r8, %r21, 2;
	mul.lo.s32 	%r9, %r4, %r21;
	and.b32  	%r10, %r22, 3;
	sub.s32 	%r11, %r10, %r22;
	mul.wide.s32 	%rd3, %r21, 2;

$L__BB4_5:
	setp.lt.u32 	%p4, %r7, 3;
	mov.f32 	%f27, 0f00000000;
	mov.u32 	%r38, 0;
	@%p4 bra 	$L__BB4_8;

	add.s32 	%r36, %r9, %r34;

$L__BB4_7:
	mul.wide.s32 	%rd8, %r36, 2;
	add.s64 	%rd9, %rd1, %rd8;
	ld.global.nc.u16 	%rs2, [%rd9];
	// begin inline asm
	{ mov.b32 %f13, {0,%rs2};}

	// end inline asm
	add.ftz.f32 	%f17, %f27, %f13;
	add.s64 	%rd10, %rd9, %rd3;
	ld.global.nc.u16 	%rs3, [%rd10];
	// begin inline asm
	{ mov.b32 %f14, {0,%rs3};}

	// end inline asm
	add.ftz.f32 	%f18, %f17, %f14;
	add.s64 	%rd11, %rd10, %rd3;
	ld.global.nc.u16 	%rs4, [%rd11];
	// begin inline asm
	{ mov.b32 %f15, {0,%rs4};}

	// end inline asm
	add.ftz.f32 	%f19, %f18, %f15;
	add.s64 	%rd12, %rd11, %rd3;
	ld.global.nc.u16 	%rs5, [%rd12];
	// begin inline asm
	{ mov.b32 %f16, {0,%rs5};}

	// end inline asm
	add.ftz.f32 	%f27, %f19, %f16;
	add.s32 	%r36, %r36, %r8;
	add.s32 	%r38, %r38, 4;
	add.s32 	%r27, %r11, %r38;
	setp.ne.s32 	%p5, %r27, 0;
	@%p5 bra 	$L__BB4_7;

$L__BB4_8:
	setp.eq.s32 	%p6, %r10, 0;
	@%p6 bra 	$L__BB4_12;

	setp.eq.s32 	%p7, %r10, 1;
	add.s32 	%r19, %r38, %r4;
	mad.lo.s32 	%r28, %r19, %r21, %r34;
	mul.wide.s32 	%rd13, %r28, 2;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.nc.u16 	%rs6, [%rd14];
	// begin inline asm
	{ mov.b32 %f20, {0,%rs6};}

	// end inline asm
	add.ftz.f32 	%f27, %f27, %f20;
	@%p7 bra 	$L__BB4_12;

	setp.eq.s32 	%p8, %r10, 2;
	add.s32 	%r29, %r19, 1;
	mad.lo.s32 	%r30, %r29, %r21, %r34;
	mul.wide.s32 	%rd15, %r30, 2;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.nc.u16 	%rs7, [%rd16];
	// begin inline asm
	{ mov.b32 %f21, {0,%rs7};}

	// end inline asm
	add.ftz.f32 	%f27, %f27, %f21;
	@%p8 bra 	$L__BB4_12;

	add.s32 	%r31, %r19, 2;
	mad.lo.s32 	%r32, %r31, %r21, %r34;
	mul.wide.s32 	%rd17, %r32, 2;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.nc.u16 	%rs8, [%rd18];
	// begin inline asm
	{ mov.b32 %f22, {0,%rs8};}

	// end inline asm
	add.ftz.f32 	%f27, %f27, %f22;

$L__BB4_12:
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs9, %f27;}

	// end inline asm
	add.s32 	%r33, %r34, %r2;
	mul.wide.s32 	%rd19, %r33, 2;
	add.s64 	%rd20, %rd2, %rd19;
	st.global.u16 	[%rd20], %rs9;
	add.s32 	%r34, %r34, %r3;
	setp.lt.s32 	%p9, %r34, %r21;
	@%p9 bra 	$L__BB4_5;
	bra.uni 	$L__BB4_13;

$L__BB4_3:
	mov.f32 	%f9, 0f00000000;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs1, %f9;}

	// end inline asm
	add.s32 	%r24, %r34, %r2;
	mul.wide.s32 	%rd6, %r24, 2;
	add.s64 	%rd7, %rd2, %rd6;
	st.global.u16 	[%rd7], %rs1;
	add.s32 	%r34, %r34, %r3;
	setp.lt.s32 	%p3, %r34, %r21;
	@%p3 bra 	$L__BB4_3;

$L__BB4_13:
	ret;

}
	// .globl	_ZN3cub11EmptyKernelIvEEvv
.visible .entry _ZN3cub11EmptyKernelIvEEvv()
{



	ret;

}

