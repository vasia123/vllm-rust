//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32267302
// Cuda compilation tools, release 12.0, V12.0.140
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_89
.address_size 64

	// .globl	rms_norm_bf16
.shared .align 4 .b8 _ZZ16block_reduce_sumfE9warp_sums[128];
// _ZZ13rms_norm_bf16E9s_inv_rms has been demoted
// _ZZ13rms_norm_fp16E9s_inv_rms has been demoted
// _ZZ12rms_norm_f32E9s_inv_rms has been demoted
// _ZZ23fused_add_rms_norm_bf16E9s_inv_rms has been demoted

.visible .entry rms_norm_bf16(
	.param .u64 rms_norm_bf16_param_0,
	.param .u64 rms_norm_bf16_param_1,
	.param .u64 rms_norm_bf16_param_2,
	.param .f32 rms_norm_bf16_param_3,
	.param .u32 rms_norm_bf16_param_4
)
{
	.reg .pred 	%p<24>;
	.reg .b16 	%rs<21>;
	.reg .f32 	%f<73>;
	.reg .b32 	%r<71>;
	.reg .b64 	%rd<30>;
	// demoted variable
	.shared .align 4 .f32 _ZZ13rms_norm_bf16E9s_inv_rms;

	ld.param.u64 	%rd5, [rms_norm_bf16_param_0];
	ld.param.u64 	%rd6, [rms_norm_bf16_param_1];
	ld.param.u64 	%rd7, [rms_norm_bf16_param_2];
	ld.param.f32 	%f12, [rms_norm_bf16_param_3];
	ld.param.u32 	%r17, [rms_norm_bf16_param_4];
	cvta.to.global.u64 	%rd1, %rd5;
	cvta.to.global.u64 	%rd2, %rd7;
	cvta.to.global.u64 	%rd3, %rd6;
	mov.u32 	%r18, %ctaid.x;
	mul.lo.s32 	%r19, %r18, %r17;
	cvt.s64.s32 	%rd4, %r19;
	mov.u32 	%r69, %tid.x;
	shr.s32 	%r20, %r17, 31;
	shr.u32 	%r21, %r20, 30;
	add.s32 	%r22, %r17, %r21;
	shr.s32 	%r2, %r22, 2;
	setp.ge.s32 	%p1, %r69, %r2;
	mov.f32 	%f71, 0f00000000;
	@%p1 bra 	$L__BB0_3;

	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r67, %r69;

$L__BB0_2:
	shl.b32 	%r23, %r67, 2;
	cvt.s64.s32 	%rd8, %r23;
	add.s64 	%rd9, %rd8, %rd4;
	shl.b64 	%rd10, %rd9, 1;
	add.s64 	%rd11, %rd3, %rd10;
	ld.global.nc.u16 	%rs1, [%rd11];
	// begin inline asm
	{ mov.b32 %f15, {0,%rs1};}

	// end inline asm
	ld.global.nc.u16 	%rs2, [%rd11+2];
	// begin inline asm
	{ mov.b32 %f16, {0,%rs2};}

	// end inline asm
	ld.global.nc.u16 	%rs3, [%rd11+4];
	// begin inline asm
	{ mov.b32 %f17, {0,%rs3};}

	// end inline asm
	ld.global.nc.u16 	%rs4, [%rd11+6];
	// begin inline asm
	{ mov.b32 %f18, {0,%rs4};}

	// end inline asm
	mul.ftz.f32 	%f19, %f16, %f16;
	fma.rn.ftz.f32 	%f20, %f15, %f15, %f19;
	fma.rn.ftz.f32 	%f21, %f17, %f17, %f20;
	fma.rn.ftz.f32 	%f22, %f18, %f18, %f21;
	add.ftz.f32 	%f71, %f71, %f22;
	add.s32 	%r67, %r67, %r3;
	setp.lt.s32 	%p2, %r67, %r2;
	@%p2 bra 	$L__BB0_2;

$L__BB0_3:
	shl.b32 	%r24, %r2, 2;
	add.s32 	%r70, %r24, %r69;
	setp.lt.s32 	%p3, %r70, %r17;
	@%p3 bra 	$L__BB0_4;
	bra.uni 	$L__BB0_6;

$L__BB0_4:
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r68, %r70;

$L__BB0_5:
	cvt.s64.s32 	%rd12, %r68;
	add.s64 	%rd13, %rd12, %rd4;
	shl.b64 	%rd14, %rd13, 1;
	add.s64 	%rd15, %rd3, %rd14;
	ld.global.nc.u16 	%rs5, [%rd15];
	// begin inline asm
	{ mov.b32 %f23, {0,%rs5};}

	// end inline asm
	fma.rn.ftz.f32 	%f71, %f23, %f23, %f71;
	add.s32 	%r68, %r68, %r7;
	setp.lt.s32 	%p4, %r68, %r17;
	@%p4 bra 	$L__BB0_5;

$L__BB0_6:
	shr.u32 	%r10, %r69, 5;
	mov.b32 	%r25, %f71;
	mov.u32 	%r26, 31;
	mov.u32 	%r27, 16;
	mov.u32 	%r28, -1;
	shfl.sync.bfly.b32 	%r29|%p5, %r25, %r27, %r26, %r28;
	mov.b32 	%f24, %r29;
	add.ftz.f32 	%f25, %f71, %f24;
	mov.b32 	%r30, %f25;
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r32|%p6, %r30, %r31, %r26, %r28;
	mov.b32 	%f26, %r32;
	add.ftz.f32 	%f27, %f25, %f26;
	mov.b32 	%r33, %f27;
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r35|%p7, %r33, %r34, %r26, %r28;
	mov.b32 	%f28, %r35;
	add.ftz.f32 	%f29, %f27, %f28;
	mov.b32 	%r36, %f29;
	mov.u32 	%r37, 2;
	shfl.sync.bfly.b32 	%r38|%p8, %r36, %r37, %r26, %r28;
	mov.b32 	%f30, %r38;
	add.ftz.f32 	%f31, %f29, %f30;
	mov.b32 	%r39, %f31;
	mov.u32 	%r40, 1;
	shfl.sync.bfly.b32 	%r41|%p9, %r39, %r40, %r26, %r28;
	mov.b32 	%f32, %r41;
	add.ftz.f32 	%f7, %f31, %f32;
	mov.u32 	%r11, %ntid.x;
	and.b32  	%r12, %r69, 31;
	setp.ne.s32 	%p10, %r12, 0;
	@%p10 bra 	$L__BB0_8;

	shl.b32 	%r42, %r10, 2;
	mov.u32 	%r43, _ZZ16block_reduce_sumfE9warp_sums;
	add.s32 	%r44, %r43, %r42;
	st.shared.f32 	[%r44], %f7;

$L__BB0_8:
	bar.sync 	0;
	setp.ne.s32 	%p11, %r10, 0;
	@%p11 bra 	$L__BB0_13;

	shr.u32 	%r45, %r11, 5;
	setp.ge.u32 	%p12, %r12, %r45;
	mov.f32 	%f72, 0f00000000;
	@%p12 bra 	$L__BB0_11;

	shl.b32 	%r46, %r12, 2;
	mov.u32 	%r47, _ZZ16block_reduce_sumfE9warp_sums;
	add.s32 	%r48, %r47, %r46;
	ld.shared.f32 	%f72, [%r48];

$L__BB0_11:
	mov.b32 	%r49, %f72;
	mov.u32 	%r50, 31;
	mov.u32 	%r51, 16;
	mov.u32 	%r52, -1;
	shfl.sync.bfly.b32 	%r53|%p13, %r49, %r51, %r50, %r52;
	mov.b32 	%f34, %r53;
	add.ftz.f32 	%f35, %f72, %f34;
	mov.b32 	%r54, %f35;
	mov.u32 	%r55, 8;
	shfl.sync.bfly.b32 	%r56|%p14, %r54, %r55, %r50, %r52;
	mov.b32 	%f36, %r56;
	add.ftz.f32 	%f37, %f35, %f36;
	mov.b32 	%r57, %f37;
	mov.u32 	%r58, 4;
	shfl.sync.bfly.b32 	%r59|%p15, %r57, %r58, %r50, %r52;
	mov.b32 	%f38, %r59;
	add.ftz.f32 	%f39, %f37, %f38;
	mov.b32 	%r60, %f39;
	mov.u32 	%r61, 2;
	shfl.sync.bfly.b32 	%r62|%p16, %r60, %r61, %r50, %r52;
	mov.b32 	%f40, %r62;
	add.ftz.f32 	%f41, %f39, %f40;
	mov.b32 	%r63, %f41;
	mov.u32 	%r64, 1;
	shfl.sync.bfly.b32 	%r65|%p17, %r63, %r64, %r50, %r52;
	mov.b32 	%f42, %r65;
	add.ftz.f32 	%f10, %f41, %f42;
	@%p10 bra 	$L__BB0_13;

	st.shared.f32 	[_ZZ16block_reduce_sumfE9warp_sums], %f10;

$L__BB0_13:
	bar.sync 	0;
	setp.ne.s32 	%p19, %r69, 0;
	@%p19 bra 	$L__BB0_15;

	ld.shared.f32 	%f43, [_ZZ16block_reduce_sumfE9warp_sums];
	cvt.rn.f32.s32 	%f44, %r17;
	div.approx.ftz.f32 	%f45, %f43, %f44;
	add.ftz.f32 	%f46, %f45, %f12;
	rsqrt.approx.ftz.f32 	%f47, %f46;
	st.shared.f32 	[_ZZ13rms_norm_bf16E9s_inv_rms], %f47;

$L__BB0_15:
	bar.sync 	0;
	ld.shared.f32 	%f11, [_ZZ13rms_norm_bf16E9s_inv_rms];
	@%p1 bra 	$L__BB0_18;

$L__BB0_17:
	shl.b32 	%r66, %r69, 2;
	cvt.s64.s32 	%rd16, %r66;
	add.s64 	%rd17, %rd16, %rd4;
	shl.b64 	%rd18, %rd17, 1;
	add.s64 	%rd19, %rd3, %rd18;
	ld.global.nc.u16 	%rs6, [%rd19];
	// begin inline asm
	{ mov.b32 %f48, {0,%rs6};}

	// end inline asm
	ld.global.nc.u16 	%rs7, [%rd19+2];
	// begin inline asm
	{ mov.b32 %f49, {0,%rs7};}

	// end inline asm
	ld.global.nc.u16 	%rs8, [%rd19+4];
	// begin inline asm
	{ mov.b32 %f50, {0,%rs8};}

	// end inline asm
	ld.global.nc.u16 	%rs9, [%rd19+6];
	// begin inline asm
	{ mov.b32 %f51, {0,%rs9};}

	// end inline asm
	mul.wide.s32 	%rd20, %r66, 2;
	add.s64 	%rd21, %rd2, %rd20;
	ld.global.nc.u16 	%rs10, [%rd21];
	// begin inline asm
	{ mov.b32 %f52, {0,%rs10};}

	// end inline asm
	ld.global.nc.u16 	%rs11, [%rd21+2];
	// begin inline asm
	{ mov.b32 %f53, {0,%rs11};}

	// end inline asm
	ld.global.nc.u16 	%rs12, [%rd21+4];
	// begin inline asm
	{ mov.b32 %f54, {0,%rs12};}

	// end inline asm
	ld.global.nc.u16 	%rs13, [%rd21+6];
	// begin inline asm
	{ mov.b32 %f55, {0,%rs13};}

	// end inline asm
	mul.ftz.f32 	%f60, %f11, %f48;
	mul.ftz.f32 	%f56, %f60, %f52;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs14, %f56;}

	// end inline asm
	add.s64 	%rd22, %rd1, %rd18;
	st.global.u16 	[%rd22], %rs14;
	mul.ftz.f32 	%f61, %f11, %f49;
	mul.ftz.f32 	%f57, %f61, %f53;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs15, %f57;}

	// end inline asm
	st.global.u16 	[%rd22+2], %rs15;
	mul.ftz.f32 	%f62, %f11, %f50;
	mul.ftz.f32 	%f58, %f62, %f54;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs16, %f58;}

	// end inline asm
	st.global.u16 	[%rd22+4], %rs16;
	mul.ftz.f32 	%f63, %f11, %f51;
	mul.ftz.f32 	%f59, %f63, %f55;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs17, %f59;}

	// end inline asm
	st.global.u16 	[%rd22+6], %rs17;
	add.s32 	%r69, %r69, %r11;
	setp.lt.s32 	%p21, %r69, %r2;
	@%p21 bra 	$L__BB0_17;

$L__BB0_18:
	setp.ge.s32 	%p22, %r70, %r17;
	@%p22 bra 	$L__BB0_21;

$L__BB0_20:
	cvt.s64.s32 	%rd23, %r70;
	add.s64 	%rd24, %rd23, %rd4;
	shl.b64 	%rd25, %rd24, 1;
	add.s64 	%rd26, %rd3, %rd25;
	ld.global.nc.u16 	%rs18, [%rd26];
	// begin inline asm
	{ mov.b32 %f64, {0,%rs18};}

	// end inline asm
	mul.wide.s32 	%rd27, %r70, 2;
	add.s64 	%rd28, %rd2, %rd27;
	ld.global.nc.u16 	%rs19, [%rd28];
	// begin inline asm
	{ mov.b32 %f65, {0,%rs19};}

	// end inline asm
	mul.ftz.f32 	%f67, %f11, %f64;
	mul.ftz.f32 	%f66, %f67, %f65;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs20, %f66;}

	// end inline asm
	add.s64 	%rd29, %rd1, %rd25;
	st.global.u16 	[%rd29], %rs20;
	add.s32 	%r70, %r70, %r11;
	setp.lt.s32 	%p23, %r70, %r17;
	@%p23 bra 	$L__BB0_20;

$L__BB0_21:
	ret;

}
	// .globl	rms_norm_fp16
.visible .entry rms_norm_fp16(
	.param .u64 rms_norm_fp16_param_0,
	.param .u64 rms_norm_fp16_param_1,
	.param .u64 rms_norm_fp16_param_2,
	.param .f32 rms_norm_fp16_param_3,
	.param .u32 rms_norm_fp16_param_4
)
{
	.reg .pred 	%p<24>;
	.reg .b16 	%rs<21>;
	.reg .f32 	%f<73>;
	.reg .b32 	%r<71>;
	.reg .b64 	%rd<30>;
	// demoted variable
	.shared .align 4 .f32 _ZZ13rms_norm_fp16E9s_inv_rms;

	ld.param.u64 	%rd5, [rms_norm_fp16_param_0];
	ld.param.u64 	%rd6, [rms_norm_fp16_param_1];
	ld.param.u64 	%rd7, [rms_norm_fp16_param_2];
	ld.param.f32 	%f12, [rms_norm_fp16_param_3];
	ld.param.u32 	%r17, [rms_norm_fp16_param_4];
	cvta.to.global.u64 	%rd1, %rd5;
	cvta.to.global.u64 	%rd2, %rd7;
	cvta.to.global.u64 	%rd3, %rd6;
	mov.u32 	%r18, %ctaid.x;
	mul.lo.s32 	%r19, %r18, %r17;
	cvt.s64.s32 	%rd4, %r19;
	mov.u32 	%r69, %tid.x;
	shr.s32 	%r20, %r17, 31;
	shr.u32 	%r21, %r20, 30;
	add.s32 	%r22, %r17, %r21;
	shr.s32 	%r2, %r22, 2;
	setp.ge.s32 	%p1, %r69, %r2;
	mov.f32 	%f71, 0f00000000;
	@%p1 bra 	$L__BB1_3;

	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r67, %r69;

$L__BB1_2:
	shl.b32 	%r23, %r67, 2;
	cvt.s64.s32 	%rd8, %r23;
	add.s64 	%rd9, %rd8, %rd4;
	shl.b64 	%rd10, %rd9, 1;
	add.s64 	%rd11, %rd3, %rd10;
	ld.global.nc.u16 	%rs1, [%rd11];
	// begin inline asm
	{  cvt.f32.f16 %f15, %rs1;}

	// end inline asm
	ld.global.nc.u16 	%rs2, [%rd11+2];
	// begin inline asm
	{  cvt.f32.f16 %f16, %rs2;}

	// end inline asm
	ld.global.nc.u16 	%rs3, [%rd11+4];
	// begin inline asm
	{  cvt.f32.f16 %f17, %rs3;}

	// end inline asm
	ld.global.nc.u16 	%rs4, [%rd11+6];
	// begin inline asm
	{  cvt.f32.f16 %f18, %rs4;}

	// end inline asm
	mul.ftz.f32 	%f19, %f16, %f16;
	fma.rn.ftz.f32 	%f20, %f15, %f15, %f19;
	fma.rn.ftz.f32 	%f21, %f17, %f17, %f20;
	fma.rn.ftz.f32 	%f22, %f18, %f18, %f21;
	add.ftz.f32 	%f71, %f71, %f22;
	add.s32 	%r67, %r67, %r3;
	setp.lt.s32 	%p2, %r67, %r2;
	@%p2 bra 	$L__BB1_2;

$L__BB1_3:
	shl.b32 	%r24, %r2, 2;
	add.s32 	%r70, %r24, %r69;
	setp.lt.s32 	%p3, %r70, %r17;
	@%p3 bra 	$L__BB1_4;
	bra.uni 	$L__BB1_6;

$L__BB1_4:
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r68, %r70;

$L__BB1_5:
	cvt.s64.s32 	%rd12, %r68;
	add.s64 	%rd13, %rd12, %rd4;
	shl.b64 	%rd14, %rd13, 1;
	add.s64 	%rd15, %rd3, %rd14;
	ld.global.nc.u16 	%rs5, [%rd15];
	// begin inline asm
	{  cvt.f32.f16 %f23, %rs5;}

	// end inline asm
	fma.rn.ftz.f32 	%f71, %f23, %f23, %f71;
	add.s32 	%r68, %r68, %r7;
	setp.lt.s32 	%p4, %r68, %r17;
	@%p4 bra 	$L__BB1_5;

$L__BB1_6:
	shr.u32 	%r10, %r69, 5;
	mov.b32 	%r25, %f71;
	mov.u32 	%r26, 31;
	mov.u32 	%r27, 16;
	mov.u32 	%r28, -1;
	shfl.sync.bfly.b32 	%r29|%p5, %r25, %r27, %r26, %r28;
	mov.b32 	%f24, %r29;
	add.ftz.f32 	%f25, %f71, %f24;
	mov.b32 	%r30, %f25;
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r32|%p6, %r30, %r31, %r26, %r28;
	mov.b32 	%f26, %r32;
	add.ftz.f32 	%f27, %f25, %f26;
	mov.b32 	%r33, %f27;
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r35|%p7, %r33, %r34, %r26, %r28;
	mov.b32 	%f28, %r35;
	add.ftz.f32 	%f29, %f27, %f28;
	mov.b32 	%r36, %f29;
	mov.u32 	%r37, 2;
	shfl.sync.bfly.b32 	%r38|%p8, %r36, %r37, %r26, %r28;
	mov.b32 	%f30, %r38;
	add.ftz.f32 	%f31, %f29, %f30;
	mov.b32 	%r39, %f31;
	mov.u32 	%r40, 1;
	shfl.sync.bfly.b32 	%r41|%p9, %r39, %r40, %r26, %r28;
	mov.b32 	%f32, %r41;
	add.ftz.f32 	%f7, %f31, %f32;
	mov.u32 	%r11, %ntid.x;
	and.b32  	%r12, %r69, 31;
	setp.ne.s32 	%p10, %r12, 0;
	@%p10 bra 	$L__BB1_8;

	shl.b32 	%r42, %r10, 2;
	mov.u32 	%r43, _ZZ16block_reduce_sumfE9warp_sums;
	add.s32 	%r44, %r43, %r42;
	st.shared.f32 	[%r44], %f7;

$L__BB1_8:
	bar.sync 	0;
	setp.ne.s32 	%p11, %r10, 0;
	@%p11 bra 	$L__BB1_13;

	shr.u32 	%r45, %r11, 5;
	setp.ge.u32 	%p12, %r12, %r45;
	mov.f32 	%f72, 0f00000000;
	@%p12 bra 	$L__BB1_11;

	shl.b32 	%r46, %r12, 2;
	mov.u32 	%r47, _ZZ16block_reduce_sumfE9warp_sums;
	add.s32 	%r48, %r47, %r46;
	ld.shared.f32 	%f72, [%r48];

$L__BB1_11:
	mov.b32 	%r49, %f72;
	mov.u32 	%r50, 31;
	mov.u32 	%r51, 16;
	mov.u32 	%r52, -1;
	shfl.sync.bfly.b32 	%r53|%p13, %r49, %r51, %r50, %r52;
	mov.b32 	%f34, %r53;
	add.ftz.f32 	%f35, %f72, %f34;
	mov.b32 	%r54, %f35;
	mov.u32 	%r55, 8;
	shfl.sync.bfly.b32 	%r56|%p14, %r54, %r55, %r50, %r52;
	mov.b32 	%f36, %r56;
	add.ftz.f32 	%f37, %f35, %f36;
	mov.b32 	%r57, %f37;
	mov.u32 	%r58, 4;
	shfl.sync.bfly.b32 	%r59|%p15, %r57, %r58, %r50, %r52;
	mov.b32 	%f38, %r59;
	add.ftz.f32 	%f39, %f37, %f38;
	mov.b32 	%r60, %f39;
	mov.u32 	%r61, 2;
	shfl.sync.bfly.b32 	%r62|%p16, %r60, %r61, %r50, %r52;
	mov.b32 	%f40, %r62;
	add.ftz.f32 	%f41, %f39, %f40;
	mov.b32 	%r63, %f41;
	mov.u32 	%r64, 1;
	shfl.sync.bfly.b32 	%r65|%p17, %r63, %r64, %r50, %r52;
	mov.b32 	%f42, %r65;
	add.ftz.f32 	%f10, %f41, %f42;
	@%p10 bra 	$L__BB1_13;

	st.shared.f32 	[_ZZ16block_reduce_sumfE9warp_sums], %f10;

$L__BB1_13:
	bar.sync 	0;
	setp.ne.s32 	%p19, %r69, 0;
	@%p19 bra 	$L__BB1_15;

	ld.shared.f32 	%f43, [_ZZ16block_reduce_sumfE9warp_sums];
	cvt.rn.f32.s32 	%f44, %r17;
	div.approx.ftz.f32 	%f45, %f43, %f44;
	add.ftz.f32 	%f46, %f45, %f12;
	rsqrt.approx.ftz.f32 	%f47, %f46;
	st.shared.f32 	[_ZZ13rms_norm_fp16E9s_inv_rms], %f47;

$L__BB1_15:
	bar.sync 	0;
	ld.shared.f32 	%f11, [_ZZ13rms_norm_fp16E9s_inv_rms];
	@%p1 bra 	$L__BB1_18;

$L__BB1_17:
	shl.b32 	%r66, %r69, 2;
	cvt.s64.s32 	%rd16, %r66;
	add.s64 	%rd17, %rd16, %rd4;
	shl.b64 	%rd18, %rd17, 1;
	add.s64 	%rd19, %rd3, %rd18;
	ld.global.nc.u16 	%rs6, [%rd19];
	// begin inline asm
	{  cvt.f32.f16 %f48, %rs6;}

	// end inline asm
	ld.global.nc.u16 	%rs7, [%rd19+2];
	// begin inline asm
	{  cvt.f32.f16 %f49, %rs7;}

	// end inline asm
	ld.global.nc.u16 	%rs8, [%rd19+4];
	// begin inline asm
	{  cvt.f32.f16 %f50, %rs8;}

	// end inline asm
	ld.global.nc.u16 	%rs9, [%rd19+6];
	// begin inline asm
	{  cvt.f32.f16 %f51, %rs9;}

	// end inline asm
	mul.wide.s32 	%rd20, %r66, 2;
	add.s64 	%rd21, %rd2, %rd20;
	ld.global.nc.u16 	%rs10, [%rd21];
	// begin inline asm
	{  cvt.f32.f16 %f52, %rs10;}

	// end inline asm
	ld.global.nc.u16 	%rs11, [%rd21+2];
	// begin inline asm
	{  cvt.f32.f16 %f53, %rs11;}

	// end inline asm
	ld.global.nc.u16 	%rs12, [%rd21+4];
	// begin inline asm
	{  cvt.f32.f16 %f54, %rs12;}

	// end inline asm
	ld.global.nc.u16 	%rs13, [%rd21+6];
	// begin inline asm
	{  cvt.f32.f16 %f55, %rs13;}

	// end inline asm
	mul.ftz.f32 	%f60, %f11, %f48;
	mul.ftz.f32 	%f56, %f60, %f52;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs14, %f56;}

	// end inline asm
	add.s64 	%rd22, %rd1, %rd18;
	st.global.u16 	[%rd22], %rs14;
	mul.ftz.f32 	%f61, %f11, %f49;
	mul.ftz.f32 	%f57, %f61, %f53;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs15, %f57;}

	// end inline asm
	st.global.u16 	[%rd22+2], %rs15;
	mul.ftz.f32 	%f62, %f11, %f50;
	mul.ftz.f32 	%f58, %f62, %f54;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs16, %f58;}

	// end inline asm
	st.global.u16 	[%rd22+4], %rs16;
	mul.ftz.f32 	%f63, %f11, %f51;
	mul.ftz.f32 	%f59, %f63, %f55;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs17, %f59;}

	// end inline asm
	st.global.u16 	[%rd22+6], %rs17;
	add.s32 	%r69, %r69, %r11;
	setp.lt.s32 	%p21, %r69, %r2;
	@%p21 bra 	$L__BB1_17;

$L__BB1_18:
	setp.ge.s32 	%p22, %r70, %r17;
	@%p22 bra 	$L__BB1_21;

$L__BB1_20:
	cvt.s64.s32 	%rd23, %r70;
	add.s64 	%rd24, %rd23, %rd4;
	shl.b64 	%rd25, %rd24, 1;
	add.s64 	%rd26, %rd3, %rd25;
	ld.global.nc.u16 	%rs18, [%rd26];
	// begin inline asm
	{  cvt.f32.f16 %f64, %rs18;}

	// end inline asm
	mul.wide.s32 	%rd27, %r70, 2;
	add.s64 	%rd28, %rd2, %rd27;
	ld.global.nc.u16 	%rs19, [%rd28];
	// begin inline asm
	{  cvt.f32.f16 %f65, %rs19;}

	// end inline asm
	mul.ftz.f32 	%f67, %f11, %f64;
	mul.ftz.f32 	%f66, %f67, %f65;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs20, %f66;}

	// end inline asm
	add.s64 	%rd29, %rd1, %rd25;
	st.global.u16 	[%rd29], %rs20;
	add.s32 	%r70, %r70, %r11;
	setp.lt.s32 	%p23, %r70, %r17;
	@%p23 bra 	$L__BB1_20;

$L__BB1_21:
	ret;

}
	// .globl	rms_norm_f32
.visible .entry rms_norm_f32(
	.param .u64 rms_norm_f32_param_0,
	.param .u64 rms_norm_f32_param_1,
	.param .u64 rms_norm_f32_param_2,
	.param .f32 rms_norm_f32_param_3,
	.param .u32 rms_norm_f32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<44>;
	.reg .b32 	%r<57>;
	.reg .b64 	%rd<19>;
	// demoted variable
	.shared .align 4 .f32 _ZZ12rms_norm_f32E9s_inv_rms;

	ld.param.u64 	%rd5, [rms_norm_f32_param_0];
	ld.param.u64 	%rd7, [rms_norm_f32_param_1];
	ld.param.u64 	%rd6, [rms_norm_f32_param_2];
	ld.param.f32 	%f9, [rms_norm_f32_param_3];
	ld.param.u32 	%r10, [rms_norm_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd7;
	mov.u32 	%r11, %ctaid.x;
	mul.lo.s32 	%r12, %r11, %r10;
	cvt.s64.s32 	%rd2, %r12;
	mov.u32 	%r56, %tid.x;
	mov.f32 	%f42, 0f00000000;
	setp.lt.s32 	%p1, %r56, %r10;
	@%p1 bra 	$L__BB2_2;
	bra.uni 	$L__BB2_4;

$L__BB2_2:
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r55, %r56;

$L__BB2_3:
	cvt.s64.s32 	%rd8, %r55;
	add.s64 	%rd9, %rd8, %rd2;
	shl.b64 	%rd10, %rd9, 2;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.nc.f32 	%f12, [%rd11];
	fma.rn.ftz.f32 	%f42, %f12, %f12, %f42;
	add.s32 	%r55, %r55, %r2;
	setp.lt.s32 	%p2, %r55, %r10;
	@%p2 bra 	$L__BB2_3;

$L__BB2_4:
	shr.u32 	%r5, %r56, 5;
	mov.b32 	%r13, %f42;
	mov.u32 	%r14, 31;
	mov.u32 	%r15, 16;
	mov.u32 	%r16, -1;
	shfl.sync.bfly.b32 	%r17|%p3, %r13, %r15, %r14, %r16;
	mov.b32 	%f13, %r17;
	add.ftz.f32 	%f14, %f42, %f13;
	mov.b32 	%r18, %f14;
	mov.u32 	%r19, 8;
	shfl.sync.bfly.b32 	%r20|%p4, %r18, %r19, %r14, %r16;
	mov.b32 	%f15, %r20;
	add.ftz.f32 	%f16, %f14, %f15;
	mov.b32 	%r21, %f16;
	mov.u32 	%r22, 4;
	shfl.sync.bfly.b32 	%r23|%p5, %r21, %r22, %r14, %r16;
	mov.b32 	%f17, %r23;
	add.ftz.f32 	%f18, %f16, %f17;
	mov.b32 	%r24, %f18;
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r14, %r16;
	mov.b32 	%f19, %r26;
	add.ftz.f32 	%f20, %f18, %f19;
	mov.b32 	%r27, %f20;
	mov.u32 	%r28, 1;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r14, %r16;
	mov.b32 	%f21, %r29;
	add.ftz.f32 	%f4, %f20, %f21;
	and.b32  	%r6, %r56, 31;
	setp.ne.s32 	%p8, %r6, 0;
	@%p8 bra 	$L__BB2_6;

	shl.b32 	%r30, %r5, 2;
	mov.u32 	%r31, _ZZ16block_reduce_sumfE9warp_sums;
	add.s32 	%r32, %r31, %r30;
	st.shared.f32 	[%r32], %f4;

$L__BB2_6:
	bar.sync 	0;
	setp.ne.s32 	%p9, %r5, 0;
	@%p9 bra 	$L__BB2_11;

	mov.u32 	%r33, %ntid.x;
	shr.u32 	%r34, %r33, 5;
	setp.ge.u32 	%p10, %r6, %r34;
	mov.f32 	%f43, 0f00000000;
	@%p10 bra 	$L__BB2_9;

	shl.b32 	%r35, %r6, 2;
	mov.u32 	%r36, _ZZ16block_reduce_sumfE9warp_sums;
	add.s32 	%r37, %r36, %r35;
	ld.shared.f32 	%f43, [%r37];

$L__BB2_9:
	mov.b32 	%r38, %f43;
	mov.u32 	%r39, 31;
	mov.u32 	%r40, 16;
	mov.u32 	%r41, -1;
	shfl.sync.bfly.b32 	%r42|%p11, %r38, %r40, %r39, %r41;
	mov.b32 	%f23, %r42;
	add.ftz.f32 	%f24, %f43, %f23;
	mov.b32 	%r43, %f24;
	mov.u32 	%r44, 8;
	shfl.sync.bfly.b32 	%r45|%p12, %r43, %r44, %r39, %r41;
	mov.b32 	%f25, %r45;
	add.ftz.f32 	%f26, %f24, %f25;
	mov.b32 	%r46, %f26;
	mov.u32 	%r47, 4;
	shfl.sync.bfly.b32 	%r48|%p13, %r46, %r47, %r39, %r41;
	mov.b32 	%f27, %r48;
	add.ftz.f32 	%f28, %f26, %f27;
	mov.b32 	%r49, %f28;
	mov.u32 	%r50, 2;
	shfl.sync.bfly.b32 	%r51|%p14, %r49, %r50, %r39, %r41;
	mov.b32 	%f29, %r51;
	add.ftz.f32 	%f30, %f28, %f29;
	mov.b32 	%r52, %f30;
	mov.u32 	%r53, 1;
	shfl.sync.bfly.b32 	%r54|%p15, %r52, %r53, %r39, %r41;
	mov.b32 	%f31, %r54;
	add.ftz.f32 	%f7, %f30, %f31;
	@%p8 bra 	$L__BB2_11;

	st.shared.f32 	[_ZZ16block_reduce_sumfE9warp_sums], %f7;

$L__BB2_11:
	bar.sync 	0;
	setp.ne.s32 	%p17, %r56, 0;
	@%p17 bra 	$L__BB2_13;

	ld.shared.f32 	%f32, [_ZZ16block_reduce_sumfE9warp_sums];
	cvt.rn.f32.s32 	%f33, %r10;
	div.approx.ftz.f32 	%f34, %f32, %f33;
	add.ftz.f32 	%f35, %f34, %f9;
	rsqrt.approx.ftz.f32 	%f36, %f35;
	st.shared.f32 	[_ZZ12rms_norm_f32E9s_inv_rms], %f36;

$L__BB2_13:
	setp.ge.s32 	%p18, %r56, %r10;
	bar.sync 	0;
	@%p18 bra 	$L__BB2_16;

	ld.shared.f32 	%f8, [_ZZ12rms_norm_f32E9s_inv_rms];
	cvta.to.global.u64 	%rd3, %rd6;
	cvta.to.global.u64 	%rd4, %rd5;
	mov.u32 	%r7, %ntid.x;

$L__BB2_15:
	cvt.s64.s32 	%rd12, %r56;
	add.s64 	%rd13, %rd12, %rd2;
	shl.b64 	%rd14, %rd13, 2;
	add.s64 	%rd15, %rd1, %rd14;
	ld.global.nc.f32 	%f37, [%rd15];
	mul.ftz.f32 	%f38, %f8, %f37;
	mul.wide.s32 	%rd16, %r56, 4;
	add.s64 	%rd17, %rd3, %rd16;
	ld.global.nc.f32 	%f39, [%rd17];
	mul.ftz.f32 	%f40, %f38, %f39;
	add.s64 	%rd18, %rd4, %rd14;
	st.global.f32 	[%rd18], %f40;
	add.s32 	%r56, %r56, %r7;
	setp.lt.s32 	%p19, %r56, %r10;
	@%p19 bra 	$L__BB2_15;

$L__BB2_16:
	ret;

}
	// .globl	fused_add_rms_norm_bf16
.visible .entry fused_add_rms_norm_bf16(
	.param .u64 fused_add_rms_norm_bf16_param_0,
	.param .u64 fused_add_rms_norm_bf16_param_1,
	.param .u64 fused_add_rms_norm_bf16_param_2,
	.param .u64 fused_add_rms_norm_bf16_param_3,
	.param .f32 fused_add_rms_norm_bf16_param_4,
	.param .u32 fused_add_rms_norm_bf16_param_5
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<7>;
	.reg .f32 	%f<46>;
	.reg .b32 	%r<57>;
	.reg .b64 	%rd<22>;
	// demoted variable
	.shared .align 4 .f32 _ZZ23fused_add_rms_norm_bf16E9s_inv_rms;

	ld.param.u64 	%rd5, [fused_add_rms_norm_bf16_param_0];
	ld.param.u64 	%rd6, [fused_add_rms_norm_bf16_param_1];
	ld.param.u64 	%rd8, [fused_add_rms_norm_bf16_param_2];
	ld.param.u64 	%rd7, [fused_add_rms_norm_bf16_param_3];
	ld.param.f32 	%f9, [fused_add_rms_norm_bf16_param_4];
	ld.param.u32 	%r10, [fused_add_rms_norm_bf16_param_5];
	cvta.to.global.u64 	%rd1, %rd8;
	mov.u32 	%r11, %ctaid.x;
	mul.lo.s32 	%r12, %r11, %r10;
	cvt.s64.s32 	%rd2, %r12;
	mov.u32 	%r56, %tid.x;
	mov.f32 	%f44, 0f00000000;
	setp.lt.s32 	%p1, %r56, %r10;
	@%p1 bra 	$L__BB3_2;
	bra.uni 	$L__BB3_4;

$L__BB3_2:
	mov.u32 	%r2, %ntid.x;
	cvta.to.global.u64 	%rd13, %rd6;
	mov.u32 	%r55, %r56;

$L__BB3_3:
	cvt.s64.s32 	%rd9, %r55;
	add.s64 	%rd10, %rd9, %rd2;
	shl.b64 	%rd11, %rd10, 1;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.u16 	%rs1, [%rd12];
	// begin inline asm
	{ mov.b32 %f12, {0,%rs1};}

	// end inline asm
	add.s64 	%rd14, %rd13, %rd11;
	ld.global.nc.u16 	%rs2, [%rd14];
	// begin inline asm
	{ mov.b32 %f13, {0,%rs2};}

	// end inline asm
	add.ftz.f32 	%f14, %f12, %f13;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs3, %f14;}

	// end inline asm
	st.global.u16 	[%rd12], %rs3;
	fma.rn.ftz.f32 	%f44, %f14, %f14, %f44;
	add.s32 	%r55, %r55, %r2;
	setp.lt.s32 	%p2, %r55, %r10;
	@%p2 bra 	$L__BB3_3;

$L__BB3_4:
	shr.u32 	%r5, %r56, 5;
	mov.b32 	%r13, %f44;
	mov.u32 	%r14, 31;
	mov.u32 	%r15, 16;
	mov.u32 	%r16, -1;
	shfl.sync.bfly.b32 	%r17|%p3, %r13, %r15, %r14, %r16;
	mov.b32 	%f15, %r17;
	add.ftz.f32 	%f16, %f44, %f15;
	mov.b32 	%r18, %f16;
	mov.u32 	%r19, 8;
	shfl.sync.bfly.b32 	%r20|%p4, %r18, %r19, %r14, %r16;
	mov.b32 	%f17, %r20;
	add.ftz.f32 	%f18, %f16, %f17;
	mov.b32 	%r21, %f18;
	mov.u32 	%r22, 4;
	shfl.sync.bfly.b32 	%r23|%p5, %r21, %r22, %r14, %r16;
	mov.b32 	%f19, %r23;
	add.ftz.f32 	%f20, %f18, %f19;
	mov.b32 	%r24, %f20;
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r14, %r16;
	mov.b32 	%f21, %r26;
	add.ftz.f32 	%f22, %f20, %f21;
	mov.b32 	%r27, %f22;
	mov.u32 	%r28, 1;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r14, %r16;
	mov.b32 	%f23, %r29;
	add.ftz.f32 	%f4, %f22, %f23;
	and.b32  	%r6, %r56, 31;
	setp.ne.s32 	%p8, %r6, 0;
	@%p8 bra 	$L__BB3_6;

	shl.b32 	%r30, %r5, 2;
	mov.u32 	%r31, _ZZ16block_reduce_sumfE9warp_sums;
	add.s32 	%r32, %r31, %r30;
	st.shared.f32 	[%r32], %f4;

$L__BB3_6:
	bar.sync 	0;
	setp.ne.s32 	%p9, %r5, 0;
	@%p9 bra 	$L__BB3_11;

	mov.u32 	%r33, %ntid.x;
	shr.u32 	%r34, %r33, 5;
	setp.ge.u32 	%p10, %r6, %r34;
	mov.f32 	%f45, 0f00000000;
	@%p10 bra 	$L__BB3_9;

	shl.b32 	%r35, %r6, 2;
	mov.u32 	%r36, _ZZ16block_reduce_sumfE9warp_sums;
	add.s32 	%r37, %r36, %r35;
	ld.shared.f32 	%f45, [%r37];

$L__BB3_9:
	mov.b32 	%r38, %f45;
	mov.u32 	%r39, 31;
	mov.u32 	%r40, 16;
	mov.u32 	%r41, -1;
	shfl.sync.bfly.b32 	%r42|%p11, %r38, %r40, %r39, %r41;
	mov.b32 	%f25, %r42;
	add.ftz.f32 	%f26, %f45, %f25;
	mov.b32 	%r43, %f26;
	mov.u32 	%r44, 8;
	shfl.sync.bfly.b32 	%r45|%p12, %r43, %r44, %r39, %r41;
	mov.b32 	%f27, %r45;
	add.ftz.f32 	%f28, %f26, %f27;
	mov.b32 	%r46, %f28;
	mov.u32 	%r47, 4;
	shfl.sync.bfly.b32 	%r48|%p13, %r46, %r47, %r39, %r41;
	mov.b32 	%f29, %r48;
	add.ftz.f32 	%f30, %f28, %f29;
	mov.b32 	%r49, %f30;
	mov.u32 	%r50, 2;
	shfl.sync.bfly.b32 	%r51|%p14, %r49, %r50, %r39, %r41;
	mov.b32 	%f31, %r51;
	add.ftz.f32 	%f32, %f30, %f31;
	mov.b32 	%r52, %f32;
	mov.u32 	%r53, 1;
	shfl.sync.bfly.b32 	%r54|%p15, %r52, %r53, %r39, %r41;
	mov.b32 	%f33, %r54;
	add.ftz.f32 	%f7, %f32, %f33;
	@%p8 bra 	$L__BB3_11;

	st.shared.f32 	[_ZZ16block_reduce_sumfE9warp_sums], %f7;

$L__BB3_11:
	bar.sync 	0;
	setp.ne.s32 	%p17, %r56, 0;
	@%p17 bra 	$L__BB3_13;

	ld.shared.f32 	%f34, [_ZZ16block_reduce_sumfE9warp_sums];
	cvt.rn.f32.s32 	%f35, %r10;
	div.approx.ftz.f32 	%f36, %f34, %f35;
	add.ftz.f32 	%f37, %f36, %f9;
	rsqrt.approx.ftz.f32 	%f38, %f37;
	st.shared.f32 	[_ZZ23fused_add_rms_norm_bf16E9s_inv_rms], %f38;

$L__BB3_13:
	setp.ge.s32 	%p18, %r56, %r10;
	bar.sync 	0;
	@%p18 bra 	$L__BB3_16;

	ld.shared.f32 	%f8, [_ZZ23fused_add_rms_norm_bf16E9s_inv_rms];
	cvta.to.global.u64 	%rd3, %rd7;
	cvta.to.global.u64 	%rd4, %rd5;
	mov.u32 	%r7, %ntid.x;

$L__BB3_15:
	cvt.s64.s32 	%rd15, %r56;
	add.s64 	%rd16, %rd15, %rd2;
	shl.b64 	%rd17, %rd16, 1;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.u16 	%rs4, [%rd18];
	// begin inline asm
	{ mov.b32 %f39, {0,%rs4};}

	// end inline asm
	mul.wide.s32 	%rd19, %r56, 2;
	add.s64 	%rd20, %rd3, %rd19;
	ld.global.nc.u16 	%rs5, [%rd20];
	// begin inline asm
	{ mov.b32 %f40, {0,%rs5};}

	// end inline asm
	mul.ftz.f32 	%f42, %f8, %f39;
	mul.ftz.f32 	%f41, %f42, %f40;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs6, %f41;}

	// end inline asm
	add.s64 	%rd21, %rd4, %rd17;
	st.global.u16 	[%rd21], %rs6;
	add.s32 	%r56, %r56, %r7;
	setp.lt.s32 	%p19, %r56, %r10;
	@%p19 bra 	$L__BB3_15;

$L__BB3_16:
	ret;

}

