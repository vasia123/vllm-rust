//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32267302
// Cuda compilation tools, release 12.0, V12.0.140
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_89
.address_size 64

	// .globl	custom_allreduce_1stage_bf16

.visible .entry custom_allreduce_1stage_bf16(
	.param .u64 custom_allreduce_1stage_bf16_param_0,
	.param .align 16 .b8 custom_allreduce_1stage_bf16_param_1[64],
	.param .u64 custom_allreduce_1stage_bf16_param_2,
	.param .u64 custom_allreduce_1stage_bf16_param_3,
	.param .u32 custom_allreduce_1stage_bf16_param_4,
	.param .u32 custom_allreduce_1stage_bf16_param_5,
	.param .u32 custom_allreduce_1stage_bf16_param_6
)
.maxntid 512, 1, 1
.minnctapersm 1
{
	.local .align 16 .b8 	__local_depot0[128];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<89>;
	.reg .f32 	%f<201>;
	.reg .b32 	%r<136>;
	.reg .b64 	%rd<98>;


	mov.u64 	%SPL, __local_depot0;
	ld.param.u64 	%rd13, [custom_allreduce_1stage_bf16_param_0];
	ld.param.u64 	%rd11, [custom_allreduce_1stage_bf16_param_2];
	ld.param.u64 	%rd12, [custom_allreduce_1stage_bf16_param_3];
	ld.param.u32 	%r17, [custom_allreduce_1stage_bf16_param_4];
	ld.param.u32 	%r18, [custom_allreduce_1stage_bf16_param_5];
	ld.param.u32 	%r19, [custom_allreduce_1stage_bf16_param_6];
	add.u64 	%rd15, %SPL, 0;
	add.u64 	%rd1, %SPL, 64;
	ld.param.u64 	%rd17, [custom_allreduce_1stage_bf16_param_1+8];
	ld.param.u64 	%rd18, [custom_allreduce_1stage_bf16_param_1];
	st.local.v2.u64 	[%rd15], {%rd18, %rd17};
	ld.param.u64 	%rd19, [custom_allreduce_1stage_bf16_param_1+24];
	ld.param.u64 	%rd20, [custom_allreduce_1stage_bf16_param_1+16];
	st.local.v2.u64 	[%rd15+16], {%rd20, %rd19};
	ld.param.u64 	%rd21, [custom_allreduce_1stage_bf16_param_1+40];
	ld.param.u64 	%rd22, [custom_allreduce_1stage_bf16_param_1+32];
	st.local.v2.u64 	[%rd15+32], {%rd22, %rd21};
	ld.param.u64 	%rd23, [custom_allreduce_1stage_bf16_param_1+56];
	ld.param.u64 	%rd24, [custom_allreduce_1stage_bf16_param_1+48];
	st.local.v2.u64 	[%rd15+48], {%rd24, %rd23};
	cvta.to.global.u64 	%rd25, %rd13;
	ld.global.nc.v2.u64 	{%rd26, %rd27}, [%rd25];
	ld.global.nc.v2.u64 	{%rd29, %rd30}, [%rd25+16];
	ld.global.nc.v2.u64 	{%rd33, %rd34}, [%rd25+32];
	ld.global.nc.v2.u64 	{%rd37, %rd38}, [%rd25+48];
	st.local.v2.u64 	[%rd1], {%rd26, %rd27};
	st.local.v2.u64 	[%rd1+16], {%rd29, %rd30};
	st.local.v2.u64 	[%rd1+32], {%rd33, %rd34};
	st.local.v2.u64 	[%rd1+48], {%rd37, %rd38};
	mov.u32 	%r20, %ctaid.x;
	cvt.u64.u32 	%rd2, %r20;
	cvta.to.global.u64 	%rd41, %rd11;
	mul.wide.u32 	%rd42, %r20, 4;
	add.s64 	%rd43, %rd41, %rd42;
	add.s64 	%rd3, %rd43, 2304;
	ld.global.u32 	%r21, [%rd43+2304];
	add.s32 	%r1, %r21, 1;
	mov.u32 	%r2, %tid.x;
	setp.ge.u32 	%p1, %r2, %r18;
	mul.wide.u32 	%rd44, %r2, 8;
	add.s64 	%rd5, %rd15, %rd44;
	@%p1 bra 	$L__BB0_3;

	ld.local.u64 	%rd46, [%rd5];
	shl.b64 	%rd47, %rd2, 5;
	add.s64 	%rd48, %rd46, %rd47;
	mul.wide.s32 	%rd49, %r17, 4;
	add.s64 	%rd45, %rd48, %rd49;
	add.s64 	%rd50, %rd11, %rd47;
	mul.wide.u32 	%rd51, %r2, 4;
	add.s64 	%rd6, %rd50, %rd51;
	// begin inline asm
	st.volatile.global.u32 [%rd45], %r1;
	// end inline asm

$L__BB0_2:
	// begin inline asm
	ld.volatile.global.u32 %r23, [%rd6];
	// end inline asm
	setp.ne.s32 	%p2, %r23, %r1;
	@%p2 bra 	$L__BB0_2;

$L__BB0_3:
	bar.sync 	0;
	setp.ne.s32 	%p3, %r2, 0;
	@%p3 bra 	$L__BB0_5;

	add.s32 	%r129, %r21, 1;
	st.global.u32 	[%rd3], %r129;

$L__BB0_5:
	cvt.u32.u64 	%r24, %rd2;
	mov.u32 	%r3, %ntid.x;
	mad.lo.s32 	%r131, %r24, %r3, %r2;
	setp.ge.s32 	%p4, %r131, %r19;
	@%p4 bra 	$L__BB0_18;

	setp.gt.s32 	%p5, %r18, 1;
	@%p5 bra 	$L__BB0_9;
	bra.uni 	$L__BB0_7;

$L__BB0_9:
	add.s32 	%r39, %r18, -2;
	add.s32 	%r49, %r18, -1;

$L__BB0_10:
	setp.lt.u32 	%p7, %r39, 3;
	cvt.s64.s32 	%rd8, %r131;
	mul.wide.s32 	%rd56, %r131, 16;
	add.s64 	%rd57, %rd26, %rd56;
	ld.v4.u32 	{%r40, %r41, %r42, %r43}, [%rd57];
	mov.b32 	{%rs17, %rs18}, %r40;
	mov.u32 	%r135, 1;
	mov.b32 	{%rs19, %rs20}, %r41;
	mov.b32 	{%rs21, %rs22}, %r42;
	mov.b32 	{%rs23, %rs24}, %r43;
	// begin inline asm
	{ mov.b32 %f200, {0,%rs17};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f199, {0,%rs18};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f198, {0,%rs19};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f197, {0,%rs20};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f196, {0,%rs21};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f195, {0,%rs22};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f194, {0,%rs23};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f193, {0,%rs24};}

	// end inline asm
	@%p7 bra 	$L__BB0_13;

	and.b32  	%r50, %r49, 3;
	sub.s32 	%r134, %r49, %r50;

$L__BB0_12:
	mul.wide.s32 	%rd58, %r135, 8;
	add.s64 	%rd59, %rd1, %rd58;
	ld.local.u64 	%rd60, [%rd59];
	shl.b64 	%rd61, %rd8, 4;
	add.s64 	%rd62, %rd60, %rd61;
	ld.v4.u32 	{%r51, %r52, %r53, %r54}, [%rd62];
	mov.b32 	{%rs25, %rs26}, %r51;
	mov.b32 	{%rs27, %rs28}, %r52;
	mov.b32 	{%rs29, %rs30}, %r53;
	mov.b32 	{%rs31, %rs32}, %r54;
	// begin inline asm
	{ mov.b32 %f89, {0,%rs25};}

	// end inline asm
	add.ftz.f32 	%f121, %f89, %f200;
	// begin inline asm
	{ mov.b32 %f90, {0,%rs26};}

	// end inline asm
	add.ftz.f32 	%f122, %f90, %f199;
	// begin inline asm
	{ mov.b32 %f91, {0,%rs27};}

	// end inline asm
	add.ftz.f32 	%f123, %f91, %f198;
	// begin inline asm
	{ mov.b32 %f92, {0,%rs28};}

	// end inline asm
	add.ftz.f32 	%f124, %f92, %f197;
	// begin inline asm
	{ mov.b32 %f93, {0,%rs29};}

	// end inline asm
	add.ftz.f32 	%f125, %f93, %f196;
	// begin inline asm
	{ mov.b32 %f94, {0,%rs30};}

	// end inline asm
	add.ftz.f32 	%f126, %f94, %f195;
	// begin inline asm
	{ mov.b32 %f95, {0,%rs31};}

	// end inline asm
	add.ftz.f32 	%f127, %f95, %f194;
	// begin inline asm
	{ mov.b32 %f96, {0,%rs32};}

	// end inline asm
	add.ftz.f32 	%f128, %f96, %f193;
	ld.local.u64 	%rd63, [%rd59+8];
	add.s64 	%rd64, %rd63, %rd61;
	ld.v4.u32 	{%r59, %r60, %r61, %r62}, [%rd64];
	mov.b32 	{%rs33, %rs34}, %r59;
	mov.b32 	{%rs35, %rs36}, %r60;
	mov.b32 	{%rs37, %rs38}, %r61;
	mov.b32 	{%rs39, %rs40}, %r62;
	// begin inline asm
	{ mov.b32 %f97, {0,%rs33};}

	// end inline asm
	add.ftz.f32 	%f129, %f97, %f121;
	// begin inline asm
	{ mov.b32 %f98, {0,%rs34};}

	// end inline asm
	add.ftz.f32 	%f130, %f98, %f122;
	// begin inline asm
	{ mov.b32 %f99, {0,%rs35};}

	// end inline asm
	add.ftz.f32 	%f131, %f99, %f123;
	// begin inline asm
	{ mov.b32 %f100, {0,%rs36};}

	// end inline asm
	add.ftz.f32 	%f132, %f100, %f124;
	// begin inline asm
	{ mov.b32 %f101, {0,%rs37};}

	// end inline asm
	add.ftz.f32 	%f133, %f101, %f125;
	// begin inline asm
	{ mov.b32 %f102, {0,%rs38};}

	// end inline asm
	add.ftz.f32 	%f134, %f102, %f126;
	// begin inline asm
	{ mov.b32 %f103, {0,%rs39};}

	// end inline asm
	add.ftz.f32 	%f135, %f103, %f127;
	// begin inline asm
	{ mov.b32 %f104, {0,%rs40};}

	// end inline asm
	add.ftz.f32 	%f136, %f104, %f128;
	ld.local.u64 	%rd65, [%rd59+16];
	add.s64 	%rd66, %rd65, %rd61;
	ld.v4.u32 	{%r67, %r68, %r69, %r70}, [%rd66];
	mov.b32 	{%rs41, %rs42}, %r67;
	mov.b32 	{%rs43, %rs44}, %r68;
	mov.b32 	{%rs45, %rs46}, %r69;
	mov.b32 	{%rs47, %rs48}, %r70;
	// begin inline asm
	{ mov.b32 %f105, {0,%rs41};}

	// end inline asm
	add.ftz.f32 	%f137, %f105, %f129;
	// begin inline asm
	{ mov.b32 %f106, {0,%rs42};}

	// end inline asm
	add.ftz.f32 	%f138, %f106, %f130;
	// begin inline asm
	{ mov.b32 %f107, {0,%rs43};}

	// end inline asm
	add.ftz.f32 	%f139, %f107, %f131;
	// begin inline asm
	{ mov.b32 %f108, {0,%rs44};}

	// end inline asm
	add.ftz.f32 	%f140, %f108, %f132;
	// begin inline asm
	{ mov.b32 %f109, {0,%rs45};}

	// end inline asm
	add.ftz.f32 	%f141, %f109, %f133;
	// begin inline asm
	{ mov.b32 %f110, {0,%rs46};}

	// end inline asm
	add.ftz.f32 	%f142, %f110, %f134;
	// begin inline asm
	{ mov.b32 %f111, {0,%rs47};}

	// end inline asm
	add.ftz.f32 	%f143, %f111, %f135;
	// begin inline asm
	{ mov.b32 %f112, {0,%rs48};}

	// end inline asm
	add.ftz.f32 	%f144, %f112, %f136;
	ld.local.u64 	%rd67, [%rd59+24];
	add.s64 	%rd68, %rd67, %rd61;
	ld.v4.u32 	{%r75, %r76, %r77, %r78}, [%rd68];
	mov.b32 	{%rs49, %rs50}, %r75;
	mov.b32 	{%rs51, %rs52}, %r76;
	mov.b32 	{%rs53, %rs54}, %r77;
	mov.b32 	{%rs55, %rs56}, %r78;
	// begin inline asm
	{ mov.b32 %f113, {0,%rs49};}

	// end inline asm
	add.ftz.f32 	%f200, %f113, %f137;
	// begin inline asm
	{ mov.b32 %f114, {0,%rs50};}

	// end inline asm
	add.ftz.f32 	%f199, %f114, %f138;
	// begin inline asm
	{ mov.b32 %f115, {0,%rs51};}

	// end inline asm
	add.ftz.f32 	%f198, %f115, %f139;
	// begin inline asm
	{ mov.b32 %f116, {0,%rs52};}

	// end inline asm
	add.ftz.f32 	%f197, %f116, %f140;
	// begin inline asm
	{ mov.b32 %f117, {0,%rs53};}

	// end inline asm
	add.ftz.f32 	%f196, %f117, %f141;
	// begin inline asm
	{ mov.b32 %f118, {0,%rs54};}

	// end inline asm
	add.ftz.f32 	%f195, %f118, %f142;
	// begin inline asm
	{ mov.b32 %f119, {0,%rs55};}

	// end inline asm
	add.ftz.f32 	%f194, %f119, %f143;
	// begin inline asm
	{ mov.b32 %f120, {0,%rs56};}

	// end inline asm
	add.ftz.f32 	%f193, %f120, %f144;
	add.s32 	%r135, %r135, 4;
	add.s32 	%r134, %r134, -4;
	setp.ne.s32 	%p8, %r134, 0;
	@%p8 bra 	$L__BB0_12;

$L__BB0_13:
	and.b32  	%r84, %r49, 3;
	setp.eq.s32 	%p9, %r84, 0;
	@%p9 bra 	$L__BB0_17;

	setp.eq.s32 	%p10, %r84, 1;
	mul.wide.s32 	%rd69, %r135, 8;
	add.s64 	%rd9, %rd1, %rd69;
	ld.local.u64 	%rd70, [%rd9];
	shl.b64 	%rd71, %rd8, 4;
	add.s64 	%rd72, %rd70, %rd71;
	ld.v4.u32 	{%r87, %r88, %r89, %r90}, [%rd72];
	mov.b32 	{%rs57, %rs58}, %r87;
	mov.b32 	{%rs59, %rs60}, %r88;
	mov.b32 	{%rs61, %rs62}, %r89;
	mov.b32 	{%rs63, %rs64}, %r90;
	// begin inline asm
	{ mov.b32 %f145, {0,%rs57};}

	// end inline asm
	add.ftz.f32 	%f200, %f145, %f200;
	// begin inline asm
	{ mov.b32 %f146, {0,%rs58};}

	// end inline asm
	add.ftz.f32 	%f199, %f146, %f199;
	// begin inline asm
	{ mov.b32 %f147, {0,%rs59};}

	// end inline asm
	add.ftz.f32 	%f198, %f147, %f198;
	// begin inline asm
	{ mov.b32 %f148, {0,%rs60};}

	// end inline asm
	add.ftz.f32 	%f197, %f148, %f197;
	// begin inline asm
	{ mov.b32 %f149, {0,%rs61};}

	// end inline asm
	add.ftz.f32 	%f196, %f149, %f196;
	// begin inline asm
	{ mov.b32 %f150, {0,%rs62};}

	// end inline asm
	add.ftz.f32 	%f195, %f150, %f195;
	// begin inline asm
	{ mov.b32 %f151, {0,%rs63};}

	// end inline asm
	add.ftz.f32 	%f194, %f151, %f194;
	// begin inline asm
	{ mov.b32 %f152, {0,%rs64};}

	// end inline asm
	add.ftz.f32 	%f193, %f152, %f193;
	@%p10 bra 	$L__BB0_17;

	setp.eq.s32 	%p11, %r84, 2;
	ld.local.u64 	%rd73, [%rd9+8];
	add.s64 	%rd75, %rd73, %rd71;
	ld.v4.u32 	{%r97, %r98, %r99, %r100}, [%rd75];
	mov.b32 	{%rs65, %rs66}, %r97;
	mov.b32 	{%rs67, %rs68}, %r98;
	mov.b32 	{%rs69, %rs70}, %r99;
	mov.b32 	{%rs71, %rs72}, %r100;
	// begin inline asm
	{ mov.b32 %f153, {0,%rs65};}

	// end inline asm
	add.ftz.f32 	%f200, %f153, %f200;
	// begin inline asm
	{ mov.b32 %f154, {0,%rs66};}

	// end inline asm
	add.ftz.f32 	%f199, %f154, %f199;
	// begin inline asm
	{ mov.b32 %f155, {0,%rs67};}

	// end inline asm
	add.ftz.f32 	%f198, %f155, %f198;
	// begin inline asm
	{ mov.b32 %f156, {0,%rs68};}

	// end inline asm
	add.ftz.f32 	%f197, %f156, %f197;
	// begin inline asm
	{ mov.b32 %f157, {0,%rs69};}

	// end inline asm
	add.ftz.f32 	%f196, %f157, %f196;
	// begin inline asm
	{ mov.b32 %f158, {0,%rs70};}

	// end inline asm
	add.ftz.f32 	%f195, %f158, %f195;
	// begin inline asm
	{ mov.b32 %f159, {0,%rs71};}

	// end inline asm
	add.ftz.f32 	%f194, %f159, %f194;
	// begin inline asm
	{ mov.b32 %f160, {0,%rs72};}

	// end inline asm
	add.ftz.f32 	%f193, %f160, %f193;
	@%p11 bra 	$L__BB0_17;

	ld.local.u64 	%rd76, [%rd9+16];
	add.s64 	%rd78, %rd76, %rd71;
	ld.v4.u32 	{%r105, %r106, %r107, %r108}, [%rd78];
	mov.b32 	{%rs73, %rs74}, %r105;
	mov.b32 	{%rs75, %rs76}, %r106;
	mov.b32 	{%rs77, %rs78}, %r107;
	mov.b32 	{%rs79, %rs80}, %r108;
	// begin inline asm
	{ mov.b32 %f161, {0,%rs73};}

	// end inline asm
	add.ftz.f32 	%f200, %f161, %f200;
	// begin inline asm
	{ mov.b32 %f162, {0,%rs74};}

	// end inline asm
	add.ftz.f32 	%f199, %f162, %f199;
	// begin inline asm
	{ mov.b32 %f163, {0,%rs75};}

	// end inline asm
	add.ftz.f32 	%f198, %f163, %f198;
	// begin inline asm
	{ mov.b32 %f164, {0,%rs76};}

	// end inline asm
	add.ftz.f32 	%f197, %f164, %f197;
	// begin inline asm
	{ mov.b32 %f165, {0,%rs77};}

	// end inline asm
	add.ftz.f32 	%f196, %f165, %f196;
	// begin inline asm
	{ mov.b32 %f166, {0,%rs78};}

	// end inline asm
	add.ftz.f32 	%f195, %f166, %f195;
	// begin inline asm
	{ mov.b32 %f167, {0,%rs79};}

	// end inline asm
	add.ftz.f32 	%f194, %f167, %f194;
	// begin inline asm
	{ mov.b32 %f168, {0,%rs80};}

	// end inline asm
	add.ftz.f32 	%f193, %f168, %f193;

$L__BB0_17:
	cvta.to.global.u64 	%rd79, %rd12;
	shl.b64 	%rd80, %rd8, 4;
	add.s64 	%rd81, %rd79, %rd80;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs82, %f199;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs81, %f200;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs84, %f197;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs83, %f198;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs86, %f195;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs85, %f196;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs88, %f193;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs87, %f194;}

	// end inline asm
	mov.b32 	%r113, {%rs87, %rs88};
	mov.b32 	%r114, {%rs85, %rs86};
	mov.b32 	%r115, {%rs83, %rs84};
	mov.b32 	%r116, {%rs81, %rs82};
	st.global.v4.u32 	[%rd81], {%r116, %r115, %r114, %r113};
	mov.u32 	%r118, %nctaid.x;
	cvt.u32.u64 	%r119, %rd8;
	mad.lo.s32 	%r131, %r118, %r3, %r119;
	setp.lt.s32 	%p12, %r131, %r19;
	@%p12 bra 	$L__BB0_10;
	bra.uni 	$L__BB0_18;

$L__BB0_7:
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r25, %r3;
	cvta.to.global.u64 	%rd7, %rd12;

$L__BB0_8:
	mul.wide.s32 	%rd53, %r131, 16;
	add.s64 	%rd54, %rd26, %rd53;
	ld.v4.u32 	{%r26, %r27, %r28, %r29}, [%rd54];
	mov.b32 	{%rs1, %rs2}, %r26;
	mov.b32 	{%rs3, %rs4}, %r27;
	mov.b32 	{%rs5, %rs6}, %r28;
	mov.b32 	{%rs7, %rs8}, %r29;
	// begin inline asm
	{ mov.b32 %f65, {0,%rs1};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f66, {0,%rs2};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f67, {0,%rs3};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f68, {0,%rs4};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f69, {0,%rs5};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f70, {0,%rs6};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f71, {0,%rs7};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f72, {0,%rs8};}

	// end inline asm
	add.s64 	%rd55, %rd7, %rd53;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs10, %f66;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs9, %f65;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs12, %f68;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs11, %f67;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs14, %f70;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs13, %f69;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs16, %f72;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs15, %f71;}

	// end inline asm
	mov.b32 	%r34, {%rs15, %rs16};
	mov.b32 	%r35, {%rs13, %rs14};
	mov.b32 	%r36, {%rs11, %rs12};
	mov.b32 	%r37, {%rs9, %rs10};
	st.global.v4.u32 	[%rd55], {%r37, %r36, %r35, %r34};
	add.s32 	%r131, %r131, %r5;
	setp.lt.s32 	%p6, %r131, %r19;
	@%p6 bra 	$L__BB0_8;

$L__BB0_18:
	bar.sync 	0;
	ld.global.u32 	%r122, [%rd43+2304];
	add.s32 	%r16, %r122, 1;
	@%p1 bra 	$L__BB0_21;

	ld.local.u64 	%rd86, [%rd5];
	mul.wide.u32 	%rd87, %r20, 32;
	add.s64 	%rd88, %rd86, %rd87;
	mul.wide.s32 	%rd89, %r17, 4;
	add.s64 	%rd90, %rd88, %rd89;
	add.s64 	%rd85, %rd90, 1152;
	add.s64 	%rd91, %rd11, %rd87;
	mul.wide.u32 	%rd92, %r2, 4;
	add.s64 	%rd93, %rd91, %rd92;
	add.s64 	%rd10, %rd93, 1152;
	// begin inline asm
	st.volatile.global.u32 [%rd85], %r16;
	// end inline asm

$L__BB0_20:
	// begin inline asm
	ld.volatile.global.u32 %r126, [%rd10];
	// end inline asm
	setp.ne.s32 	%p14, %r126, %r16;
	@%p14 bra 	$L__BB0_20;

$L__BB0_21:
	@%p3 bra 	$L__BB0_23;

	add.s32 	%r130, %r122, 1;
	st.global.u32 	[%rd43+2304], %r130;

$L__BB0_23:
	ret;

}
	// .globl	custom_allreduce_2stage_bf16
.visible .entry custom_allreduce_2stage_bf16(
	.param .u64 custom_allreduce_2stage_bf16_param_0,
	.param .align 16 .b8 custom_allreduce_2stage_bf16_param_1[64],
	.param .u64 custom_allreduce_2stage_bf16_param_2,
	.param .u64 custom_allreduce_2stage_bf16_param_3,
	.param .u32 custom_allreduce_2stage_bf16_param_4,
	.param .u32 custom_allreduce_2stage_bf16_param_5,
	.param .u32 custom_allreduce_2stage_bf16_param_6
)
.maxntid 512, 1, 1
.minnctapersm 1
{
	.local .align 16 .b8 	__local_depot1[192];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<56>;
	.reg .b16 	%rs<89>;
	.reg .f32 	%f<201>;
	.reg .b32 	%r<323>;
	.reg .b64 	%rd<193>;


	mov.u64 	%SPL, __local_depot1;
	ld.param.u64 	%rd25, [custom_allreduce_2stage_bf16_param_0];
	ld.param.u64 	%rd23, [custom_allreduce_2stage_bf16_param_2];
	ld.param.u64 	%rd24, [custom_allreduce_2stage_bf16_param_3];
	ld.param.u32 	%r53, [custom_allreduce_2stage_bf16_param_4];
	ld.param.u32 	%r54, [custom_allreduce_2stage_bf16_param_5];
	ld.param.u32 	%r55, [custom_allreduce_2stage_bf16_param_6];
	cvta.to.global.u64 	%rd1, %rd25;
	add.u64 	%rd2, %SPL, 0;
	add.u64 	%rd3, %SPL, 64;
	add.u64 	%rd4, %SPL, 128;
	ld.param.u64 	%rd29, [custom_allreduce_2stage_bf16_param_1+8];
	ld.param.u64 	%rd30, [custom_allreduce_2stage_bf16_param_1];
	st.local.v2.u64 	[%rd2], {%rd30, %rd29};
	ld.param.u64 	%rd31, [custom_allreduce_2stage_bf16_param_1+24];
	ld.param.u64 	%rd32, [custom_allreduce_2stage_bf16_param_1+16];
	st.local.v2.u64 	[%rd2+16], {%rd32, %rd31};
	ld.param.u64 	%rd33, [custom_allreduce_2stage_bf16_param_1+40];
	ld.param.u64 	%rd34, [custom_allreduce_2stage_bf16_param_1+32];
	st.local.v2.u64 	[%rd2+32], {%rd34, %rd33};
	ld.param.u64 	%rd35, [custom_allreduce_2stage_bf16_param_1+56];
	ld.param.u64 	%rd36, [custom_allreduce_2stage_bf16_param_1+48];
	st.local.v2.u64 	[%rd2+48], {%rd36, %rd35};
	mov.u32 	%r1, %tid.x;
	div.s32 	%r2, %r55, %r54;
	add.s32 	%r3, %r54, -1;
	mul.lo.s32 	%r56, %r2, %r54;
	sub.s32 	%r57, %r55, %r56;
	add.s32 	%r4, %r57, %r2;
	setp.lt.s32 	%p1, %r54, 1;
	@%p1 bra 	$L__BB1_7;

	and.b32  	%r313, %r54, 3;
	setp.lt.u32 	%p2, %r3, 3;
	mov.u32 	%r311, 0;
	@%p2 bra 	$L__BB1_4;

	sub.s32 	%r310, %r54, %r313;

$L__BB1_3:
	add.s32 	%r60, %r311, %r53;
	rem.s32 	%r61, %r60, %r54;
	mul.wide.s32 	%rd37, %r61, 8;
	add.s64 	%rd38, %rd1, %rd37;
	mul.wide.s32 	%rd39, %r311, 8;
	add.s64 	%rd40, %rd3, %rd39;
	add.s64 	%rd41, %rd2, %rd37;
	ld.local.u64 	%rd42, [%rd41];
	add.s64 	%rd43, %rd4, %rd39;
	add.s32 	%r62, %r60, 1;
	rem.s32 	%r63, %r62, %r54;
	mul.wide.s32 	%rd44, %r63, 8;
	add.s64 	%rd45, %rd1, %rd44;
	ld.global.nc.u64 	%rd46, [%rd38];
	ld.global.nc.u64 	%rd47, [%rd45];
	st.local.v2.u64 	[%rd40], {%rd46, %rd47};
	add.s64 	%rd48, %rd2, %rd44;
	ld.local.u64 	%rd49, [%rd48];
	add.s64 	%rd50, %rd42, 2560;
	add.s64 	%rd51, %rd49, 2560;
	st.local.v2.u64 	[%rd43], {%rd50, %rd51};
	add.s32 	%r64, %r60, 2;
	rem.s32 	%r65, %r64, %r54;
	mul.wide.s32 	%rd52, %r65, 8;
	add.s64 	%rd53, %rd1, %rd52;
	add.s64 	%rd54, %rd2, %rd52;
	ld.local.u64 	%rd55, [%rd54];
	add.s32 	%r66, %r60, 3;
	rem.s32 	%r67, %r66, %r54;
	mul.wide.s32 	%rd56, %r67, 8;
	add.s64 	%rd57, %rd1, %rd56;
	ld.global.nc.u64 	%rd58, [%rd57];
	ld.global.nc.u64 	%rd59, [%rd53];
	st.local.v2.u64 	[%rd40+16], {%rd59, %rd58};
	add.s64 	%rd60, %rd2, %rd56;
	ld.local.u64 	%rd61, [%rd60];
	add.s64 	%rd62, %rd55, 2560;
	add.s64 	%rd63, %rd61, 2560;
	st.local.v2.u64 	[%rd43+16], {%rd62, %rd63};
	add.s32 	%r311, %r311, 4;
	add.s32 	%r310, %r310, -4;
	setp.ne.s32 	%p3, %r310, 0;
	@%p3 bra 	$L__BB1_3;

$L__BB1_4:
	setp.eq.s32 	%p4, %r313, 0;
	@%p4 bra 	$L__BB1_7;

	mul.wide.s32 	%rd64, %r311, 8;
	add.s64 	%rd192, %rd4, %rd64;
	add.s64 	%rd191, %rd3, %rd64;
	add.s32 	%r312, %r311, %r53;

$L__BB1_6:
	.pragma "nounroll";
	rem.s32 	%r68, %r312, %r54;
	mul.wide.s32 	%rd65, %r68, 8;
	add.s64 	%rd66, %rd1, %rd65;
	ld.global.nc.u64 	%rd67, [%rd66];
	st.local.u64 	[%rd191], %rd67;
	add.s64 	%rd68, %rd2, %rd65;
	ld.local.u64 	%rd69, [%rd68];
	add.s64 	%rd70, %rd69, 2560;
	st.local.u64 	[%rd192], %rd70;
	add.s64 	%rd192, %rd192, 8;
	add.s64 	%rd191, %rd191, 8;
	add.s32 	%r312, %r312, 1;
	add.s32 	%r313, %r313, -1;
	setp.ne.s32 	%p5, %r313, 0;
	@%p5 bra 	$L__BB1_6;

$L__BB1_7:
	ld.local.u64 	%rd11, [%rd4];
	mov.u32 	%r69, %ctaid.x;
	cvt.u64.u32 	%rd12, %r69;
	cvta.to.global.u64 	%rd71, %rd23;
	mul.wide.u32 	%rd72, %r69, 4;
	add.s64 	%rd73, %rd71, %rd72;
	ld.global.u32 	%r70, [%rd73+2304];
	add.s32 	%r17, %r70, 1;
	mul.wide.u32 	%rd74, %r1, 8;
	add.s64 	%rd13, %rd2, %rd74;
	setp.ge.u32 	%p6, %r1, %r54;
	@%p6 bra 	$L__BB1_10;

	ld.local.u64 	%rd76, [%rd13];
	shl.b64 	%rd77, %rd12, 5;
	add.s64 	%rd78, %rd76, %rd77;
	mul.wide.s32 	%rd79, %r53, 4;
	add.s64 	%rd75, %rd78, %rd79;
	add.s64 	%rd80, %rd23, %rd77;
	mul.wide.u32 	%rd81, %r1, 4;
	add.s64 	%rd14, %rd80, %rd81;
	// begin inline asm
	st.volatile.global.u32 [%rd75], %r17;
	// end inline asm

$L__BB1_9:
	// begin inline asm
	ld.volatile.global.u32 %r72, [%rd14];
	// end inline asm
	setp.ne.s32 	%p7, %r72, %r17;
	@%p7 bra 	$L__BB1_9;

$L__BB1_10:
	bar.sync 	0;
	setp.ne.s32 	%p8, %r1, 0;
	@%p8 bra 	$L__BB1_12;

	shl.b64 	%rd84, %rd12, 2;
	add.s64 	%rd85, %rd71, %rd84;
	add.s32 	%r306, %r70, 1;
	st.global.u32 	[%rd85+2304], %r306;

$L__BB1_12:
	mov.u32 	%r75, %ntid.x;
	mad.lo.s32 	%r319, %r69, %r75, %r1;
	mul.lo.s32 	%r78, %r2, %r53;
	add.s32 	%r314, %r78, %r319;
	setp.eq.s32 	%p9, %r3, %r53;
	add.s32 	%r80, %r78, %r2;
	selp.b32 	%r81, %r55, %r80, %p9;
	setp.ge.s32 	%p10, %r314, %r81;
	@%p10 bra 	$L__BB1_25;

	ld.local.u64 	%rd15, [%rd3];
	setp.gt.s32 	%p11, %r54, 1;
	@%p11 bra 	$L__BB1_16;
	bra.uni 	$L__BB1_14;

$L__BB1_16:
	add.s32 	%r99, %r54, -2;

$L__BB1_17:
	setp.lt.u32 	%p14, %r99, 3;
	cvt.s64.s32 	%rd16, %r314;
	mul.wide.s32 	%rd90, %r314, 16;
	add.s64 	%rd91, %rd15, %rd90;
	ld.v4.u32 	{%r100, %r101, %r102, %r103}, [%rd91];
	mov.b32 	{%rs17, %rs18}, %r100;
	mov.u32 	%r318, 1;
	mov.b32 	{%rs19, %rs20}, %r101;
	mov.b32 	{%rs21, %rs22}, %r102;
	mov.b32 	{%rs23, %rs24}, %r103;
	// begin inline asm
	{ mov.b32 %f200, {0,%rs17};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f199, {0,%rs18};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f198, {0,%rs19};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f197, {0,%rs20};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f196, {0,%rs21};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f195, {0,%rs22};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f194, {0,%rs23};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f193, {0,%rs24};}

	// end inline asm
	@%p14 bra 	$L__BB1_20;

	and.b32  	%r110, %r3, 3;
	sub.s32 	%r317, %r3, %r110;

$L__BB1_19:
	mul.wide.s32 	%rd92, %r318, 8;
	add.s64 	%rd93, %rd3, %rd92;
	ld.local.u64 	%rd94, [%rd93];
	shl.b64 	%rd95, %rd16, 4;
	add.s64 	%rd96, %rd94, %rd95;
	ld.v4.u32 	{%r111, %r112, %r113, %r114}, [%rd96];
	mov.b32 	{%rs25, %rs26}, %r111;
	mov.b32 	{%rs27, %rs28}, %r112;
	mov.b32 	{%rs29, %rs30}, %r113;
	mov.b32 	{%rs31, %rs32}, %r114;
	// begin inline asm
	{ mov.b32 %f89, {0,%rs25};}

	// end inline asm
	add.ftz.f32 	%f121, %f89, %f200;
	// begin inline asm
	{ mov.b32 %f90, {0,%rs26};}

	// end inline asm
	add.ftz.f32 	%f122, %f90, %f199;
	// begin inline asm
	{ mov.b32 %f91, {0,%rs27};}

	// end inline asm
	add.ftz.f32 	%f123, %f91, %f198;
	// begin inline asm
	{ mov.b32 %f92, {0,%rs28};}

	// end inline asm
	add.ftz.f32 	%f124, %f92, %f197;
	// begin inline asm
	{ mov.b32 %f93, {0,%rs29};}

	// end inline asm
	add.ftz.f32 	%f125, %f93, %f196;
	// begin inline asm
	{ mov.b32 %f94, {0,%rs30};}

	// end inline asm
	add.ftz.f32 	%f126, %f94, %f195;
	// begin inline asm
	{ mov.b32 %f95, {0,%rs31};}

	// end inline asm
	add.ftz.f32 	%f127, %f95, %f194;
	// begin inline asm
	{ mov.b32 %f96, {0,%rs32};}

	// end inline asm
	add.ftz.f32 	%f128, %f96, %f193;
	ld.local.u64 	%rd97, [%rd93+8];
	add.s64 	%rd98, %rd97, %rd95;
	ld.v4.u32 	{%r119, %r120, %r121, %r122}, [%rd98];
	mov.b32 	{%rs33, %rs34}, %r119;
	mov.b32 	{%rs35, %rs36}, %r120;
	mov.b32 	{%rs37, %rs38}, %r121;
	mov.b32 	{%rs39, %rs40}, %r122;
	// begin inline asm
	{ mov.b32 %f97, {0,%rs33};}

	// end inline asm
	add.ftz.f32 	%f129, %f97, %f121;
	// begin inline asm
	{ mov.b32 %f98, {0,%rs34};}

	// end inline asm
	add.ftz.f32 	%f130, %f98, %f122;
	// begin inline asm
	{ mov.b32 %f99, {0,%rs35};}

	// end inline asm
	add.ftz.f32 	%f131, %f99, %f123;
	// begin inline asm
	{ mov.b32 %f100, {0,%rs36};}

	// end inline asm
	add.ftz.f32 	%f132, %f100, %f124;
	// begin inline asm
	{ mov.b32 %f101, {0,%rs37};}

	// end inline asm
	add.ftz.f32 	%f133, %f101, %f125;
	// begin inline asm
	{ mov.b32 %f102, {0,%rs38};}

	// end inline asm
	add.ftz.f32 	%f134, %f102, %f126;
	// begin inline asm
	{ mov.b32 %f103, {0,%rs39};}

	// end inline asm
	add.ftz.f32 	%f135, %f103, %f127;
	// begin inline asm
	{ mov.b32 %f104, {0,%rs40};}

	// end inline asm
	add.ftz.f32 	%f136, %f104, %f128;
	ld.local.u64 	%rd99, [%rd93+16];
	add.s64 	%rd100, %rd99, %rd95;
	ld.v4.u32 	{%r127, %r128, %r129, %r130}, [%rd100];
	mov.b32 	{%rs41, %rs42}, %r127;
	mov.b32 	{%rs43, %rs44}, %r128;
	mov.b32 	{%rs45, %rs46}, %r129;
	mov.b32 	{%rs47, %rs48}, %r130;
	// begin inline asm
	{ mov.b32 %f105, {0,%rs41};}

	// end inline asm
	add.ftz.f32 	%f137, %f105, %f129;
	// begin inline asm
	{ mov.b32 %f106, {0,%rs42};}

	// end inline asm
	add.ftz.f32 	%f138, %f106, %f130;
	// begin inline asm
	{ mov.b32 %f107, {0,%rs43};}

	// end inline asm
	add.ftz.f32 	%f139, %f107, %f131;
	// begin inline asm
	{ mov.b32 %f108, {0,%rs44};}

	// end inline asm
	add.ftz.f32 	%f140, %f108, %f132;
	// begin inline asm
	{ mov.b32 %f109, {0,%rs45};}

	// end inline asm
	add.ftz.f32 	%f141, %f109, %f133;
	// begin inline asm
	{ mov.b32 %f110, {0,%rs46};}

	// end inline asm
	add.ftz.f32 	%f142, %f110, %f134;
	// begin inline asm
	{ mov.b32 %f111, {0,%rs47};}

	// end inline asm
	add.ftz.f32 	%f143, %f111, %f135;
	// begin inline asm
	{ mov.b32 %f112, {0,%rs48};}

	// end inline asm
	add.ftz.f32 	%f144, %f112, %f136;
	ld.local.u64 	%rd101, [%rd93+24];
	add.s64 	%rd102, %rd101, %rd95;
	ld.v4.u32 	{%r135, %r136, %r137, %r138}, [%rd102];
	mov.b32 	{%rs49, %rs50}, %r135;
	mov.b32 	{%rs51, %rs52}, %r136;
	mov.b32 	{%rs53, %rs54}, %r137;
	mov.b32 	{%rs55, %rs56}, %r138;
	// begin inline asm
	{ mov.b32 %f113, {0,%rs49};}

	// end inline asm
	add.ftz.f32 	%f200, %f113, %f137;
	// begin inline asm
	{ mov.b32 %f114, {0,%rs50};}

	// end inline asm
	add.ftz.f32 	%f199, %f114, %f138;
	// begin inline asm
	{ mov.b32 %f115, {0,%rs51};}

	// end inline asm
	add.ftz.f32 	%f198, %f115, %f139;
	// begin inline asm
	{ mov.b32 %f116, {0,%rs52};}

	// end inline asm
	add.ftz.f32 	%f197, %f116, %f140;
	// begin inline asm
	{ mov.b32 %f117, {0,%rs53};}

	// end inline asm
	add.ftz.f32 	%f196, %f117, %f141;
	// begin inline asm
	{ mov.b32 %f118, {0,%rs54};}

	// end inline asm
	add.ftz.f32 	%f195, %f118, %f142;
	// begin inline asm
	{ mov.b32 %f119, {0,%rs55};}

	// end inline asm
	add.ftz.f32 	%f194, %f119, %f143;
	// begin inline asm
	{ mov.b32 %f120, {0,%rs56};}

	// end inline asm
	add.ftz.f32 	%f193, %f120, %f144;
	add.s32 	%r318, %r318, 4;
	add.s32 	%r317, %r317, -4;
	setp.ne.s32 	%p15, %r317, 0;
	@%p15 bra 	$L__BB1_19;

$L__BB1_20:
	and.b32  	%r144, %r3, 3;
	setp.eq.s32 	%p16, %r144, 0;
	@%p16 bra 	$L__BB1_24;

	setp.eq.s32 	%p17, %r144, 1;
	mul.wide.s32 	%rd103, %r318, 8;
	add.s64 	%rd17, %rd3, %rd103;
	ld.local.u64 	%rd104, [%rd17];
	shl.b64 	%rd105, %rd16, 4;
	add.s64 	%rd106, %rd104, %rd105;
	ld.v4.u32 	{%r147, %r148, %r149, %r150}, [%rd106];
	mov.b32 	{%rs57, %rs58}, %r147;
	mov.b32 	{%rs59, %rs60}, %r148;
	mov.b32 	{%rs61, %rs62}, %r149;
	mov.b32 	{%rs63, %rs64}, %r150;
	// begin inline asm
	{ mov.b32 %f145, {0,%rs57};}

	// end inline asm
	add.ftz.f32 	%f200, %f145, %f200;
	// begin inline asm
	{ mov.b32 %f146, {0,%rs58};}

	// end inline asm
	add.ftz.f32 	%f199, %f146, %f199;
	// begin inline asm
	{ mov.b32 %f147, {0,%rs59};}

	// end inline asm
	add.ftz.f32 	%f198, %f147, %f198;
	// begin inline asm
	{ mov.b32 %f148, {0,%rs60};}

	// end inline asm
	add.ftz.f32 	%f197, %f148, %f197;
	// begin inline asm
	{ mov.b32 %f149, {0,%rs61};}

	// end inline asm
	add.ftz.f32 	%f196, %f149, %f196;
	// begin inline asm
	{ mov.b32 %f150, {0,%rs62};}

	// end inline asm
	add.ftz.f32 	%f195, %f150, %f195;
	// begin inline asm
	{ mov.b32 %f151, {0,%rs63};}

	// end inline asm
	add.ftz.f32 	%f194, %f151, %f194;
	// begin inline asm
	{ mov.b32 %f152, {0,%rs64};}

	// end inline asm
	add.ftz.f32 	%f193, %f152, %f193;
	@%p17 bra 	$L__BB1_24;

	setp.eq.s32 	%p18, %r144, 2;
	ld.local.u64 	%rd107, [%rd17+8];
	add.s64 	%rd109, %rd107, %rd105;
	ld.v4.u32 	{%r157, %r158, %r159, %r160}, [%rd109];
	mov.b32 	{%rs65, %rs66}, %r157;
	mov.b32 	{%rs67, %rs68}, %r158;
	mov.b32 	{%rs69, %rs70}, %r159;
	mov.b32 	{%rs71, %rs72}, %r160;
	// begin inline asm
	{ mov.b32 %f153, {0,%rs65};}

	// end inline asm
	add.ftz.f32 	%f200, %f153, %f200;
	// begin inline asm
	{ mov.b32 %f154, {0,%rs66};}

	// end inline asm
	add.ftz.f32 	%f199, %f154, %f199;
	// begin inline asm
	{ mov.b32 %f155, {0,%rs67};}

	// end inline asm
	add.ftz.f32 	%f198, %f155, %f198;
	// begin inline asm
	{ mov.b32 %f156, {0,%rs68};}

	// end inline asm
	add.ftz.f32 	%f197, %f156, %f197;
	// begin inline asm
	{ mov.b32 %f157, {0,%rs69};}

	// end inline asm
	add.ftz.f32 	%f196, %f157, %f196;
	// begin inline asm
	{ mov.b32 %f158, {0,%rs70};}

	// end inline asm
	add.ftz.f32 	%f195, %f158, %f195;
	// begin inline asm
	{ mov.b32 %f159, {0,%rs71};}

	// end inline asm
	add.ftz.f32 	%f194, %f159, %f194;
	// begin inline asm
	{ mov.b32 %f160, {0,%rs72};}

	// end inline asm
	add.ftz.f32 	%f193, %f160, %f193;
	@%p18 bra 	$L__BB1_24;

	ld.local.u64 	%rd110, [%rd17+16];
	add.s64 	%rd112, %rd110, %rd105;
	ld.v4.u32 	{%r165, %r166, %r167, %r168}, [%rd112];
	mov.b32 	{%rs73, %rs74}, %r165;
	mov.b32 	{%rs75, %rs76}, %r166;
	mov.b32 	{%rs77, %rs78}, %r167;
	mov.b32 	{%rs79, %rs80}, %r168;
	// begin inline asm
	{ mov.b32 %f161, {0,%rs73};}

	// end inline asm
	add.ftz.f32 	%f200, %f161, %f200;
	// begin inline asm
	{ mov.b32 %f162, {0,%rs74};}

	// end inline asm
	add.ftz.f32 	%f199, %f162, %f199;
	// begin inline asm
	{ mov.b32 %f163, {0,%rs75};}

	// end inline asm
	add.ftz.f32 	%f198, %f163, %f198;
	// begin inline asm
	{ mov.b32 %f164, {0,%rs76};}

	// end inline asm
	add.ftz.f32 	%f197, %f164, %f197;
	// begin inline asm
	{ mov.b32 %f165, {0,%rs77};}

	// end inline asm
	add.ftz.f32 	%f196, %f165, %f196;
	// begin inline asm
	{ mov.b32 %f166, {0,%rs78};}

	// end inline asm
	add.ftz.f32 	%f195, %f166, %f195;
	// begin inline asm
	{ mov.b32 %f167, {0,%rs79};}

	// end inline asm
	add.ftz.f32 	%f194, %f167, %f194;
	// begin inline asm
	{ mov.b32 %f168, {0,%rs80};}

	// end inline asm
	add.ftz.f32 	%f193, %f168, %f193;

$L__BB1_24:
	cvt.u32.u64 	%r174, %rd16;
	sub.s32 	%r175, %r174, %r78;
	mul.wide.s32 	%rd113, %r175, 16;
	add.s64 	%rd114, %rd11, %rd113;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs82, %f199;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs81, %f200;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs84, %f197;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs83, %f198;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs86, %f195;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs85, %f196;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs88, %f193;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs87, %f194;}

	// end inline asm
	mov.b32 	%r176, {%rs87, %rs88};
	mov.b32 	%r177, {%rs85, %rs86};
	mov.b32 	%r178, {%rs83, %rs84};
	mov.b32 	%r179, {%rs81, %rs82};
	st.v4.u32 	[%rd114], {%r179, %r178, %r177, %r176};
	mov.u32 	%r181, %nctaid.x;
	mad.lo.s32 	%r314, %r181, %r75, %r174;
	setp.lt.s32 	%p20, %r314, %r81;
	@%p20 bra 	$L__BB1_17;
	bra.uni 	$L__BB1_25;

$L__BB1_14:
	mov.u32 	%r82, %nctaid.x;
	mul.lo.s32 	%r20, %r82, %r75;

$L__BB1_15:
	mul.wide.s32 	%rd86, %r314, 16;
	add.s64 	%rd87, %rd15, %rd86;
	ld.v4.u32 	{%r85, %r86, %r87, %r88}, [%rd87];
	mov.b32 	{%rs1, %rs2}, %r85;
	mov.b32 	{%rs3, %rs4}, %r86;
	mov.b32 	{%rs5, %rs6}, %r87;
	mov.b32 	{%rs7, %rs8}, %r88;
	// begin inline asm
	{ mov.b32 %f65, {0,%rs1};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f66, {0,%rs2};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f67, {0,%rs3};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f68, {0,%rs4};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f69, {0,%rs5};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f70, {0,%rs6};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f71, {0,%rs7};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f72, {0,%rs8};}

	// end inline asm
	sub.s32 	%r93, %r314, %r78;
	mul.wide.s32 	%rd88, %r93, 16;
	add.s64 	%rd89, %rd11, %rd88;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs10, %f66;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs9, %f65;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs12, %f68;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs11, %f67;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs14, %f70;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs13, %f69;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs16, %f72;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs15, %f71;}

	// end inline asm
	mov.b32 	%r94, {%rs15, %rs16};
	mov.b32 	%r95, {%rs13, %rs14};
	mov.b32 	%r96, {%rs11, %rs12};
	mov.b32 	%r97, {%rs9, %rs10};
	st.v4.u32 	[%rd89], {%r97, %r96, %r95, %r94};
	add.s32 	%r314, %r314, %r20;
	setp.lt.s32 	%p13, %r314, %r81;
	@%p13 bra 	$L__BB1_15;

$L__BB1_25:
	bar.sync 	0;
	ld.global.u32 	%r187, [%rd73+2304];
	add.s32 	%r32, %r187, 1;
	@%p6 bra 	$L__BB1_28;

	ld.local.u64 	%rd119, [%rd13];
	mul.wide.u32 	%rd120, %r69, 32;
	add.s64 	%rd121, %rd119, %rd120;
	mul.wide.s32 	%rd122, %r53, 4;
	add.s64 	%rd123, %rd121, %rd122;
	add.s64 	%rd118, %rd123, 1152;
	add.s64 	%rd124, %rd23, %rd120;
	mul.wide.u32 	%rd125, %r1, 4;
	add.s64 	%rd126, %rd124, %rd125;
	add.s64 	%rd18, %rd126, 1152;
	// begin inline asm
	st.release.sys.global.u32 [%rd118], %r32;
	// end inline asm

$L__BB1_27:
	// begin inline asm
	ld.acquire.sys.global.u32 %r191, [%rd18];
	// end inline asm
	setp.ne.s32 	%p22, %r191, %r32;
	@%p22 bra 	$L__BB1_27;

$L__BB1_28:
	bar.sync 	0;
	@%p8 bra 	$L__BB1_30;

	add.s32 	%r307, %r187, 1;
	st.global.u32 	[%rd73+2304], %r307;

$L__BB1_30:
	setp.ge.s32 	%p24, %r319, %r4;
	@%p24 bra 	$L__BB1_54;

	@%p1 bra 	$L__BB1_54;

	and.b32  	%r33, %r54, 3;
	sub.s32 	%r34, %r54, %r33;
	mov.u32 	%r201, %nctaid.x;
	mul.lo.s32 	%r36, %r201, %r75;
	cvta.to.global.u64 	%rd132, %rd24;

$L__BB1_33:
	cvt.s64.s32 	%rd19, %r319;
	setp.lt.u32 	%p26, %r3, 3;
	mov.u32 	%r322, 0;
	@%p26 bra 	$L__BB1_44;

	mov.u32 	%r321, %r34;

$L__BB1_35:
	cvt.u32.u64 	%r204, %rd19;
	setp.ge.s32 	%p27, %r204, %r2;
	add.s32 	%r205, %r322, %r53;
	rem.s32 	%r41, %r205, %r54;
	setp.ne.s32 	%p28, %r41, %r3;
	mul.wide.s32 	%rd131, %r322, 8;
	add.s64 	%rd20, %rd4, %rd131;
	and.pred  	%p29, %p27, %p28;
	@%p29 bra 	$L__BB1_37;

	mad.lo.s32 	%r207, %r41, %r2, %r204;
	ld.local.u64 	%rd133, [%rd20];
	shl.b64 	%rd134, %rd19, 4;
	add.s64 	%rd135, %rd133, %rd134;
	ld.v4.u32 	{%r208, %r209, %r210, %r211}, [%rd135];
	mul.wide.s32 	%rd136, %r207, 16;
	add.s64 	%rd137, %rd132, %rd136;
	st.global.v4.u32 	[%rd137], {%r208, %r209, %r210, %r211};

$L__BB1_37:
	add.s32 	%r218, %r205, 1;
	rem.s32 	%r42, %r218, %r54;
	setp.ne.s32 	%p31, %r42, %r3;
	and.pred  	%p32, %p27, %p31;
	@%p32 bra 	$L__BB1_39;

	mad.lo.s32 	%r220, %r42, %r2, %r204;
	ld.local.u64 	%rd139, [%rd20+8];
	shl.b64 	%rd140, %rd19, 4;
	add.s64 	%rd141, %rd139, %rd140;
	ld.v4.u32 	{%r221, %r222, %r223, %r224}, [%rd141];
	mul.wide.s32 	%rd142, %r220, 16;
	add.s64 	%rd143, %rd132, %rd142;
	st.global.v4.u32 	[%rd143], {%r221, %r222, %r223, %r224};

$L__BB1_39:
	add.s32 	%r231, %r205, 2;
	rem.s32 	%r43, %r231, %r54;
	setp.ne.s32 	%p34, %r43, %r3;
	and.pred  	%p35, %p27, %p34;
	@%p35 bra 	$L__BB1_41;

	mad.lo.s32 	%r233, %r43, %r2, %r204;
	ld.local.u64 	%rd145, [%rd20+16];
	shl.b64 	%rd146, %rd19, 4;
	add.s64 	%rd147, %rd145, %rd146;
	ld.v4.u32 	{%r234, %r235, %r236, %r237}, [%rd147];
	mul.wide.s32 	%rd148, %r233, 16;
	add.s64 	%rd149, %rd132, %rd148;
	st.global.v4.u32 	[%rd149], {%r234, %r235, %r236, %r237};

$L__BB1_41:
	add.s32 	%r244, %r205, 3;
	rem.s32 	%r44, %r244, %r54;
	setp.ne.s32 	%p37, %r44, %r3;
	and.pred  	%p38, %p27, %p37;
	@%p38 bra 	$L__BB1_43;

	mad.lo.s32 	%r246, %r44, %r2, %r204;
	ld.local.u64 	%rd151, [%rd20+24];
	shl.b64 	%rd152, %rd19, 4;
	add.s64 	%rd153, %rd151, %rd152;
	ld.v4.u32 	{%r247, %r248, %r249, %r250}, [%rd153];
	mul.wide.s32 	%rd154, %r246, 16;
	add.s64 	%rd155, %rd132, %rd154;
	st.global.v4.u32 	[%rd155], {%r247, %r248, %r249, %r250};

$L__BB1_43:
	add.s32 	%r322, %r322, 4;
	add.s32 	%r321, %r321, -4;
	setp.ne.s32 	%p39, %r321, 0;
	@%p39 bra 	$L__BB1_35;

$L__BB1_44:
	setp.eq.s32 	%p40, %r33, 0;
	@%p40 bra 	$L__BB1_53;

	cvt.u32.u64 	%r255, %rd19;
	setp.ge.s32 	%p41, %r255, %r2;
	add.s32 	%r256, %r322, %r53;
	rem.s32 	%r48, %r256, %r54;
	setp.ne.s32 	%p42, %r48, %r3;
	mul.wide.s32 	%rd156, %r322, 8;
	add.s64 	%rd21, %rd4, %rd156;
	and.pred  	%p43, %p41, %p42;
	@%p43 bra 	$L__BB1_47;

	mad.lo.s32 	%r259, %r48, %r2, %r255;
	ld.local.u64 	%rd158, [%rd21];
	shl.b64 	%rd159, %rd19, 4;
	add.s64 	%rd160, %rd158, %rd159;
	ld.v4.u32 	{%r260, %r261, %r262, %r263}, [%rd160];
	mul.wide.s32 	%rd161, %r259, 16;
	add.s64 	%rd162, %rd132, %rd161;
	st.global.v4.u32 	[%rd162], {%r260, %r261, %r262, %r263};

$L__BB1_47:
	setp.eq.s32 	%p44, %r33, 1;
	@%p44 bra 	$L__BB1_53;

	add.s32 	%r270, %r256, 1;
	rem.s32 	%r49, %r270, %r54;
	setp.ne.s32 	%p46, %r49, %r3;
	and.pred  	%p47, %p41, %p46;
	@%p47 bra 	$L__BB1_50;

	mad.lo.s32 	%r273, %r49, %r2, %r255;
	ld.local.u64 	%rd164, [%rd21+8];
	shl.b64 	%rd165, %rd19, 4;
	add.s64 	%rd166, %rd164, %rd165;
	ld.v4.u32 	{%r274, %r275, %r276, %r277}, [%rd166];
	mul.wide.s32 	%rd167, %r273, 16;
	add.s64 	%rd168, %rd132, %rd167;
	st.global.v4.u32 	[%rd168], {%r274, %r275, %r276, %r277};

$L__BB1_50:
	setp.eq.s32 	%p48, %r33, 2;
	@%p48 bra 	$L__BB1_53;

	add.s32 	%r284, %r256, 2;
	rem.s32 	%r50, %r284, %r54;
	setp.ne.s32 	%p50, %r50, %r3;
	and.pred  	%p51, %p41, %p50;
	@%p51 bra 	$L__BB1_53;

	mad.lo.s32 	%r287, %r50, %r2, %r255;
	ld.local.u64 	%rd170, [%rd21+16];
	shl.b64 	%rd171, %rd19, 4;
	add.s64 	%rd172, %rd170, %rd171;
	ld.v4.u32 	{%r288, %r289, %r290, %r291}, [%rd172];
	mul.wide.s32 	%rd173, %r287, 16;
	add.s64 	%rd174, %rd132, %rd173;
	st.global.v4.u32 	[%rd174], {%r288, %r289, %r290, %r291};

$L__BB1_53:
	cvt.u32.u64 	%r296, %rd19;
	add.s32 	%r319, %r296, %r36;
	setp.lt.s32 	%p52, %r319, %r4;
	@%p52 bra 	$L__BB1_33;

$L__BB1_54:
	bar.sync 	0;
	ld.global.u32 	%r299, [%rd73+2304];
	add.s32 	%r52, %r299, 1;
	@%p6 bra 	$L__BB1_57;

	ld.local.u64 	%rd179, [%rd13];
	mul.wide.u32 	%rd180, %r69, 32;
	add.s64 	%rd181, %rd179, %rd180;
	mul.wide.s32 	%rd182, %r53, 4;
	add.s64 	%rd183, %rd181, %rd182;
	add.s64 	%rd178, %rd183, 1152;
	add.s64 	%rd184, %rd23, %rd180;
	mul.wide.u32 	%rd185, %r1, 4;
	add.s64 	%rd186, %rd184, %rd185;
	add.s64 	%rd22, %rd186, 1152;
	// begin inline asm
	st.volatile.global.u32 [%rd178], %r52;
	// end inline asm

$L__BB1_56:
	// begin inline asm
	ld.volatile.global.u32 %r303, [%rd22];
	// end inline asm
	setp.ne.s32 	%p54, %r303, %r52;
	@%p54 bra 	$L__BB1_56;

$L__BB1_57:
	@%p8 bra 	$L__BB1_59;

	add.s32 	%r308, %r299, 1;
	st.global.u32 	[%rd73+2304], %r308;

$L__BB1_59:
	ret;

}
	// .globl	custom_allreduce_1stage_f32
.visible .entry custom_allreduce_1stage_f32(
	.param .u64 custom_allreduce_1stage_f32_param_0,
	.param .align 16 .b8 custom_allreduce_1stage_f32_param_1[64],
	.param .u64 custom_allreduce_1stage_f32_param_2,
	.param .u64 custom_allreduce_1stage_f32_param_3,
	.param .u32 custom_allreduce_1stage_f32_param_4,
	.param .u32 custom_allreduce_1stage_f32_param_5,
	.param .u32 custom_allreduce_1stage_f32_param_6
)
.maxntid 512, 1, 1
.minnctapersm 1
{
	.local .align 16 .b8 	__local_depot2[128];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<19>;
	.reg .f32 	%f<133>;
	.reg .b32 	%r<70>;
	.reg .b64 	%rd<118>;


	mov.u64 	%SPL, __local_depot2;
	ld.param.u64 	%rd21, [custom_allreduce_1stage_f32_param_0];
	ld.param.u64 	%rd19, [custom_allreduce_1stage_f32_param_2];
	ld.param.u64 	%rd20, [custom_allreduce_1stage_f32_param_3];
	ld.param.u32 	%r24, [custom_allreduce_1stage_f32_param_4];
	ld.param.u32 	%r25, [custom_allreduce_1stage_f32_param_5];
	ld.param.u32 	%r26, [custom_allreduce_1stage_f32_param_6];
	cvta.to.global.u64 	%rd1, %rd20;
	add.u64 	%rd23, %SPL, 0;
	add.u64 	%rd2, %SPL, 64;
	ld.param.u64 	%rd25, [custom_allreduce_1stage_f32_param_1+8];
	ld.param.u64 	%rd26, [custom_allreduce_1stage_f32_param_1];
	st.local.v2.u64 	[%rd23], {%rd26, %rd25};
	ld.param.u64 	%rd27, [custom_allreduce_1stage_f32_param_1+24];
	ld.param.u64 	%rd28, [custom_allreduce_1stage_f32_param_1+16];
	st.local.v2.u64 	[%rd23+16], {%rd28, %rd27};
	ld.param.u64 	%rd29, [custom_allreduce_1stage_f32_param_1+40];
	ld.param.u64 	%rd30, [custom_allreduce_1stage_f32_param_1+32];
	st.local.v2.u64 	[%rd23+32], {%rd30, %rd29};
	ld.param.u64 	%rd31, [custom_allreduce_1stage_f32_param_1+56];
	ld.param.u64 	%rd32, [custom_allreduce_1stage_f32_param_1+48];
	st.local.v2.u64 	[%rd23+48], {%rd32, %rd31};
	cvta.to.global.u64 	%rd33, %rd21;
	ld.global.nc.v2.u64 	{%rd34, %rd35}, [%rd33];
	ld.global.nc.v2.u64 	{%rd37, %rd38}, [%rd33+16];
	ld.global.nc.v2.u64 	{%rd41, %rd42}, [%rd33+32];
	ld.global.nc.v2.u64 	{%rd45, %rd46}, [%rd33+48];
	st.local.v2.u64 	[%rd2], {%rd34, %rd35};
	st.local.v2.u64 	[%rd2+16], {%rd37, %rd38};
	st.local.v2.u64 	[%rd2+32], {%rd41, %rd42};
	st.local.v2.u64 	[%rd2+48], {%rd45, %rd46};
	mov.u32 	%r27, %ctaid.x;
	cvt.u64.u32 	%rd3, %r27;
	cvta.to.global.u64 	%rd49, %rd19;
	mul.wide.u32 	%rd50, %r27, 4;
	add.s64 	%rd51, %rd49, %rd50;
	add.s64 	%rd4, %rd51, 2304;
	ld.global.u32 	%r28, [%rd51+2304];
	add.s32 	%r1, %r28, 1;
	mov.u32 	%r2, %tid.x;
	setp.ge.u32 	%p1, %r2, %r25;
	mul.wide.u32 	%rd52, %r2, 8;
	add.s64 	%rd6, %rd23, %rd52;
	@%p1 bra 	$L__BB2_3;

	ld.local.u64 	%rd54, [%rd6];
	shl.b64 	%rd55, %rd3, 5;
	add.s64 	%rd56, %rd54, %rd55;
	mul.wide.s32 	%rd57, %r24, 4;
	add.s64 	%rd53, %rd56, %rd57;
	add.s64 	%rd58, %rd19, %rd55;
	mul.wide.u32 	%rd59, %r2, 4;
	add.s64 	%rd7, %rd58, %rd59;
	// begin inline asm
	st.volatile.global.u32 [%rd53], %r1;
	// end inline asm

$L__BB2_2:
	// begin inline asm
	ld.volatile.global.u32 %r30, [%rd7];
	// end inline asm
	setp.ne.s32 	%p2, %r30, %r1;
	@%p2 bra 	$L__BB2_2;

$L__BB2_3:
	bar.sync 	0;
	setp.ne.s32 	%p3, %r2, 0;
	@%p3 bra 	$L__BB2_5;

	add.s32 	%r60, %r28, 1;
	st.global.u32 	[%rd4], %r60;

$L__BB2_5:
	cvt.u32.u64 	%r31, %rd3;
	mov.u32 	%r32, %ntid.x;
	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r3, %r33, %r32;
	mad.lo.s32 	%r64, %r31, %r32, %r2;
	setp.ge.s32 	%p4, %r64, %r26;
	@%p4 bra 	$L__BB2_22;

	setp.gt.s32 	%p5, %r25, 1;
	@%p5 bra 	$L__BB2_13;
	bra.uni 	$L__BB2_7;

$L__BB2_13:
	add.s32 	%r42, %r25, -1;
	and.b32  	%r14, %r42, 3;
	add.s32 	%r44, %r25, -2;

$L__BB2_14:
	setp.lt.u32 	%p10, %r44, 3;
	cvt.s64.s32 	%rd16, %r64;
	mul.wide.s32 	%rd74, %r64, 16;
	add.s64 	%rd75, %rd34, %rd74;
	ld.f32 	%f132, [%rd75];
	ld.f32 	%f131, [%rd75+4];
	ld.f32 	%f130, [%rd75+8];
	ld.f32 	%f129, [%rd75+12];
	mov.u32 	%r69, 1;
	@%p10 bra 	$L__BB2_17;

	sub.s32 	%r68, %r42, %r14;

$L__BB2_16:
	mul.wide.s32 	%rd76, %r69, 8;
	add.s64 	%rd77, %rd2, %rd76;
	ld.local.u64 	%rd78, [%rd77];
	shl.b64 	%rd79, %rd16, 4;
	add.s64 	%rd80, %rd78, %rd79;
	ld.v4.f32 	{%f53, %f54, %f55, %f56}, [%rd80];
	add.ftz.f32 	%f61, %f53, %f132;
	add.ftz.f32 	%f62, %f54, %f131;
	add.ftz.f32 	%f63, %f55, %f130;
	add.ftz.f32 	%f64, %f56, %f129;
	ld.local.u64 	%rd81, [%rd77+8];
	add.s64 	%rd82, %rd81, %rd79;
	ld.v4.f32 	{%f65, %f66, %f67, %f68}, [%rd82];
	add.ftz.f32 	%f73, %f65, %f61;
	add.ftz.f32 	%f74, %f66, %f62;
	add.ftz.f32 	%f75, %f67, %f63;
	add.ftz.f32 	%f76, %f68, %f64;
	ld.local.u64 	%rd83, [%rd77+16];
	add.s64 	%rd84, %rd83, %rd79;
	ld.v4.f32 	{%f77, %f78, %f79, %f80}, [%rd84];
	add.ftz.f32 	%f85, %f77, %f73;
	add.ftz.f32 	%f86, %f78, %f74;
	add.ftz.f32 	%f87, %f79, %f75;
	add.ftz.f32 	%f88, %f80, %f76;
	ld.local.u64 	%rd85, [%rd77+24];
	add.s64 	%rd86, %rd85, %rd79;
	ld.v4.f32 	{%f89, %f90, %f91, %f92}, [%rd86];
	add.ftz.f32 	%f132, %f89, %f85;
	add.ftz.f32 	%f131, %f90, %f86;
	add.ftz.f32 	%f130, %f91, %f87;
	add.ftz.f32 	%f129, %f92, %f88;
	add.s32 	%r69, %r69, 4;
	add.s32 	%r68, %r68, -4;
	setp.ne.s32 	%p11, %r68, 0;
	@%p11 bra 	$L__BB2_16;

$L__BB2_17:
	setp.eq.s32 	%p12, %r14, 0;
	@%p12 bra 	$L__BB2_21;

	setp.eq.s32 	%p13, %r14, 1;
	mul.wide.s32 	%rd87, %r69, 8;
	add.s64 	%rd17, %rd2, %rd87;
	ld.local.u64 	%rd88, [%rd17];
	shl.b64 	%rd89, %rd16, 4;
	add.s64 	%rd90, %rd88, %rd89;
	ld.v4.f32 	{%f97, %f98, %f99, %f100}, [%rd90];
	add.ftz.f32 	%f132, %f97, %f132;
	add.ftz.f32 	%f131, %f98, %f131;
	add.ftz.f32 	%f130, %f99, %f130;
	add.ftz.f32 	%f129, %f100, %f129;
	@%p13 bra 	$L__BB2_21;

	setp.eq.s32 	%p14, %r14, 2;
	ld.local.u64 	%rd91, [%rd17+8];
	add.s64 	%rd93, %rd91, %rd89;
	ld.v4.f32 	{%f105, %f106, %f107, %f108}, [%rd93];
	add.ftz.f32 	%f132, %f105, %f132;
	add.ftz.f32 	%f131, %f106, %f131;
	add.ftz.f32 	%f130, %f107, %f130;
	add.ftz.f32 	%f129, %f108, %f129;
	@%p14 bra 	$L__BB2_21;

	ld.local.u64 	%rd94, [%rd17+16];
	add.s64 	%rd96, %rd94, %rd89;
	ld.v4.f32 	{%f113, %f114, %f115, %f116}, [%rd96];
	add.ftz.f32 	%f132, %f113, %f132;
	add.ftz.f32 	%f131, %f114, %f131;
	add.ftz.f32 	%f130, %f115, %f130;
	add.ftz.f32 	%f129, %f116, %f129;

$L__BB2_21:
	shl.b64 	%rd98, %rd16, 4;
	add.s64 	%rd99, %rd1, %rd98;
	st.global.v4.f32 	[%rd99], {%f132, %f131, %f130, %f129};
	cvt.u32.u64 	%r50, %rd16;
	mad.lo.s32 	%r64, %r33, %r32, %r50;
	setp.lt.s32 	%p15, %r64, %r26;
	@%p15 bra 	$L__BB2_14;
	bra.uni 	$L__BB2_22;

$L__BB2_7:
	add.s32 	%r34, %r3, %r26;
	add.s32 	%r35, %r64, %r3;
	not.b32 	%r36, %r35;
	add.s32 	%r37, %r34, %r36;
	div.u32 	%r5, %r37, %r3;
	add.s32 	%r38, %r5, 1;
	and.b32  	%r63, %r38, 3;
	setp.eq.s32 	%p6, %r63, 0;
	@%p6 bra 	$L__BB2_10;

	mul.wide.s32 	%rd61, %r64, 4;
	shl.b64 	%rd62, %rd61, 2;
	add.s64 	%rd117, %rd1, %rd62;
	mul.wide.s32 	%rd9, %r3, 16;
	mul.wide.s32 	%rd63, %r64, 16;
	add.s64 	%rd64, %rd34, %rd63;
	add.s64 	%rd116, %rd64, 8;

$L__BB2_9:
	.pragma "nounroll";
	ld.f32 	%f33, [%rd116+4];
	ld.f32 	%f34, [%rd116];
	ld.f32 	%f35, [%rd116+-4];
	ld.f32 	%f36, [%rd116+-8];
	st.global.v4.f32 	[%rd117], {%f36, %f35, %f34, %f33};
	add.s32 	%r64, %r64, %r3;
	add.s64 	%rd117, %rd117, %rd9;
	add.s64 	%rd116, %rd116, %rd9;
	add.s32 	%r63, %r63, -1;
	setp.ne.s32 	%p7, %r63, 0;
	@%p7 bra 	$L__BB2_9;

$L__BB2_10:
	setp.lt.u32 	%p8, %r5, 3;
	@%p8 bra 	$L__BB2_22;

	mul.wide.s32 	%rd15, %r3, 16;

$L__BB2_12:
	mul.wide.s32 	%rd65, %r64, 16;
	add.s64 	%rd66, %rd34, %rd65;
	add.s64 	%rd67, %rd1, %rd65;
	ld.f32 	%f37, [%rd66+12];
	ld.f32 	%f38, [%rd66+8];
	ld.f32 	%f39, [%rd66+4];
	ld.f32 	%f40, [%rd66];
	st.global.v4.f32 	[%rd67], {%f40, %f39, %f38, %f37};
	add.s64 	%rd68, %rd66, %rd15;
	ld.f32 	%f41, [%rd68+12];
	ld.f32 	%f42, [%rd68+8];
	ld.f32 	%f43, [%rd68+4];
	ld.f32 	%f44, [%rd68];
	add.s64 	%rd69, %rd67, %rd15;
	st.global.v4.f32 	[%rd69], {%f44, %f43, %f42, %f41};
	add.s32 	%r39, %r64, %r3;
	add.s32 	%r40, %r39, %r3;
	add.s64 	%rd70, %rd68, %rd15;
	ld.f32 	%f45, [%rd70+12];
	ld.f32 	%f46, [%rd70+8];
	ld.f32 	%f47, [%rd70+4];
	ld.f32 	%f48, [%rd70];
	add.s64 	%rd71, %rd69, %rd15;
	st.global.v4.f32 	[%rd71], {%f48, %f47, %f46, %f45};
	add.s32 	%r41, %r40, %r3;
	add.s64 	%rd72, %rd70, %rd15;
	ld.f32 	%f49, [%rd72+12];
	ld.f32 	%f50, [%rd72+8];
	ld.f32 	%f51, [%rd72+4];
	ld.f32 	%f52, [%rd72];
	add.s64 	%rd73, %rd71, %rd15;
	st.global.v4.f32 	[%rd73], {%f52, %f51, %f50, %f49};
	add.s32 	%r64, %r41, %r3;
	setp.lt.s32 	%p9, %r64, %r26;
	@%p9 bra 	$L__BB2_12;

$L__BB2_22:
	bar.sync 	0;
	ld.global.u32 	%r53, [%rd51+2304];
	add.s32 	%r23, %r53, 1;
	@%p1 bra 	$L__BB2_25;

	ld.local.u64 	%rd104, [%rd6];
	mul.wide.u32 	%rd105, %r27, 32;
	add.s64 	%rd106, %rd104, %rd105;
	mul.wide.s32 	%rd107, %r24, 4;
	add.s64 	%rd108, %rd106, %rd107;
	add.s64 	%rd103, %rd108, 1152;
	add.s64 	%rd109, %rd19, %rd105;
	mul.wide.u32 	%rd110, %r2, 4;
	add.s64 	%rd111, %rd109, %rd110;
	add.s64 	%rd18, %rd111, 1152;
	// begin inline asm
	st.volatile.global.u32 [%rd103], %r23;
	// end inline asm

$L__BB2_24:
	// begin inline asm
	ld.volatile.global.u32 %r57, [%rd18];
	// end inline asm
	setp.ne.s32 	%p17, %r57, %r23;
	@%p17 bra 	$L__BB2_24;

$L__BB2_25:
	@%p3 bra 	$L__BB2_27;

	add.s32 	%r61, %r53, 1;
	st.global.u32 	[%rd51+2304], %r61;

$L__BB2_27:
	ret;

}
	// .globl	custom_allreduce_2stage_f32
.visible .entry custom_allreduce_2stage_f32(
	.param .u64 custom_allreduce_2stage_f32_param_0,
	.param .align 16 .b8 custom_allreduce_2stage_f32_param_1[64],
	.param .u64 custom_allreduce_2stage_f32_param_2,
	.param .u64 custom_allreduce_2stage_f32_param_3,
	.param .u32 custom_allreduce_2stage_f32_param_4,
	.param .u32 custom_allreduce_2stage_f32_param_5,
	.param .u32 custom_allreduce_2stage_f32_param_6
)
.maxntid 512, 1, 1
.minnctapersm 1
{
	.local .align 16 .b8 	__local_depot3[192];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<57>;
	.reg .f32 	%f<189>;
	.reg .b32 	%r<189>;
	.reg .b64 	%rd<206>;


	mov.u64 	%SPL, __local_depot3;
	ld.param.u64 	%rd35, [custom_allreduce_2stage_f32_param_0];
	ld.param.u64 	%rd34, [custom_allreduce_2stage_f32_param_2];
	ld.param.u64 	%rd36, [custom_allreduce_2stage_f32_param_3];
	ld.param.u32 	%r60, [custom_allreduce_2stage_f32_param_4];
	ld.param.u32 	%r61, [custom_allreduce_2stage_f32_param_5];
	ld.param.u32 	%r62, [custom_allreduce_2stage_f32_param_6];
	cvta.to.global.u64 	%rd1, %rd35;
	cvta.to.global.u64 	%rd2, %rd36;
	add.u64 	%rd3, %SPL, 0;
	add.u64 	%rd4, %SPL, 64;
	add.u64 	%rd5, %SPL, 128;
	ld.param.u64 	%rd40, [custom_allreduce_2stage_f32_param_1+8];
	ld.param.u64 	%rd41, [custom_allreduce_2stage_f32_param_1];
	st.local.v2.u64 	[%rd3], {%rd41, %rd40};
	ld.param.u64 	%rd42, [custom_allreduce_2stage_f32_param_1+24];
	ld.param.u64 	%rd43, [custom_allreduce_2stage_f32_param_1+16];
	st.local.v2.u64 	[%rd3+16], {%rd43, %rd42};
	ld.param.u64 	%rd44, [custom_allreduce_2stage_f32_param_1+40];
	ld.param.u64 	%rd45, [custom_allreduce_2stage_f32_param_1+32];
	st.local.v2.u64 	[%rd3+32], {%rd45, %rd44};
	ld.param.u64 	%rd46, [custom_allreduce_2stage_f32_param_1+56];
	ld.param.u64 	%rd47, [custom_allreduce_2stage_f32_param_1+48];
	st.local.v2.u64 	[%rd3+48], {%rd47, %rd46};
	mov.u32 	%r63, %ntid.x;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %tid.x;
	mad.lo.s32 	%r185, %r1, %r63, %r2;
	mov.u32 	%r64, %nctaid.x;
	mul.lo.s32 	%r4, %r64, %r63;
	div.s32 	%r5, %r62, %r61;
	mul.lo.s32 	%r6, %r5, %r60;
	add.s32 	%r7, %r61, -1;
	setp.eq.s32 	%p1, %r7, %r60;
	add.s32 	%r65, %r6, %r5;
	selp.b32 	%r8, %r62, %r65, %p1;
	mul.lo.s32 	%r66, %r5, %r61;
	sub.s32 	%r67, %r62, %r66;
	add.s32 	%r9, %r67, %r5;
	setp.lt.s32 	%p2, %r61, 1;
	@%p2 bra 	$L__BB3_7;

	and.b32  	%r176, %r61, 3;
	setp.lt.u32 	%p3, %r7, 3;
	mov.u32 	%r174, 0;
	@%p3 bra 	$L__BB3_4;

	sub.s32 	%r173, %r61, %r176;

$L__BB3_3:
	add.s32 	%r70, %r174, %r60;
	rem.s32 	%r71, %r70, %r61;
	mul.wide.s32 	%rd48, %r71, 8;
	add.s64 	%rd49, %rd1, %rd48;
	mul.wide.s32 	%rd50, %r174, 8;
	add.s64 	%rd51, %rd4, %rd50;
	add.s64 	%rd52, %rd3, %rd48;
	ld.local.u64 	%rd53, [%rd52];
	add.s64 	%rd54, %rd5, %rd50;
	add.s32 	%r72, %r70, 1;
	rem.s32 	%r73, %r72, %r61;
	mul.wide.s32 	%rd55, %r73, 8;
	add.s64 	%rd56, %rd1, %rd55;
	ld.global.nc.u64 	%rd57, [%rd49];
	ld.global.nc.u64 	%rd58, [%rd56];
	st.local.v2.u64 	[%rd51], {%rd57, %rd58};
	add.s64 	%rd59, %rd3, %rd55;
	ld.local.u64 	%rd60, [%rd59];
	add.s64 	%rd61, %rd53, 2560;
	add.s64 	%rd62, %rd60, 2560;
	st.local.v2.u64 	[%rd54], {%rd61, %rd62};
	add.s32 	%r74, %r70, 2;
	rem.s32 	%r75, %r74, %r61;
	mul.wide.s32 	%rd63, %r75, 8;
	add.s64 	%rd64, %rd1, %rd63;
	add.s64 	%rd65, %rd3, %rd63;
	ld.local.u64 	%rd66, [%rd65];
	add.s32 	%r76, %r70, 3;
	rem.s32 	%r77, %r76, %r61;
	mul.wide.s32 	%rd67, %r77, 8;
	add.s64 	%rd68, %rd1, %rd67;
	ld.global.nc.u64 	%rd69, [%rd68];
	ld.global.nc.u64 	%rd70, [%rd64];
	st.local.v2.u64 	[%rd51+16], {%rd70, %rd69};
	add.s64 	%rd71, %rd3, %rd67;
	ld.local.u64 	%rd72, [%rd71];
	add.s64 	%rd73, %rd66, 2560;
	add.s64 	%rd74, %rd72, 2560;
	st.local.v2.u64 	[%rd54+16], {%rd73, %rd74};
	add.s32 	%r174, %r174, 4;
	add.s32 	%r173, %r173, -4;
	setp.ne.s32 	%p4, %r173, 0;
	@%p4 bra 	$L__BB3_3;

$L__BB3_4:
	setp.eq.s32 	%p5, %r176, 0;
	@%p5 bra 	$L__BB3_7;

	mul.wide.s32 	%rd75, %r174, 8;
	add.s64 	%rd203, %rd5, %rd75;
	add.s64 	%rd202, %rd4, %rd75;
	add.s32 	%r175, %r174, %r60;

$L__BB3_6:
	.pragma "nounroll";
	rem.s32 	%r78, %r175, %r61;
	mul.wide.s32 	%rd76, %r78, 8;
	add.s64 	%rd77, %rd1, %rd76;
	ld.global.nc.u64 	%rd78, [%rd77];
	st.local.u64 	[%rd202], %rd78;
	add.s64 	%rd79, %rd3, %rd76;
	ld.local.u64 	%rd80, [%rd79];
	add.s64 	%rd81, %rd80, 2560;
	st.local.u64 	[%rd203], %rd81;
	add.s64 	%rd203, %rd203, 8;
	add.s64 	%rd202, %rd202, 8;
	add.s32 	%r175, %r175, 1;
	add.s32 	%r176, %r176, -1;
	setp.ne.s32 	%p6, %r176, 0;
	@%p6 bra 	$L__BB3_6;

$L__BB3_7:
	ld.local.u64 	%rd12, [%rd5];
	cvt.u64.u32 	%rd13, %r1;
	cvta.to.global.u64 	%rd82, %rd34;
	mul.wide.u32 	%rd83, %r1, 4;
	add.s64 	%rd84, %rd82, %rd83;
	add.s64 	%rd14, %rd84, 2304;
	ld.global.u32 	%r79, [%rd84+2304];
	add.s32 	%r22, %r79, 1;
	mul.wide.u32 	%rd85, %r2, 8;
	add.s64 	%rd15, %rd3, %rd85;
	setp.ge.u32 	%p7, %r2, %r61;
	@%p7 bra 	$L__BB3_10;

	ld.local.u64 	%rd87, [%rd15];
	shl.b64 	%rd88, %rd13, 5;
	add.s64 	%rd89, %rd87, %rd88;
	mul.wide.s32 	%rd90, %r60, 4;
	add.s64 	%rd86, %rd89, %rd90;
	add.s64 	%rd91, %rd34, %rd88;
	mul.wide.u32 	%rd92, %r2, 4;
	add.s64 	%rd16, %rd91, %rd92;
	// begin inline asm
	st.volatile.global.u32 [%rd86], %r22;
	// end inline asm

$L__BB3_9:
	// begin inline asm
	ld.volatile.global.u32 %r81, [%rd16];
	// end inline asm
	setp.ne.s32 	%p8, %r81, %r22;
	@%p8 bra 	$L__BB3_9;

$L__BB3_10:
	bar.sync 	0;
	setp.ne.s32 	%p9, %r2, 0;
	@%p9 bra 	$L__BB3_12;

	add.s32 	%r169, %r79, 1;
	st.global.u32 	[%rd14], %r169;

$L__BB3_12:
	add.s32 	%r179, %r6, %r185;
	setp.ge.s32 	%p10, %r179, %r8;
	@%p10 bra 	$L__BB3_29;

	ld.local.u64 	%rd17, [%rd4];
	setp.gt.s32 	%p11, %r61, 1;
	@%p11 bra 	$L__BB3_20;
	bra.uni 	$L__BB3_14;

$L__BB3_20:
	add.s32 	%r95, %r61, -2;

$L__BB3_21:
	setp.lt.u32 	%p16, %r95, 3;
	cvt.s64.s32 	%rd27, %r179;
	mul.wide.s32 	%rd108, %r179, 16;
	add.s64 	%rd109, %rd17, %rd108;
	ld.f32 	%f188, [%rd109];
	ld.f32 	%f187, [%rd109+4];
	ld.f32 	%f186, [%rd109+8];
	ld.f32 	%f185, [%rd109+12];
	mov.u32 	%r184, 1;
	@%p16 bra 	$L__BB3_24;

	and.b32  	%r98, %r7, 3;
	sub.s32 	%r183, %r7, %r98;

$L__BB3_23:
	mul.wide.s32 	%rd110, %r184, 8;
	add.s64 	%rd111, %rd4, %rd110;
	ld.local.u64 	%rd112, [%rd111];
	shl.b64 	%rd113, %rd27, 4;
	add.s64 	%rd114, %rd112, %rd113;
	ld.v4.f32 	{%f53, %f54, %f55, %f56}, [%rd114];
	add.ftz.f32 	%f61, %f53, %f188;
	add.ftz.f32 	%f62, %f54, %f187;
	add.ftz.f32 	%f63, %f55, %f186;
	add.ftz.f32 	%f64, %f56, %f185;
	ld.local.u64 	%rd115, [%rd111+8];
	add.s64 	%rd116, %rd115, %rd113;
	ld.v4.f32 	{%f65, %f66, %f67, %f68}, [%rd116];
	add.ftz.f32 	%f73, %f65, %f61;
	add.ftz.f32 	%f74, %f66, %f62;
	add.ftz.f32 	%f75, %f67, %f63;
	add.ftz.f32 	%f76, %f68, %f64;
	ld.local.u64 	%rd117, [%rd111+16];
	add.s64 	%rd118, %rd117, %rd113;
	ld.v4.f32 	{%f77, %f78, %f79, %f80}, [%rd118];
	add.ftz.f32 	%f85, %f77, %f73;
	add.ftz.f32 	%f86, %f78, %f74;
	add.ftz.f32 	%f87, %f79, %f75;
	add.ftz.f32 	%f88, %f80, %f76;
	ld.local.u64 	%rd119, [%rd111+24];
	add.s64 	%rd120, %rd119, %rd113;
	ld.v4.f32 	{%f89, %f90, %f91, %f92}, [%rd120];
	add.ftz.f32 	%f188, %f89, %f85;
	add.ftz.f32 	%f187, %f90, %f86;
	add.ftz.f32 	%f186, %f91, %f87;
	add.ftz.f32 	%f185, %f92, %f88;
	add.s32 	%r184, %r184, 4;
	add.s32 	%r183, %r183, -4;
	setp.ne.s32 	%p17, %r183, 0;
	@%p17 bra 	$L__BB3_23;

$L__BB3_24:
	and.b32  	%r100, %r7, 3;
	setp.eq.s32 	%p18, %r100, 0;
	@%p18 bra 	$L__BB3_28;

	setp.eq.s32 	%p19, %r100, 1;
	mul.wide.s32 	%rd121, %r184, 8;
	add.s64 	%rd28, %rd4, %rd121;
	ld.local.u64 	%rd122, [%rd28];
	shl.b64 	%rd123, %rd27, 4;
	add.s64 	%rd124, %rd122, %rd123;
	ld.v4.f32 	{%f97, %f98, %f99, %f100}, [%rd124];
	add.ftz.f32 	%f188, %f97, %f188;
	add.ftz.f32 	%f187, %f98, %f187;
	add.ftz.f32 	%f186, %f99, %f186;
	add.ftz.f32 	%f185, %f100, %f185;
	@%p19 bra 	$L__BB3_28;

	setp.eq.s32 	%p20, %r100, 2;
	ld.local.u64 	%rd125, [%rd28+8];
	add.s64 	%rd127, %rd125, %rd123;
	ld.v4.f32 	{%f105, %f106, %f107, %f108}, [%rd127];
	add.ftz.f32 	%f188, %f105, %f188;
	add.ftz.f32 	%f187, %f106, %f187;
	add.ftz.f32 	%f186, %f107, %f186;
	add.ftz.f32 	%f185, %f108, %f185;
	@%p20 bra 	$L__BB3_28;

	ld.local.u64 	%rd128, [%rd28+16];
	add.s64 	%rd130, %rd128, %rd123;
	ld.v4.f32 	{%f113, %f114, %f115, %f116}, [%rd130];
	add.ftz.f32 	%f188, %f113, %f188;
	add.ftz.f32 	%f187, %f114, %f187;
	add.ftz.f32 	%f186, %f115, %f186;
	add.ftz.f32 	%f185, %f116, %f185;

$L__BB3_28:
	cvt.u32.u64 	%r106, %rd27;
	sub.s32 	%r107, %r106, %r6;
	mul.wide.s32 	%rd131, %r107, 16;
	add.s64 	%rd132, %rd12, %rd131;
	st.v4.f32 	[%rd132], {%f188, %f187, %f186, %f185};
	add.s32 	%r179, %r106, %r4;
	setp.lt.s32 	%p21, %r179, %r8;
	@%p21 bra 	$L__BB3_21;
	bra.uni 	$L__BB3_29;

$L__BB3_14:
	add.s32 	%r83, %r8, %r4;
	add.s32 	%r84, %r179, %r4;
	not.b32 	%r85, %r84;
	add.s32 	%r86, %r83, %r85;
	div.u32 	%r24, %r86, %r4;
	add.s32 	%r87, %r24, 1;
	and.b32  	%r178, %r87, 3;
	setp.eq.s32 	%p12, %r178, 0;
	@%p12 bra 	$L__BB3_17;

	neg.s32 	%r88, %r185;
	mul.wide.s32 	%rd94, %r88, 16;
	sub.s64 	%rd205, %rd12, %rd94;
	neg.s32 	%r89, %r4;
	mul.wide.s32 	%rd95, %r89, 16;
	neg.s64 	%rd19, %rd95;
	mul.wide.s32 	%rd96, %r179, 16;
	add.s64 	%rd97, %rd17, %rd96;
	add.s64 	%rd204, %rd97, 8;
	mul.wide.s32 	%rd21, %r4, 16;

$L__BB3_16:
	.pragma "nounroll";
	ld.f32 	%f33, [%rd204+4];
	ld.f32 	%f34, [%rd204];
	ld.f32 	%f35, [%rd204+-4];
	ld.f32 	%f36, [%rd204+-8];
	st.v4.f32 	[%rd205], {%f36, %f35, %f34, %f33};
	add.s32 	%r179, %r179, %r4;
	add.s64 	%rd205, %rd205, %rd19;
	add.s64 	%rd204, %rd204, %rd21;
	add.s32 	%r178, %r178, -1;
	setp.ne.s32 	%p13, %r178, 0;
	@%p13 bra 	$L__BB3_16;

$L__BB3_17:
	setp.lt.u32 	%p14, %r24, 3;
	@%p14 bra 	$L__BB3_29;

	mul.wide.s32 	%rd26, %r4, 16;

$L__BB3_19:
	mul.wide.s32 	%rd98, %r179, 16;
	add.s64 	%rd99, %rd17, %rd98;
	sub.s32 	%r90, %r179, %r6;
	mul.wide.s32 	%rd100, %r90, 16;
	add.s64 	%rd101, %rd12, %rd100;
	ld.f32 	%f37, [%rd99+12];
	ld.f32 	%f38, [%rd99+8];
	ld.f32 	%f39, [%rd99+4];
	ld.f32 	%f40, [%rd99];
	st.v4.f32 	[%rd101], {%f40, %f39, %f38, %f37};
	add.s64 	%rd102, %rd99, %rd26;
	ld.f32 	%f41, [%rd102+12];
	ld.f32 	%f42, [%rd102+8];
	ld.f32 	%f43, [%rd102+4];
	ld.f32 	%f44, [%rd102];
	add.s64 	%rd103, %rd101, %rd26;
	st.v4.f32 	[%rd103], {%f44, %f43, %f42, %f41};
	add.s32 	%r91, %r179, %r4;
	add.s32 	%r92, %r91, %r4;
	add.s64 	%rd104, %rd102, %rd26;
	ld.f32 	%f45, [%rd104+12];
	ld.f32 	%f46, [%rd104+8];
	ld.f32 	%f47, [%rd104+4];
	ld.f32 	%f48, [%rd104];
	add.s64 	%rd105, %rd103, %rd26;
	st.v4.f32 	[%rd105], {%f48, %f47, %f46, %f45};
	add.s32 	%r93, %r92, %r4;
	add.s64 	%rd106, %rd104, %rd26;
	ld.f32 	%f49, [%rd106+12];
	ld.f32 	%f50, [%rd106+8];
	ld.f32 	%f51, [%rd106+4];
	ld.f32 	%f52, [%rd106];
	add.s64 	%rd107, %rd105, %rd26;
	st.v4.f32 	[%rd107], {%f52, %f51, %f50, %f49};
	add.s32 	%r179, %r93, %r4;
	setp.lt.s32 	%p15, %r179, %r8;
	@%p15 bra 	$L__BB3_19;

$L__BB3_29:
	bar.sync 	0;
	ld.global.u32 	%r110, [%rd84+2304];
	add.s32 	%r41, %r110, 1;
	@%p7 bra 	$L__BB3_32;

	ld.local.u64 	%rd137, [%rd15];
	mul.wide.u32 	%rd138, %r1, 32;
	add.s64 	%rd139, %rd137, %rd138;
	mul.wide.s32 	%rd140, %r60, 4;
	add.s64 	%rd141, %rd139, %rd140;
	add.s64 	%rd136, %rd141, 1152;
	add.s64 	%rd142, %rd34, %rd138;
	mul.wide.u32 	%rd143, %r2, 4;
	add.s64 	%rd144, %rd142, %rd143;
	add.s64 	%rd29, %rd144, 1152;
	// begin inline asm
	st.release.sys.global.u32 [%rd136], %r41;
	// end inline asm

$L__BB3_31:
	// begin inline asm
	ld.acquire.sys.global.u32 %r114, [%rd29];
	// end inline asm
	setp.ne.s32 	%p23, %r114, %r41;
	@%p23 bra 	$L__BB3_31;

$L__BB3_32:
	bar.sync 	0;
	@%p9 bra 	$L__BB3_34;

	add.s32 	%r170, %r110, 1;
	st.global.u32 	[%rd84+2304], %r170;

$L__BB3_34:
	setp.ge.s32 	%p25, %r185, %r9;
	@%p25 bra 	$L__BB3_58;

	@%p2 bra 	$L__BB3_58;

	and.b32  	%r42, %r61, 3;
	sub.s32 	%r43, %r61, %r42;

$L__BB3_37:
	cvt.s64.s32 	%rd30, %r185;
	setp.lt.u32 	%p27, %r7, 3;
	mov.u32 	%r188, 0;
	@%p27 bra 	$L__BB3_48;

	mov.u32 	%r187, %r43;

$L__BB3_39:
	cvt.u32.u64 	%r126, %rd30;
	setp.ge.s32 	%p28, %r126, %r5;
	add.s32 	%r127, %r188, %r60;
	rem.s32 	%r48, %r127, %r61;
	setp.ne.s32 	%p29, %r48, %r7;
	mul.wide.s32 	%rd149, %r188, 8;
	add.s64 	%rd31, %rd5, %rd149;
	and.pred  	%p30, %p28, %p29;
	@%p30 bra 	$L__BB3_41;

	mad.lo.s32 	%r129, %r48, %r5, %r126;
	ld.local.u64 	%rd150, [%rd31];
	shl.b64 	%rd151, %rd30, 4;
	add.s64 	%rd152, %rd150, %rd151;
	ld.v4.f32 	{%f121, %f122, %f123, %f124}, [%rd152];
	mul.wide.s32 	%rd153, %r129, 16;
	add.s64 	%rd154, %rd2, %rd153;
	st.global.v4.f32 	[%rd154], {%f121, %f122, %f123, %f124};

$L__BB3_41:
	add.s32 	%r132, %r127, 1;
	rem.s32 	%r49, %r132, %r61;
	setp.ne.s32 	%p32, %r49, %r7;
	and.pred  	%p33, %p28, %p32;
	@%p33 bra 	$L__BB3_43;

	mad.lo.s32 	%r134, %r49, %r5, %r126;
	ld.local.u64 	%rd155, [%rd31+8];
	shl.b64 	%rd156, %rd30, 4;
	add.s64 	%rd157, %rd155, %rd156;
	ld.v4.f32 	{%f129, %f130, %f131, %f132}, [%rd157];
	mul.wide.s32 	%rd158, %r134, 16;
	add.s64 	%rd159, %rd2, %rd158;
	st.global.v4.f32 	[%rd159], {%f129, %f130, %f131, %f132};

$L__BB3_43:
	add.s32 	%r137, %r127, 2;
	rem.s32 	%r50, %r137, %r61;
	setp.ne.s32 	%p35, %r50, %r7;
	and.pred  	%p36, %p28, %p35;
	@%p36 bra 	$L__BB3_45;

	mad.lo.s32 	%r139, %r50, %r5, %r126;
	ld.local.u64 	%rd160, [%rd31+16];
	shl.b64 	%rd161, %rd30, 4;
	add.s64 	%rd162, %rd160, %rd161;
	ld.v4.f32 	{%f137, %f138, %f139, %f140}, [%rd162];
	mul.wide.s32 	%rd163, %r139, 16;
	add.s64 	%rd164, %rd2, %rd163;
	st.global.v4.f32 	[%rd164], {%f137, %f138, %f139, %f140};

$L__BB3_45:
	add.s32 	%r142, %r127, 3;
	rem.s32 	%r51, %r142, %r61;
	setp.ne.s32 	%p38, %r51, %r7;
	and.pred  	%p39, %p28, %p38;
	@%p39 bra 	$L__BB3_47;

	mad.lo.s32 	%r144, %r51, %r5, %r126;
	ld.local.u64 	%rd165, [%rd31+24];
	shl.b64 	%rd166, %rd30, 4;
	add.s64 	%rd167, %rd165, %rd166;
	ld.v4.f32 	{%f145, %f146, %f147, %f148}, [%rd167];
	mul.wide.s32 	%rd168, %r144, 16;
	add.s64 	%rd169, %rd2, %rd168;
	st.global.v4.f32 	[%rd169], {%f145, %f146, %f147, %f148};

$L__BB3_47:
	add.s32 	%r188, %r188, 4;
	add.s32 	%r187, %r187, -4;
	setp.ne.s32 	%p40, %r187, 0;
	@%p40 bra 	$L__BB3_39;

$L__BB3_48:
	setp.eq.s32 	%p41, %r42, 0;
	@%p41 bra 	$L__BB3_57;

	cvt.u32.u64 	%r145, %rd30;
	setp.ge.s32 	%p42, %r145, %r5;
	add.s32 	%r146, %r188, %r60;
	rem.s32 	%r55, %r146, %r61;
	setp.ne.s32 	%p43, %r55, %r7;
	mul.wide.s32 	%rd170, %r188, 8;
	add.s64 	%rd32, %rd5, %rd170;
	and.pred  	%p44, %p42, %p43;
	@%p44 bra 	$L__BB3_51;

	mad.lo.s32 	%r148, %r55, %r5, %r145;
	ld.local.u64 	%rd171, [%rd32];
	shl.b64 	%rd172, %rd30, 4;
	add.s64 	%rd173, %rd171, %rd172;
	ld.v4.f32 	{%f153, %f154, %f155, %f156}, [%rd173];
	mul.wide.s32 	%rd174, %r148, 16;
	add.s64 	%rd175, %rd2, %rd174;
	st.global.v4.f32 	[%rd175], {%f153, %f154, %f155, %f156};

$L__BB3_51:
	setp.eq.s32 	%p45, %r42, 1;
	@%p45 bra 	$L__BB3_57;

	add.s32 	%r151, %r146, 1;
	rem.s32 	%r56, %r151, %r61;
	setp.ne.s32 	%p47, %r56, %r7;
	and.pred  	%p48, %p42, %p47;
	@%p48 bra 	$L__BB3_54;

	mad.lo.s32 	%r153, %r56, %r5, %r145;
	ld.local.u64 	%rd176, [%rd32+8];
	shl.b64 	%rd177, %rd30, 4;
	add.s64 	%rd178, %rd176, %rd177;
	ld.v4.f32 	{%f161, %f162, %f163, %f164}, [%rd178];
	mul.wide.s32 	%rd179, %r153, 16;
	add.s64 	%rd180, %rd2, %rd179;
	st.global.v4.f32 	[%rd180], {%f161, %f162, %f163, %f164};

$L__BB3_54:
	setp.eq.s32 	%p49, %r42, 2;
	@%p49 bra 	$L__BB3_57;

	add.s32 	%r156, %r146, 2;
	rem.s32 	%r57, %r156, %r61;
	setp.ne.s32 	%p51, %r57, %r7;
	and.pred  	%p52, %p42, %p51;
	@%p52 bra 	$L__BB3_57;

	mad.lo.s32 	%r158, %r57, %r5, %r145;
	ld.local.u64 	%rd181, [%rd32+16];
	shl.b64 	%rd182, %rd30, 4;
	add.s64 	%rd183, %rd181, %rd182;
	ld.v4.f32 	{%f169, %f170, %f171, %f172}, [%rd183];
	mul.wide.s32 	%rd184, %r158, 16;
	add.s64 	%rd185, %rd2, %rd184;
	st.global.v4.f32 	[%rd185], {%f169, %f170, %f171, %f172};

$L__BB3_57:
	cvt.u32.u64 	%r159, %rd30;
	add.s32 	%r185, %r159, %r4;
	setp.lt.s32 	%p53, %r185, %r9;
	@%p53 bra 	$L__BB3_37;

$L__BB3_58:
	bar.sync 	0;
	ld.global.u32 	%r162, [%rd84+2304];
	add.s32 	%r59, %r162, 1;
	@%p7 bra 	$L__BB3_61;

	ld.local.u64 	%rd190, [%rd15];
	mul.wide.u32 	%rd191, %r1, 32;
	add.s64 	%rd192, %rd190, %rd191;
	mul.wide.s32 	%rd193, %r60, 4;
	add.s64 	%rd194, %rd192, %rd193;
	add.s64 	%rd189, %rd194, 1152;
	add.s64 	%rd195, %rd34, %rd191;
	mul.wide.u32 	%rd196, %r2, 4;
	add.s64 	%rd197, %rd195, %rd196;
	add.s64 	%rd33, %rd197, 1152;
	// begin inline asm
	st.volatile.global.u32 [%rd189], %r59;
	// end inline asm

$L__BB3_60:
	// begin inline asm
	ld.volatile.global.u32 %r166, [%rd33];
	// end inline asm
	setp.ne.s32 	%p55, %r166, %r59;
	@%p55 bra 	$L__BB3_60;

$L__BB3_61:
	@%p9 bra 	$L__BB3_63;

	add.s32 	%r171, %r162, 1;
	st.global.u32 	[%rd84+2304], %r171;

$L__BB3_63:
	ret;

}
	// .globl	custom_allreduce_barrier
.visible .entry custom_allreduce_barrier(
	.param .align 16 .b8 custom_allreduce_barrier_param_0[64],
	.param .u64 custom_allreduce_barrier_param_1,
	.param .u32 custom_allreduce_barrier_param_2,
	.param .u32 custom_allreduce_barrier_param_3
)
{
	.local .align 16 .b8 	__local_depot4[64];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<7>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<40>;


	mov.u64 	%SPL, __local_depot4;
	ld.param.u64 	%rd7, [custom_allreduce_barrier_param_1];
	ld.param.u32 	%r4, [custom_allreduce_barrier_param_2];
	ld.param.u32 	%r5, [custom_allreduce_barrier_param_3];
	cvta.to.global.u64 	%rd8, %rd7;
	add.u64 	%rd10, %SPL, 0;
	ld.param.u64 	%rd11, [custom_allreduce_barrier_param_0+8];
	ld.param.u64 	%rd12, [custom_allreduce_barrier_param_0];
	st.local.v2.u64 	[%rd10], {%rd12, %rd11};
	ld.param.u64 	%rd13, [custom_allreduce_barrier_param_0+24];
	ld.param.u64 	%rd14, [custom_allreduce_barrier_param_0+16];
	st.local.v2.u64 	[%rd10+16], {%rd14, %rd13};
	ld.param.u64 	%rd15, [custom_allreduce_barrier_param_0+40];
	ld.param.u64 	%rd16, [custom_allreduce_barrier_param_0+32];
	st.local.v2.u64 	[%rd10+32], {%rd16, %rd15};
	ld.param.u64 	%rd17, [custom_allreduce_barrier_param_0+56];
	ld.param.u64 	%rd18, [custom_allreduce_barrier_param_0+48];
	st.local.v2.u64 	[%rd10+48], {%rd18, %rd17};
	mov.u32 	%r6, %ctaid.x;
	cvt.u64.u32 	%rd1, %r6;
	mul.wide.u32 	%rd19, %r6, 4;
	add.s64 	%rd20, %rd8, %rd19;
	add.s64 	%rd2, %rd20, 2304;
	ld.global.u32 	%r7, [%rd20+2304];
	add.s32 	%r1, %r7, 1;
	mov.u32 	%r2, %tid.x;
	setp.ge.u32 	%p1, %r2, %r5;
	cvt.u64.u32 	%rd3, %r2;
	mul.wide.u32 	%rd21, %r2, 8;
	add.s64 	%rd4, %rd10, %rd21;
	@%p1 bra 	$L__BB4_3;

	ld.local.u64 	%rd23, [%rd4];
	shl.b64 	%rd24, %rd1, 5;
	add.s64 	%rd25, %rd23, %rd24;
	mul.wide.s32 	%rd26, %r4, 4;
	add.s64 	%rd22, %rd25, %rd26;
	add.s64 	%rd27, %rd7, %rd24;
	mul.wide.u32 	%rd28, %r2, 4;
	add.s64 	%rd5, %rd27, %rd28;
	// begin inline asm
	st.volatile.global.u32 [%rd22], %r1;
	// end inline asm

$L__BB4_2:
	// begin inline asm
	ld.volatile.global.u32 %r9, [%rd5];
	// end inline asm
	setp.ne.s32 	%p2, %r9, %r1;
	@%p2 bra 	$L__BB4_2;

$L__BB4_3:
	bar.sync 	0;
	setp.ne.s32 	%p3, %r2, 0;
	@%p3 bra 	$L__BB4_5;

	add.s32 	%r13, %r7, 1;
	st.global.u32 	[%rd2], %r13;

$L__BB4_5:
	bar.sync 	0;
	ld.global.u32 	%r10, [%rd2];
	add.s32 	%r3, %r10, 1;
	@%p1 bra 	$L__BB4_8;

	ld.local.u64 	%rd31, [%rd4];
	shl.b64 	%rd32, %rd1, 5;
	add.s64 	%rd33, %rd31, %rd32;
	mul.wide.s32 	%rd34, %r4, 4;
	add.s64 	%rd35, %rd33, %rd34;
	add.s64 	%rd30, %rd35, 1152;
	add.s64 	%rd36, %rd7, %rd32;
	shl.b64 	%rd37, %rd3, 2;
	add.s64 	%rd38, %rd36, %rd37;
	add.s64 	%rd6, %rd38, 1152;
	// begin inline asm
	st.volatile.global.u32 [%rd30], %r3;
	// end inline asm

$L__BB4_7:
	// begin inline asm
	ld.volatile.global.u32 %r12, [%rd6];
	// end inline asm
	setp.ne.s32 	%p5, %r12, %r3;
	@%p5 bra 	$L__BB4_7;

$L__BB4_8:
	@%p3 bra 	$L__BB4_10;

	add.s32 	%r14, %r10, 1;
	st.global.u32 	[%rd2], %r14;

$L__BB4_10:
	ret;

}

