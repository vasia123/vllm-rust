//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32267302
// Cuda compilation tools, release 12.0, V12.0.140
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_89
.address_size 64

	// .globl	argmax_bf16
// _ZZ11argmax_bf16E9smem_vals has been demoted
// _ZZ11argmax_bf16E9smem_idxs has been demoted
// _ZZ16softmax_to_probsE4smem has been demoted
// _ZZ10argmax_f32E9smem_vals has been demoted
// _ZZ10argmax_f32E9smem_idxs has been demoted
// _ZZ20softmax_to_probs_f32E4smem has been demoted

.visible .entry argmax_bf16(
	.param .u64 argmax_bf16_param_0,
	.param .u64 argmax_bf16_param_1,
	.param .u32 argmax_bf16_param_2
)
{
	.reg .pred 	%p<46>;
	.reg .b16 	%rs<6>;
	.reg .f32 	%f<48>;
	.reg .b32 	%r<126>;
	.reg .b64 	%rd<19>;
	// demoted variable
	.shared .align 4 .b8 _ZZ11argmax_bf16E9smem_vals[32];
	// demoted variable
	.shared .align 4 .b8 _ZZ11argmax_bf16E9smem_idxs[32];

	ld.param.u64 	%rd6, [argmax_bf16_param_0];
	ld.param.u64 	%rd7, [argmax_bf16_param_1];
	ld.param.u32 	%r27, [argmax_bf16_param_2];
	cvta.to.global.u64 	%rd1, %rd7;
	mov.u32 	%r1, %ctaid.x;
	mul.lo.s32 	%r29, %r1, %r27;
	cvt.s64.s32 	%rd2, %r29;
	mov.u32 	%r2, %tid.x;
	setp.ge.s32 	%p1, %r2, %r27;
	mov.u32 	%r124, 0;
	mov.f32 	%f46, 0fFF7FFFFF;
	@%p1 bra 	$L__BB0_7;

	not.b32 	%r32, %r2;
	add.s32 	%r33, %r32, %r27;
	shr.u32 	%r34, %r33, 8;
	add.s32 	%r3, %r34, 1;
	and.b32  	%r123, %r3, 3;
	setp.lt.u32 	%p2, %r33, 768;
	mov.f32 	%f46, 0fFF7FFFFF;
	mov.u32 	%r124, 0;
	mov.u32 	%r119, %r2;
	@%p2 bra 	$L__BB0_4;

	sub.s32 	%r117, %r3, %r123;
	mov.u32 	%r119, %r2;

$L__BB0_3:
	cvt.s64.s32 	%rd8, %r119;
	add.s64 	%rd9, %rd8, %rd2;
	shl.b64 	%rd10, %rd9, 1;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.nc.u16 	%rs1, [%rd11];
	// begin inline asm
	{ mov.b32 %f15, {0,%rs1};}

	// end inline asm
	setp.gt.ftz.f32 	%p3, %f15, %f46;
	selp.f32 	%f19, %f15, %f46, %p3;
	selp.b32 	%r36, %r119, %r124, %p3;
	ld.global.nc.u16 	%rs2, [%rd11+512];
	// begin inline asm
	{ mov.b32 %f16, {0,%rs2};}

	// end inline asm
	setp.gt.ftz.f32 	%p4, %f16, %f19;
	selp.f32 	%f20, %f16, %f19, %p4;
	add.s32 	%r37, %r119, 256;
	selp.b32 	%r38, %r37, %r36, %p4;
	ld.global.nc.u16 	%rs3, [%rd11+1024];
	// begin inline asm
	{ mov.b32 %f17, {0,%rs3};}

	// end inline asm
	setp.gt.ftz.f32 	%p5, %f17, %f20;
	selp.f32 	%f21, %f17, %f20, %p5;
	add.s32 	%r39, %r119, 512;
	selp.b32 	%r40, %r39, %r38, %p5;
	ld.global.nc.u16 	%rs4, [%rd11+1536];
	// begin inline asm
	{ mov.b32 %f18, {0,%rs4};}

	// end inline asm
	setp.gt.ftz.f32 	%p6, %f18, %f21;
	selp.f32 	%f46, %f18, %f21, %p6;
	add.s32 	%r41, %r119, 768;
	selp.b32 	%r124, %r41, %r40, %p6;
	add.s32 	%r119, %r119, 1024;
	add.s32 	%r117, %r117, -4;
	setp.ne.s32 	%p7, %r117, 0;
	@%p7 bra 	$L__BB0_3;

$L__BB0_4:
	setp.eq.s32 	%p8, %r123, 0;
	@%p8 bra 	$L__BB0_7;

	cvt.s64.s32 	%rd12, %r119;
	add.s64 	%rd13, %rd12, %rd2;
	shl.b64 	%rd14, %rd13, 1;
	add.s64 	%rd18, %rd1, %rd14;

$L__BB0_6:
	.pragma "nounroll";
	ld.global.nc.u16 	%rs5, [%rd18];
	// begin inline asm
	{ mov.b32 %f22, {0,%rs5};}

	// end inline asm
	setp.gt.ftz.f32 	%p9, %f22, %f46;
	selp.f32 	%f46, %f22, %f46, %p9;
	selp.b32 	%r124, %r119, %r124, %p9;
	add.s32 	%r119, %r119, 256;
	add.s64 	%rd18, %rd18, 512;
	add.s32 	%r123, %r123, -1;
	setp.ne.s32 	%p10, %r123, 0;
	@%p10 bra 	$L__BB0_6;

$L__BB0_7:
	mov.b32 	%r42, %f46;
	mov.u32 	%r43, 31;
	mov.u32 	%r44, 16;
	mov.u32 	%r45, -1;
	shfl.sync.bfly.b32 	%r46|%p11, %r42, %r44, %r43, %r45;
	mov.b32 	%f23, %r46;
	shfl.sync.bfly.b32 	%r47|%p12, %r124, %r44, %r43, %r45;
	setp.lt.ftz.f32 	%p13, %f46, %f23;
	selp.f32 	%f24, %f23, %f46, %p13;
	selp.b32 	%r48, %r47, %r124, %p13;
	mov.b32 	%r49, %f24;
	mov.u32 	%r50, 8;
	shfl.sync.bfly.b32 	%r51|%p14, %r49, %r50, %r43, %r45;
	mov.b32 	%f25, %r51;
	shfl.sync.bfly.b32 	%r52|%p15, %r48, %r50, %r43, %r45;
	setp.lt.ftz.f32 	%p16, %f24, %f25;
	selp.f32 	%f26, %f25, %f24, %p16;
	selp.b32 	%r53, %r52, %r48, %p16;
	mov.b32 	%r54, %f26;
	mov.u32 	%r55, 4;
	shfl.sync.bfly.b32 	%r56|%p17, %r54, %r55, %r43, %r45;
	mov.b32 	%f27, %r56;
	shfl.sync.bfly.b32 	%r57|%p18, %r53, %r55, %r43, %r45;
	setp.lt.ftz.f32 	%p19, %f26, %f27;
	selp.f32 	%f28, %f27, %f26, %p19;
	selp.b32 	%r58, %r57, %r53, %p19;
	mov.b32 	%r59, %f28;
	mov.u32 	%r60, 2;
	shfl.sync.bfly.b32 	%r61|%p20, %r59, %r60, %r43, %r45;
	mov.b32 	%f29, %r61;
	shfl.sync.bfly.b32 	%r62|%p21, %r58, %r60, %r43, %r45;
	setp.lt.ftz.f32 	%p22, %f28, %f29;
	selp.f32 	%f30, %f29, %f28, %p22;
	selp.b32 	%r63, %r62, %r58, %p22;
	mov.b32 	%r64, %f30;
	mov.u32 	%r65, 1;
	shfl.sync.bfly.b32 	%r66|%p23, %r64, %r65, %r43, %r45;
	mov.b32 	%f31, %r66;
	shfl.sync.bfly.b32 	%r67|%p24, %r63, %r65, %r43, %r45;
	setp.lt.ftz.f32 	%p25, %f30, %f31;
	selp.f32 	%f8, %f31, %f30, %p25;
	selp.b32 	%r22, %r67, %r63, %p25;
	shr.s32 	%r68, %r2, 31;
	shr.u32 	%r69, %r68, 27;
	add.s32 	%r70, %r2, %r69;
	and.b32  	%r71, %r70, -32;
	sub.s32 	%r23, %r2, %r71;
	setp.ne.s32 	%p26, %r23, 0;
	@%p26 bra 	$L__BB0_9;

	shr.s32 	%r75, %r70, 5;
	shl.b32 	%r76, %r75, 2;
	mov.u32 	%r77, _ZZ11argmax_bf16E9smem_vals;
	add.s32 	%r78, %r77, %r76;
	st.shared.f32 	[%r78], %f8;
	mov.u32 	%r79, _ZZ11argmax_bf16E9smem_idxs;
	add.s32 	%r80, %r79, %r76;
	st.shared.u32 	[%r80], %r22;

$L__BB0_9:
	bar.sync 	0;
	add.s32 	%r81, %r2, 31;
	setp.gt.u32 	%p27, %r81, 62;
	@%p27 bra 	$L__BB0_16;

	setp.gt.s32 	%p28, %r23, 7;
	mov.f32 	%f47, 0fFF7FFFFF;
	@%p28 bra 	$L__BB0_12;

	shl.b32 	%r82, %r23, 2;
	mov.u32 	%r83, _ZZ11argmax_bf16E9smem_vals;
	add.s32 	%r84, %r83, %r82;
	ld.shared.f32 	%f47, [%r84];

$L__BB0_12:
	mov.u32 	%r125, 0;
	@%p28 bra 	$L__BB0_14;

	shl.b32 	%r86, %r23, 2;
	mov.u32 	%r87, _ZZ11argmax_bf16E9smem_idxs;
	add.s32 	%r88, %r87, %r86;
	ld.shared.u32 	%r125, [%r88];

$L__BB0_14:
	mov.b32 	%r89, %f47;
	mov.u32 	%r90, 31;
	mov.u32 	%r91, 16;
	mov.u32 	%r92, -1;
	shfl.sync.bfly.b32 	%r93|%p30, %r89, %r91, %r90, %r92;
	mov.b32 	%f33, %r93;
	shfl.sync.bfly.b32 	%r94|%p31, %r125, %r91, %r90, %r92;
	setp.lt.ftz.f32 	%p32, %f47, %f33;
	selp.f32 	%f34, %f33, %f47, %p32;
	selp.b32 	%r95, %r94, %r125, %p32;
	mov.b32 	%r96, %f34;
	mov.u32 	%r97, 8;
	shfl.sync.bfly.b32 	%r98|%p33, %r96, %r97, %r90, %r92;
	mov.b32 	%f35, %r98;
	shfl.sync.bfly.b32 	%r99|%p34, %r95, %r97, %r90, %r92;
	setp.lt.ftz.f32 	%p35, %f34, %f35;
	selp.f32 	%f36, %f35, %f34, %p35;
	selp.b32 	%r100, %r99, %r95, %p35;
	mov.b32 	%r101, %f36;
	mov.u32 	%r102, 4;
	shfl.sync.bfly.b32 	%r103|%p36, %r101, %r102, %r90, %r92;
	mov.b32 	%f37, %r103;
	shfl.sync.bfly.b32 	%r104|%p37, %r100, %r102, %r90, %r92;
	setp.lt.ftz.f32 	%p38, %f36, %f37;
	selp.f32 	%f38, %f37, %f36, %p38;
	selp.b32 	%r105, %r104, %r100, %p38;
	mov.b32 	%r106, %f38;
	mov.u32 	%r107, 2;
	shfl.sync.bfly.b32 	%r108|%p39, %r106, %r107, %r90, %r92;
	mov.b32 	%f39, %r108;
	shfl.sync.bfly.b32 	%r109|%p40, %r105, %r107, %r90, %r92;
	setp.lt.ftz.f32 	%p41, %f38, %f39;
	selp.f32 	%f40, %f39, %f38, %p41;
	selp.b32 	%r110, %r109, %r105, %p41;
	mov.b32 	%r111, %f40;
	mov.u32 	%r112, 1;
	shfl.sync.bfly.b32 	%r113|%p42, %r111, %r112, %r90, %r92;
	mov.b32 	%f41, %r113;
	shfl.sync.bfly.b32 	%r114|%p43, %r110, %r112, %r90, %r92;
	setp.lt.ftz.f32 	%p44, %f40, %f41;
	selp.b32 	%r26, %r114, %r110, %p44;
	@%p26 bra 	$L__BB0_16;

	cvta.to.global.u64 	%rd15, %rd6;
	mul.wide.s32 	%rd16, %r1, 4;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.u32 	[%rd17], %r26;

$L__BB0_16:
	ret;

}
	// .globl	temperature_scale_bf16
.visible .entry temperature_scale_bf16(
	.param .u64 temperature_scale_bf16_param_0,
	.param .f32 temperature_scale_bf16_param_1,
	.param .u32 temperature_scale_bf16_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<11>;
	.reg .f32 	%f<12>;
	.reg .b32 	%r<21>;
	.reg .b64 	%rd<19>;


	ld.param.u64 	%rd9, [temperature_scale_bf16_param_0];
	ld.param.f32 	%f1, [temperature_scale_bf16_param_1];
	ld.param.u32 	%r11, [temperature_scale_bf16_param_2];
	cvta.to.global.u64 	%rd1, %rd9;
	mov.u32 	%r12, %ctaid.x;
	mul.lo.s32 	%r13, %r12, %r11;
	cvt.s64.s32 	%rd2, %r13;
	mov.u32 	%r19, %tid.x;
	setp.ge.s32 	%p1, %r19, %r11;
	@%p1 bra 	$L__BB1_7;

	not.b32 	%r14, %r19;
	add.s32 	%r2, %r14, %r11;
	shr.u32 	%r15, %r2, 8;
	add.s32 	%r16, %r15, 1;
	and.b32  	%r18, %r16, 3;
	setp.eq.s32 	%p2, %r18, 0;
	@%p2 bra 	$L__BB1_4;

	cvt.s64.s32 	%rd10, %r19;
	add.s64 	%rd11, %rd10, %rd2;
	shl.b64 	%rd12, %rd11, 1;
	add.s64 	%rd17, %rd1, %rd12;

$L__BB1_3:
	.pragma "nounroll";
	ld.global.u16 	%rs1, [%rd17];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs1};}

	// end inline asm
	mul.ftz.f32 	%f3, %f2, %f1;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs2, %f3;}

	// end inline asm
	st.global.u16 	[%rd17], %rs2;
	add.s32 	%r19, %r19, 256;
	add.s64 	%rd17, %rd17, 512;
	add.s32 	%r18, %r18, -1;
	setp.ne.s32 	%p3, %r18, 0;
	@%p3 bra 	$L__BB1_3;

$L__BB1_4:
	setp.lt.u32 	%p4, %r2, 768;
	@%p4 bra 	$L__BB1_7;

	cvt.s64.s32 	%rd13, %r19;
	add.s64 	%rd14, %rd13, %rd2;
	shl.b64 	%rd15, %rd14, 1;
	add.s64 	%rd16, %rd1, %rd15;
	add.s64 	%rd18, %rd16, 1024;

$L__BB1_6:
	ld.global.u16 	%rs3, [%rd18+-1024];
	// begin inline asm
	{ mov.b32 %f4, {0,%rs3};}

	// end inline asm
	mul.ftz.f32 	%f5, %f4, %f1;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs4, %f5;}

	// end inline asm
	st.global.u16 	[%rd18+-1024], %rs4;
	ld.global.u16 	%rs5, [%rd18+-512];
	// begin inline asm
	{ mov.b32 %f6, {0,%rs5};}

	// end inline asm
	mul.ftz.f32 	%f7, %f6, %f1;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs6, %f7;}

	// end inline asm
	st.global.u16 	[%rd18+-512], %rs6;
	ld.global.u16 	%rs7, [%rd18];
	// begin inline asm
	{ mov.b32 %f8, {0,%rs7};}

	// end inline asm
	mul.ftz.f32 	%f9, %f8, %f1;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs8, %f9;}

	// end inline asm
	st.global.u16 	[%rd18], %rs8;
	ld.global.u16 	%rs9, [%rd18+512];
	// begin inline asm
	{ mov.b32 %f10, {0,%rs9};}

	// end inline asm
	mul.ftz.f32 	%f11, %f10, %f1;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs10, %f11;}

	// end inline asm
	st.global.u16 	[%rd18+512], %rs10;
	add.s64 	%rd18, %rd18, 2048;
	add.s32 	%r19, %r19, 1024;
	setp.lt.s32 	%p5, %r19, %r11;
	@%p5 bra 	$L__BB1_6;

$L__BB1_7:
	ret;

}
	// .globl	softmax_to_probs
.visible .entry softmax_to_probs(
	.param .u64 softmax_to_probs_param_0,
	.param .u64 softmax_to_probs_param_1,
	.param .u32 softmax_to_probs_param_2
)
{
	.reg .pred 	%p<44>;
	.reg .b16 	%rs<11>;
	.reg .f32 	%f<127>;
	.reg .b32 	%r<140>;
	.reg .b64 	%rd<64>;
	// demoted variable
	.shared .align 4 .b8 _ZZ16softmax_to_probsE4smem[32];

	ld.param.u64 	%rd29, [softmax_to_probs_param_0];
	ld.param.u64 	%rd28, [softmax_to_probs_param_1];
	ld.param.u32 	%r33, [softmax_to_probs_param_2];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd29;
	mov.u32 	%r138, %tid.x;
	shr.s32 	%r34, %r138, 31;
	shr.u32 	%r35, %r34, 27;
	add.s32 	%r36, %r138, %r35;
	and.b32  	%r37, %r36, -32;
	sub.s32 	%r2, %r138, %r37;
	mov.u32 	%r38, %ctaid.x;
	mul.lo.s32 	%r39, %r38, %r33;
	cvt.s64.s32 	%rd3, %r39;
	setp.ge.s32 	%p1, %r138, %r33;
	mov.f32 	%f119, 0fFF7FFFFF;
	@%p1 bra 	$L__BB2_7;

	not.b32 	%r40, %r138;
	add.s32 	%r3, %r40, %r33;
	shr.u32 	%r41, %r3, 8;
	add.s32 	%r42, %r41, 1;
	and.b32  	%r129, %r42, 3;
	setp.eq.s32 	%p2, %r129, 0;
	mov.f32 	%f119, 0fFF7FFFFF;
	mov.u32 	%r130, %r138;
	@%p2 bra 	$L__BB2_4;

	cvt.s64.s32 	%rd30, %r138;
	add.s64 	%rd31, %rd30, %rd3;
	shl.b64 	%rd32, %rd31, 1;
	add.s64 	%rd56, %rd1, %rd32;
	mov.u32 	%r130, %r138;

$L__BB2_3:
	.pragma "nounroll";
	ld.global.nc.u16 	%rs1, [%rd56];
	// begin inline asm
	{ mov.b32 %f29, {0,%rs1};}

	// end inline asm
	max.ftz.f32 	%f119, %f119, %f29;
	add.s32 	%r130, %r130, 256;
	add.s64 	%rd56, %rd56, 512;
	add.s32 	%r129, %r129, -1;
	setp.ne.s32 	%p3, %r129, 0;
	@%p3 bra 	$L__BB2_3;

$L__BB2_4:
	setp.lt.u32 	%p4, %r3, 768;
	@%p4 bra 	$L__BB2_7;

	cvt.s64.s32 	%rd33, %r130;
	add.s64 	%rd34, %rd33, %rd3;
	shl.b64 	%rd35, %rd34, 1;
	add.s64 	%rd36, %rd1, %rd35;
	add.s64 	%rd57, %rd36, 1024;

$L__BB2_6:
	ld.global.nc.u16 	%rs2, [%rd57+-1024];
	// begin inline asm
	{ mov.b32 %f30, {0,%rs2};}

	// end inline asm
	max.ftz.f32 	%f34, %f119, %f30;
	ld.global.nc.u16 	%rs3, [%rd57+-512];
	// begin inline asm
	{ mov.b32 %f31, {0,%rs3};}

	// end inline asm
	max.ftz.f32 	%f35, %f34, %f31;
	ld.global.nc.u16 	%rs4, [%rd57];
	// begin inline asm
	{ mov.b32 %f32, {0,%rs4};}

	// end inline asm
	max.ftz.f32 	%f36, %f35, %f32;
	ld.global.nc.u16 	%rs5, [%rd57+512];
	// begin inline asm
	{ mov.b32 %f33, {0,%rs5};}

	// end inline asm
	max.ftz.f32 	%f119, %f36, %f33;
	add.s64 	%rd57, %rd57, 2048;
	add.s32 	%r130, %r130, 1024;
	setp.lt.s32 	%p5, %r130, %r33;
	@%p5 bra 	$L__BB2_6;

$L__BB2_7:
	mov.b32 	%r43, %f119;
	mov.u32 	%r44, 31;
	mov.u32 	%r45, 16;
	mov.u32 	%r46, -1;
	shfl.sync.bfly.b32 	%r47|%p6, %r43, %r45, %r44, %r46;
	mov.b32 	%f37, %r47;
	max.ftz.f32 	%f38, %f119, %f37;
	mov.b32 	%r48, %f38;
	mov.u32 	%r49, 8;
	shfl.sync.bfly.b32 	%r50|%p7, %r48, %r49, %r44, %r46;
	mov.b32 	%f39, %r50;
	max.ftz.f32 	%f40, %f38, %f39;
	mov.b32 	%r51, %f40;
	mov.u32 	%r52, 4;
	shfl.sync.bfly.b32 	%r53|%p8, %r51, %r52, %r44, %r46;
	mov.b32 	%f41, %r53;
	max.ftz.f32 	%f42, %f40, %f41;
	mov.b32 	%r54, %f42;
	mov.u32 	%r55, 2;
	shfl.sync.bfly.b32 	%r56|%p9, %r54, %r55, %r44, %r46;
	mov.b32 	%f43, %r56;
	max.ftz.f32 	%f44, %f42, %f43;
	mov.b32 	%r57, %f44;
	mov.u32 	%r58, 1;
	shfl.sync.bfly.b32 	%r59|%p10, %r57, %r58, %r44, %r46;
	mov.b32 	%f45, %r59;
	max.ftz.f32 	%f8, %f44, %f45;
	shr.s32 	%r63, %r36, 5;
	shl.b32 	%r64, %r63, 2;
	mov.u32 	%r65, _ZZ16softmax_to_probsE4smem;
	add.s32 	%r12, %r65, %r64;
	setp.ne.s32 	%p11, %r2, 0;
	@%p11 bra 	$L__BB2_9;

	st.shared.f32 	[%r12], %f8;

$L__BB2_9:
	bar.sync 	0;
	add.s32 	%r13, %r138, 31;
	setp.gt.u32 	%p12, %r13, 62;
	shl.b32 	%r66, %r2, 2;
	add.s32 	%r14, %r65, %r66;
	@%p12 bra 	$L__BB2_14;

	setp.gt.s32 	%p13, %r2, 7;
	mov.f32 	%f120, 0fFF7FFFFF;
	@%p13 bra 	$L__BB2_12;

	ld.shared.f32 	%f120, [%r14];

$L__BB2_12:
	mov.b32 	%r68, %f120;
	mov.u32 	%r69, 31;
	mov.u32 	%r70, 16;
	mov.u32 	%r71, -1;
	shfl.sync.bfly.b32 	%r72|%p14, %r68, %r70, %r69, %r71;
	mov.b32 	%f47, %r72;
	max.ftz.f32 	%f48, %f120, %f47;
	mov.b32 	%r73, %f48;
	mov.u32 	%r74, 8;
	shfl.sync.bfly.b32 	%r75|%p15, %r73, %r74, %r69, %r71;
	mov.b32 	%f49, %r75;
	max.ftz.f32 	%f50, %f48, %f49;
	mov.b32 	%r76, %f50;
	mov.u32 	%r77, 4;
	shfl.sync.bfly.b32 	%r78|%p16, %r76, %r77, %r69, %r71;
	mov.b32 	%f51, %r78;
	max.ftz.f32 	%f52, %f50, %f51;
	mov.b32 	%r79, %f52;
	mov.u32 	%r80, 2;
	shfl.sync.bfly.b32 	%r81|%p17, %r79, %r80, %r69, %r71;
	mov.b32 	%f53, %r81;
	max.ftz.f32 	%f54, %f52, %f53;
	mov.b32 	%r82, %f54;
	mov.u32 	%r83, 1;
	shfl.sync.bfly.b32 	%r84|%p18, %r82, %r83, %r69, %r71;
	mov.b32 	%f55, %r84;
	max.ftz.f32 	%f11, %f54, %f55;
	@%p11 bra 	$L__BB2_14;

	st.shared.f32 	[_ZZ16softmax_to_probsE4smem], %f11;

$L__BB2_14:
	bar.sync 	0;
	mov.f32 	%f125, 0f00000000;
	ld.shared.f32 	%f12, [_ZZ16softmax_to_probsE4smem];
	@%p1 bra 	$L__BB2_21;

	not.b32 	%r85, %r138;
	add.s32 	%r15, %r85, %r33;
	shr.u32 	%r86, %r15, 8;
	add.s32 	%r87, %r86, 1;
	and.b32  	%r133, %r87, 3;
	setp.eq.s32 	%p21, %r133, 0;
	mov.f32 	%f125, 0f00000000;
	mov.u32 	%r134, %r138;
	@%p21 bra 	$L__BB2_18;

	cvt.s64.s32 	%rd37, %r138;
	add.s64 	%rd38, %rd37, %rd3;
	shl.b64 	%rd39, %rd38, 2;
	add.s64 	%rd59, %rd2, %rd39;
	shl.b64 	%rd41, %rd38, 1;
	add.s64 	%rd58, %rd1, %rd41;
	mov.u32 	%r134, %r138;

$L__BB2_17:
	.pragma "nounroll";
	ld.global.nc.u16 	%rs6, [%rd58];
	// begin inline asm
	{ mov.b32 %f60, {0,%rs6};}

	// end inline asm
	sub.ftz.f32 	%f61, %f60, %f12;
	mul.ftz.f32 	%f62, %f61, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f63, %f62;
	st.global.f32 	[%rd59], %f63;
	add.ftz.f32 	%f125, %f125, %f63;
	add.s32 	%r134, %r134, 256;
	add.s64 	%rd59, %rd59, 1024;
	add.s64 	%rd58, %rd58, 512;
	add.s32 	%r133, %r133, -1;
	setp.ne.s32 	%p22, %r133, 0;
	@%p22 bra 	$L__BB2_17;

$L__BB2_18:
	setp.lt.u32 	%p23, %r15, 768;
	@%p23 bra 	$L__BB2_21;

	cvt.s64.s32 	%rd42, %r134;
	add.s64 	%rd43, %rd42, %rd3;
	shl.b64 	%rd44, %rd43, 2;
	add.s64 	%rd45, %rd2, %rd44;
	add.s64 	%rd61, %rd45, 2048;
	shl.b64 	%rd47, %rd43, 1;
	add.s64 	%rd48, %rd1, %rd47;
	add.s64 	%rd60, %rd48, 1024;

$L__BB2_20:
	ld.global.nc.u16 	%rs7, [%rd60+-1024];
	// begin inline asm
	{ mov.b32 %f64, {0,%rs7};}

	// end inline asm
	sub.ftz.f32 	%f68, %f64, %f12;
	mul.ftz.f32 	%f69, %f68, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f70, %f69;
	st.global.f32 	[%rd61+-2048], %f70;
	add.ftz.f32 	%f71, %f125, %f70;
	ld.global.nc.u16 	%rs8, [%rd60+-512];
	// begin inline asm
	{ mov.b32 %f65, {0,%rs8};}

	// end inline asm
	sub.ftz.f32 	%f72, %f65, %f12;
	mul.ftz.f32 	%f73, %f72, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f74, %f73;
	st.global.f32 	[%rd61+-1024], %f74;
	add.ftz.f32 	%f75, %f71, %f74;
	ld.global.nc.u16 	%rs9, [%rd60];
	// begin inline asm
	{ mov.b32 %f66, {0,%rs9};}

	// end inline asm
	sub.ftz.f32 	%f76, %f66, %f12;
	mul.ftz.f32 	%f77, %f76, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f78, %f77;
	st.global.f32 	[%rd61], %f78;
	add.ftz.f32 	%f79, %f75, %f78;
	ld.global.nc.u16 	%rs10, [%rd60+512];
	// begin inline asm
	{ mov.b32 %f67, {0,%rs10};}

	// end inline asm
	sub.ftz.f32 	%f80, %f67, %f12;
	mul.ftz.f32 	%f81, %f80, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f82, %f81;
	st.global.f32 	[%rd61+1024], %f82;
	add.ftz.f32 	%f125, %f79, %f82;
	add.s64 	%rd61, %rd61, 4096;
	add.s64 	%rd60, %rd60, 2048;
	add.s32 	%r134, %r134, 1024;
	setp.lt.s32 	%p24, %r134, %r33;
	@%p24 bra 	$L__BB2_20;

$L__BB2_21:
	mov.b32 	%r88, %f125;
	mov.u32 	%r89, 31;
	mov.u32 	%r90, 16;
	mov.u32 	%r91, -1;
	shfl.sync.bfly.b32 	%r92|%p25, %r88, %r90, %r89, %r91;
	mov.b32 	%f83, %r92;
	add.ftz.f32 	%f84, %f125, %f83;
	mov.b32 	%r93, %f84;
	mov.u32 	%r94, 8;
	shfl.sync.bfly.b32 	%r95|%p26, %r93, %r94, %r89, %r91;
	mov.b32 	%f85, %r95;
	add.ftz.f32 	%f86, %f84, %f85;
	mov.b32 	%r96, %f86;
	mov.u32 	%r97, 4;
	shfl.sync.bfly.b32 	%r98|%p27, %r96, %r97, %r89, %r91;
	mov.b32 	%f87, %r98;
	add.ftz.f32 	%f88, %f86, %f87;
	mov.b32 	%r99, %f88;
	mov.u32 	%r100, 2;
	shfl.sync.bfly.b32 	%r101|%p28, %r99, %r100, %r89, %r91;
	mov.b32 	%f89, %r101;
	add.ftz.f32 	%f90, %f88, %f89;
	mov.b32 	%r102, %f90;
	mov.u32 	%r103, 1;
	shfl.sync.bfly.b32 	%r104|%p29, %r102, %r103, %r89, %r91;
	mov.b32 	%f91, %r104;
	add.ftz.f32 	%f20, %f90, %f91;
	@%p11 bra 	$L__BB2_23;

	st.shared.f32 	[%r12], %f20;

$L__BB2_23:
	bar.sync 	0;
	@%p12 bra 	$L__BB2_28;

	setp.gt.s32 	%p32, %r2, 7;
	mov.f32 	%f126, 0f00000000;
	@%p32 bra 	$L__BB2_26;

	ld.shared.f32 	%f126, [%r14];

$L__BB2_26:
	mov.b32 	%r108, %f126;
	mov.u32 	%r109, 31;
	mov.u32 	%r110, 16;
	mov.u32 	%r111, -1;
	shfl.sync.bfly.b32 	%r112|%p33, %r108, %r110, %r109, %r111;
	mov.b32 	%f93, %r112;
	add.ftz.f32 	%f94, %f126, %f93;
	mov.b32 	%r113, %f94;
	mov.u32 	%r114, 8;
	shfl.sync.bfly.b32 	%r115|%p34, %r113, %r114, %r109, %r111;
	mov.b32 	%f95, %r115;
	add.ftz.f32 	%f96, %f94, %f95;
	mov.b32 	%r116, %f96;
	mov.u32 	%r117, 4;
	shfl.sync.bfly.b32 	%r118|%p35, %r116, %r117, %r109, %r111;
	mov.b32 	%f97, %r118;
	add.ftz.f32 	%f98, %f96, %f97;
	mov.b32 	%r119, %f98;
	mov.u32 	%r120, 2;
	shfl.sync.bfly.b32 	%r121|%p36, %r119, %r120, %r109, %r111;
	mov.b32 	%f99, %r121;
	add.ftz.f32 	%f100, %f98, %f99;
	mov.b32 	%r122, %f100;
	mov.u32 	%r123, 1;
	shfl.sync.bfly.b32 	%r124|%p37, %r122, %r123, %r109, %r111;
	mov.b32 	%f101, %r124;
	add.ftz.f32 	%f23, %f100, %f101;
	@%p11 bra 	$L__BB2_28;

	st.shared.f32 	[_ZZ16softmax_to_probsE4smem], %f23;

$L__BB2_28:
	bar.sync 	0;
	ld.shared.f32 	%f102, [_ZZ16softmax_to_probsE4smem];
	add.ftz.f32 	%f103, %f102, 0f358637BD;
	mov.f32 	%f104, 0f3F800000;
	div.approx.ftz.f32 	%f24, %f104, %f103;
	@%p1 bra 	$L__BB2_35;

	not.b32 	%r125, %r138;
	add.s32 	%r24, %r125, %r33;
	shr.u32 	%r126, %r24, 8;
	add.s32 	%r127, %r126, 1;
	and.b32  	%r137, %r127, 3;
	setp.eq.s32 	%p40, %r137, 0;
	@%p40 bra 	$L__BB2_32;

	cvt.s64.s32 	%rd49, %r138;
	add.s64 	%rd50, %rd49, %rd3;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd62, %rd2, %rd51;

$L__BB2_31:
	.pragma "nounroll";
	ld.global.f32 	%f105, [%rd62];
	mul.ftz.f32 	%f106, %f24, %f105;
	st.global.f32 	[%rd62], %f106;
	add.s32 	%r138, %r138, 256;
	add.s64 	%rd62, %rd62, 1024;
	add.s32 	%r137, %r137, -1;
	setp.ne.s32 	%p41, %r137, 0;
	@%p41 bra 	$L__BB2_31;

$L__BB2_32:
	setp.lt.u32 	%p42, %r24, 768;
	@%p42 bra 	$L__BB2_35;

	cvt.s64.s32 	%rd52, %r138;
	add.s64 	%rd53, %rd52, %rd3;
	shl.b64 	%rd54, %rd53, 2;
	add.s64 	%rd55, %rd2, %rd54;
	add.s64 	%rd63, %rd55, 2048;

$L__BB2_34:
	ld.global.f32 	%f107, [%rd63+-2048];
	mul.ftz.f32 	%f108, %f24, %f107;
	st.global.f32 	[%rd63+-2048], %f108;
	ld.global.f32 	%f109, [%rd63+-1024];
	mul.ftz.f32 	%f110, %f24, %f109;
	st.global.f32 	[%rd63+-1024], %f110;
	ld.global.f32 	%f111, [%rd63];
	mul.ftz.f32 	%f112, %f24, %f111;
	st.global.f32 	[%rd63], %f112;
	ld.global.f32 	%f113, [%rd63+1024];
	mul.ftz.f32 	%f114, %f24, %f113;
	st.global.f32 	[%rd63+1024], %f114;
	add.s64 	%rd63, %rd63, 4096;
	add.s32 	%r138, %r138, 1024;
	setp.lt.s32 	%p43, %r138, %r33;
	@%p43 bra 	$L__BB2_34;

$L__BB2_35:
	ret;

}
	// .globl	top_k_top_p_sample
.visible .entry top_k_top_p_sample(
	.param .u64 top_k_top_p_sample_param_0,
	.param .u64 top_k_top_p_sample_param_1,
	.param .u64 top_k_top_p_sample_param_2,
	.param .u32 top_k_top_p_sample_param_3,
	.param .u32 top_k_top_p_sample_param_4,
	.param .f32 top_k_top_p_sample_param_5
)
{
	.reg .pred 	%p<112>;
	.reg .f32 	%f<152>;
	.reg .b32 	%r<78>;
	.reg .b64 	%rd<59>;


	ld.param.u64 	%rd12, [top_k_top_p_sample_param_0];
	ld.param.u64 	%rd13, [top_k_top_p_sample_param_1];
	ld.param.u64 	%rd14, [top_k_top_p_sample_param_2];
	ld.param.u32 	%r43, [top_k_top_p_sample_param_3];
	ld.param.u32 	%r44, [top_k_top_p_sample_param_4];
	ld.param.f32 	%f51, [top_k_top_p_sample_param_5];
	cvta.to.global.u64 	%rd1, %rd13;
	mov.u32 	%r45, %ctaid.x;
	mul.lo.s32 	%r46, %r45, %r43;
	cvt.s64.s32 	%rd2, %r46;
	cvt.s64.s32 	%rd3, %r45;
	cvta.to.global.u64 	%rd15, %rd14;
	mul.wide.s32 	%rd16, %r45, 4;
	add.s64 	%rd17, %rd15, %rd16;
	ld.global.nc.f32 	%f1, [%rd17];
	setp.ge.s32 	%p1, %r44, %r43;
	setp.lt.s32 	%p2, %r44, 1;
	or.pred  	%p3, %p2, %p1;
	setp.lt.s32 	%p4, %r43, 1;
	mov.f32 	%f148, 0f00000000;
	or.pred  	%p5, %p3, %p4;
	mov.f32 	%f127, %f148;
	@%p5 bra 	$L__BB3_11;

	add.s32 	%r1, %r43, -1;
	and.b32  	%r2, %r43, 3;
	sub.s32 	%r3, %r43, %r2;
	mov.u32 	%r47, 0;
	mov.f32 	%f122, 0f7F7FFFFF;
	mov.u32 	%r60, %r47;

$L__BB3_2:
	setp.lt.u32 	%p6, %r1, 3;
	mov.f32 	%f126, 0fFF7FFFFF;
	mov.u32 	%r63, %r47;
	@%p6 bra 	$L__BB3_5;

	mov.u32 	%r63, 0;
	mov.u32 	%r62, %r3;

$L__BB3_4:
	cvt.s64.s32 	%rd18, %r63;
	add.s64 	%rd19, %rd18, %rd2;
	shl.b64 	%rd20, %rd19, 2;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.nc.f32 	%f58, [%rd21];
	setp.lt.ftz.f32 	%p7, %f58, %f122;
	setp.gt.ftz.f32 	%p8, %f58, %f126;
	and.pred  	%p9, %p7, %p8;
	selp.f32 	%f59, %f58, %f126, %p9;
	ld.global.nc.f32 	%f60, [%rd21+4];
	setp.lt.ftz.f32 	%p10, %f60, %f122;
	setp.gt.ftz.f32 	%p11, %f60, %f59;
	and.pred  	%p12, %p10, %p11;
	selp.f32 	%f61, %f60, %f59, %p12;
	ld.global.nc.f32 	%f62, [%rd21+8];
	setp.lt.ftz.f32 	%p13, %f62, %f122;
	setp.gt.ftz.f32 	%p14, %f62, %f61;
	and.pred  	%p15, %p13, %p14;
	selp.f32 	%f63, %f62, %f61, %p15;
	ld.global.nc.f32 	%f64, [%rd21+12];
	setp.lt.ftz.f32 	%p16, %f64, %f122;
	setp.gt.ftz.f32 	%p17, %f64, %f63;
	and.pred  	%p18, %p16, %p17;
	selp.f32 	%f126, %f64, %f63, %p18;
	add.s32 	%r63, %r63, 4;
	add.s32 	%r62, %r62, -4;
	setp.ne.s32 	%p19, %r62, 0;
	@%p19 bra 	$L__BB3_4;

$L__BB3_5:
	setp.eq.s32 	%p20, %r2, 0;
	@%p20 bra 	$L__BB3_9;

	setp.eq.s32 	%p21, %r2, 1;
	cvt.s64.s32 	%rd22, %r63;
	add.s64 	%rd23, %rd22, %rd2;
	shl.b64 	%rd24, %rd23, 2;
	add.s64 	%rd4, %rd1, %rd24;
	ld.global.nc.f32 	%f65, [%rd4];
	setp.lt.ftz.f32 	%p22, %f65, %f122;
	setp.gt.ftz.f32 	%p23, %f65, %f126;
	and.pred  	%p24, %p22, %p23;
	selp.f32 	%f126, %f65, %f126, %p24;
	@%p21 bra 	$L__BB3_9;

	setp.eq.s32 	%p25, %r2, 2;
	ld.global.nc.f32 	%f66, [%rd4+4];
	setp.lt.ftz.f32 	%p26, %f66, %f122;
	setp.gt.ftz.f32 	%p27, %f66, %f126;
	and.pred  	%p28, %p26, %p27;
	selp.f32 	%f126, %f66, %f126, %p28;
	@%p25 bra 	$L__BB3_9;

	ld.global.nc.f32 	%f67, [%rd4+8];
	setp.lt.ftz.f32 	%p29, %f67, %f122;
	setp.gt.ftz.f32 	%p30, %f67, %f126;
	and.pred  	%p31, %p29, %p30;
	selp.f32 	%f126, %f67, %f126, %p31;

$L__BB3_9:
	add.s32 	%r60, %r60, 1;
	setp.le.ftz.f32 	%p32, %f126, 0fFF7FFFFF;
	@%p32 bra 	$L__BB3_11;

	setp.lt.s32 	%p33, %r60, %r44;
	mov.f32 	%f127, %f126;
	mov.f32 	%f122, %f126;
	@%p33 bra 	$L__BB3_2;

$L__BB3_11:
	@%p4 bra 	$L__BB3_18;

	add.s32 	%r51, %r43, -1;
	and.b32  	%r67, %r43, 3;
	setp.lt.u32 	%p35, %r51, 3;
	mov.f32 	%f148, 0f00000000;
	mov.u32 	%r66, 0;
	@%p35 bra 	$L__BB3_15;

	sub.s32 	%r65, %r43, %r67;

$L__BB3_14:
	cvt.s64.s32 	%rd25, %r66;
	add.s64 	%rd26, %rd25, %rd2;
	shl.b64 	%rd27, %rd26, 2;
	add.s64 	%rd28, %rd1, %rd27;
	ld.global.nc.f32 	%f72, [%rd28];
	setp.ltu.ftz.f32 	%p36, %f72, %f127;
	add.ftz.f32 	%f73, %f148, %f72;
	selp.f32 	%f74, %f148, %f73, %p36;
	ld.global.nc.f32 	%f75, [%rd28+4];
	setp.ltu.ftz.f32 	%p37, %f75, %f127;
	add.ftz.f32 	%f76, %f74, %f75;
	selp.f32 	%f77, %f74, %f76, %p37;
	ld.global.nc.f32 	%f78, [%rd28+8];
	setp.ltu.ftz.f32 	%p38, %f78, %f127;
	add.ftz.f32 	%f79, %f77, %f78;
	selp.f32 	%f80, %f77, %f79, %p38;
	ld.global.nc.f32 	%f81, [%rd28+12];
	setp.ltu.ftz.f32 	%p39, %f81, %f127;
	add.ftz.f32 	%f82, %f80, %f81;
	selp.f32 	%f148, %f80, %f82, %p39;
	add.s32 	%r66, %r66, 4;
	add.s32 	%r65, %r65, -4;
	setp.ne.s32 	%p40, %r65, 0;
	@%p40 bra 	$L__BB3_14;

$L__BB3_15:
	setp.eq.s32 	%p41, %r67, 0;
	@%p41 bra 	$L__BB3_18;

	cvt.s64.s32 	%rd29, %r66;
	add.s64 	%rd30, %rd29, %rd2;
	shl.b64 	%rd31, %rd30, 2;
	add.s64 	%rd57, %rd1, %rd31;

$L__BB3_17:
	.pragma "nounroll";
	ld.global.nc.f32 	%f83, [%rd57];
	setp.ltu.ftz.f32 	%p42, %f83, %f127;
	add.ftz.f32 	%f84, %f148, %f83;
	selp.f32 	%f148, %f148, %f84, %p42;
	add.s64 	%rd57, %rd57, 4;
	add.s32 	%r67, %r67, -1;
	setp.ne.s32 	%p43, %r67, 0;
	@%p43 bra 	$L__BB3_17;

$L__BB3_18:
	setp.leu.ftz.f32 	%p44, %f51, 0f00000000;
	setp.geu.ftz.f32 	%p45, %f51, 0f3F800000;
	or.pred  	%p46, %p45, %p44;
	@%p46 bra 	$L__BB3_41;

	mul.ftz.f32 	%f20, %f148, %f51;
	setp.leu.ftz.f32 	%p47, %f20, 0f00000000;
	mov.f32 	%f148, 0f00000000;
	or.pred  	%p49, %p47, %p4;
	mov.f32 	%f143, %f148;
	@%p49 bra 	$L__BB3_34;

	add.s32 	%r20, %r43, -1;
	and.b32  	%r21, %r43, 3;
	sub.s32 	%r22, %r43, %r21;
	mov.f32 	%f133, 0f7F7FFFFF;
	mov.f32 	%f141, %f148;

$L__BB3_21:
	setp.lt.u32 	%p50, %r20, 3;
	mov.f32 	%f139, 0fFF7FFFFF;
	mov.u32 	%r70, 0;
	@%p50 bra 	$L__BB3_24;

	mov.u32 	%r69, %r22;

$L__BB3_23:
	cvt.s64.s32 	%rd32, %r70;
	add.s64 	%rd33, %rd32, %rd2;
	shl.b64 	%rd34, %rd33, 2;
	add.s64 	%rd35, %rd1, %rd34;
	ld.global.nc.f32 	%f92, [%rd35];
	setp.ge.ftz.f32 	%p51, %f92, %f127;
	setp.lt.ftz.f32 	%p52, %f92, %f133;
	and.pred  	%p53, %p51, %p52;
	setp.gt.ftz.f32 	%p54, %f92, %f139;
	and.pred  	%p55, %p54, %p53;
	selp.f32 	%f93, %f92, %f139, %p55;
	ld.global.nc.f32 	%f94, [%rd35+4];
	setp.ge.ftz.f32 	%p56, %f94, %f127;
	setp.lt.ftz.f32 	%p57, %f94, %f133;
	and.pred  	%p58, %p56, %p57;
	setp.gt.ftz.f32 	%p59, %f94, %f93;
	and.pred  	%p60, %p59, %p58;
	selp.f32 	%f95, %f94, %f93, %p60;
	ld.global.nc.f32 	%f96, [%rd35+8];
	setp.ge.ftz.f32 	%p61, %f96, %f127;
	setp.lt.ftz.f32 	%p62, %f96, %f133;
	and.pred  	%p63, %p61, %p62;
	setp.gt.ftz.f32 	%p64, %f96, %f95;
	and.pred  	%p65, %p64, %p63;
	selp.f32 	%f97, %f96, %f95, %p65;
	ld.global.nc.f32 	%f98, [%rd35+12];
	setp.ge.ftz.f32 	%p66, %f98, %f127;
	setp.lt.ftz.f32 	%p67, %f98, %f133;
	and.pred  	%p68, %p66, %p67;
	setp.gt.ftz.f32 	%p69, %f98, %f97;
	and.pred  	%p70, %p69, %p68;
	selp.f32 	%f139, %f98, %f97, %p70;
	add.s32 	%r70, %r70, 4;
	add.s32 	%r69, %r69, -4;
	setp.ne.s32 	%p71, %r69, 0;
	@%p71 bra 	$L__BB3_23;

$L__BB3_24:
	setp.eq.s32 	%p72, %r21, 0;
	@%p72 bra 	$L__BB3_28;

	setp.eq.s32 	%p73, %r21, 1;
	cvt.s64.s32 	%rd36, %r70;
	add.s64 	%rd37, %rd36, %rd2;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd8, %rd1, %rd38;
	ld.global.nc.f32 	%f99, [%rd8];
	setp.ge.ftz.f32 	%p74, %f99, %f127;
	setp.lt.ftz.f32 	%p75, %f99, %f133;
	and.pred  	%p76, %p74, %p75;
	setp.gt.ftz.f32 	%p77, %f99, %f139;
	and.pred  	%p78, %p77, %p76;
	selp.f32 	%f139, %f99, %f139, %p78;
	@%p73 bra 	$L__BB3_28;

	setp.eq.s32 	%p79, %r21, 2;
	ld.global.nc.f32 	%f100, [%rd8+4];
	setp.ge.ftz.f32 	%p80, %f100, %f127;
	setp.lt.ftz.f32 	%p81, %f100, %f133;
	and.pred  	%p82, %p80, %p81;
	setp.gt.ftz.f32 	%p83, %f100, %f139;
	and.pred  	%p84, %p83, %p82;
	selp.f32 	%f139, %f100, %f139, %p84;
	@%p79 bra 	$L__BB3_28;

	ld.global.nc.f32 	%f101, [%rd8+8];
	setp.ge.ftz.f32 	%p85, %f101, %f127;
	setp.lt.ftz.f32 	%p86, %f101, %f133;
	and.pred  	%p87, %p85, %p86;
	setp.gt.ftz.f32 	%p88, %f101, %f139;
	and.pred  	%p89, %p88, %p87;
	selp.f32 	%f139, %f101, %f139, %p89;

$L__BB3_28:
	setp.le.ftz.f32 	%p90, %f139, 0fFF7FFFFF;
	@%p90 bra 	$L__BB3_34;

	mov.u32 	%r71, 0;

$L__BB3_30:
	cvt.s64.s32 	%rd39, %r71;
	add.s64 	%rd40, %rd39, %rd2;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd42, %rd1, %rd41;
	ld.global.nc.f32 	%f102, [%rd42];
	setp.neu.ftz.f32 	%p91, %f102, %f139;
	setp.ltu.ftz.f32 	%p92, %f102, %f127;
	or.pred  	%p93, %p91, %p92;
	@%p93 bra 	$L__BB3_32;

	add.ftz.f32 	%f141, %f139, %f141;
	setp.ge.ftz.f32 	%p94, %f141, %f20;
	@%p94 bra 	$L__BB3_33;

$L__BB3_32:
	add.s32 	%r71, %r71, 1;
	setp.lt.s32 	%p95, %r71, %r43;
	@%p95 bra 	$L__BB3_30;

$L__BB3_33:
	setp.lt.ftz.f32 	%p96, %f141, %f20;
	mov.f32 	%f133, %f139;
	mov.f32 	%f143, %f139;
	@%p96 bra 	$L__BB3_21;

$L__BB3_34:
	setp.gt.ftz.f32 	%p97, %f143, %f127;
	selp.f32 	%f127, %f143, %f127, %p97;
	@%p4 bra 	$L__BB3_41;

	add.s32 	%r57, %r43, -1;
	and.b32  	%r75, %r43, 3;
	setp.lt.u32 	%p99, %r57, 3;
	mov.f32 	%f148, 0f00000000;
	mov.u32 	%r74, 0;
	@%p99 bra 	$L__BB3_38;

	sub.s32 	%r73, %r43, %r75;

$L__BB3_37:
	cvt.s64.s32 	%rd43, %r74;
	add.s64 	%rd44, %rd43, %rd2;
	shl.b64 	%rd45, %rd44, 2;
	add.s64 	%rd46, %rd1, %rd45;
	ld.global.nc.f32 	%f107, [%rd46];
	setp.ltu.ftz.f32 	%p100, %f107, %f127;
	add.ftz.f32 	%f108, %f148, %f107;
	selp.f32 	%f109, %f148, %f108, %p100;
	ld.global.nc.f32 	%f110, [%rd46+4];
	setp.ltu.ftz.f32 	%p101, %f110, %f127;
	add.ftz.f32 	%f111, %f109, %f110;
	selp.f32 	%f112, %f109, %f111, %p101;
	ld.global.nc.f32 	%f113, [%rd46+8];
	setp.ltu.ftz.f32 	%p102, %f113, %f127;
	add.ftz.f32 	%f114, %f112, %f113;
	selp.f32 	%f115, %f112, %f114, %p102;
	ld.global.nc.f32 	%f116, [%rd46+12];
	setp.ltu.ftz.f32 	%p103, %f116, %f127;
	add.ftz.f32 	%f117, %f115, %f116;
	selp.f32 	%f148, %f115, %f117, %p103;
	add.s32 	%r74, %r74, 4;
	add.s32 	%r73, %r73, -4;
	setp.ne.s32 	%p104, %r73, 0;
	@%p104 bra 	$L__BB3_37;

$L__BB3_38:
	setp.eq.s32 	%p105, %r75, 0;
	@%p105 bra 	$L__BB3_41;

	cvt.s64.s32 	%rd47, %r74;
	add.s64 	%rd48, %rd47, %rd2;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd58, %rd1, %rd49;

$L__BB3_40:
	.pragma "nounroll";
	ld.global.nc.f32 	%f118, [%rd58];
	setp.ltu.ftz.f32 	%p106, %f118, %f127;
	add.ftz.f32 	%f119, %f148, %f118;
	selp.f32 	%f148, %f148, %f119, %p106;
	add.s64 	%rd58, %rd58, 4;
	add.s32 	%r75, %r75, -1;
	setp.ne.s32 	%p107, %r75, 0;
	@%p107 bra 	$L__BB3_40;

$L__BB3_41:
	mul.ftz.f32 	%f46, %f1, %f148;
	add.s32 	%r39, %r43, -1;
	mov.u32 	%r77, %r39;
	@%p4 bra 	$L__BB3_46;

	mov.f32 	%f151, 0f00000000;
	mov.u32 	%r76, 0;

$L__BB3_43:
	cvt.s64.s32 	%rd50, %r76;
	add.s64 	%rd51, %rd50, %rd2;
	shl.b64 	%rd52, %rd51, 2;
	add.s64 	%rd53, %rd1, %rd52;
	ld.global.nc.f32 	%f48, [%rd53];
	setp.ltu.ftz.f32 	%p109, %f48, %f127;
	@%p109 bra 	$L__BB3_45;

	add.ftz.f32 	%f151, %f151, %f48;
	setp.gt.ftz.f32 	%p110, %f151, %f46;
	mov.u32 	%r77, %r76;
	@%p110 bra 	$L__BB3_46;

$L__BB3_45:
	add.s32 	%r76, %r76, 1;
	setp.lt.s32 	%p111, %r76, %r43;
	mov.u32 	%r77, %r39;
	@%p111 bra 	$L__BB3_43;

$L__BB3_46:
	cvta.to.global.u64 	%rd54, %rd12;
	shl.b64 	%rd55, %rd3, 2;
	add.s64 	%rd56, %rd54, %rd55;
	st.global.u32 	[%rd56], %r77;
	ret;

}
	// .globl	argmax_f32
.visible .entry argmax_f32(
	.param .u64 argmax_f32_param_0,
	.param .u64 argmax_f32_param_1,
	.param .u32 argmax_f32_param_2
)
{
	.reg .pred 	%p<46>;
	.reg .f32 	%f<48>;
	.reg .b32 	%r<126>;
	.reg .b64 	%rd<19>;
	// demoted variable
	.shared .align 4 .b8 _ZZ10argmax_f32E9smem_vals[32];
	// demoted variable
	.shared .align 4 .b8 _ZZ10argmax_f32E9smem_idxs[32];

	ld.param.u64 	%rd6, [argmax_f32_param_0];
	ld.param.u64 	%rd7, [argmax_f32_param_1];
	ld.param.u32 	%r27, [argmax_f32_param_2];
	cvta.to.global.u64 	%rd1, %rd7;
	mov.u32 	%r1, %ctaid.x;
	mul.lo.s32 	%r29, %r1, %r27;
	cvt.s64.s32 	%rd2, %r29;
	mov.u32 	%r2, %tid.x;
	setp.ge.s32 	%p1, %r2, %r27;
	mov.u32 	%r124, 0;
	mov.f32 	%f46, 0fFF7FFFFF;
	@%p1 bra 	$L__BB4_7;

	not.b32 	%r32, %r2;
	add.s32 	%r33, %r32, %r27;
	shr.u32 	%r34, %r33, 8;
	add.s32 	%r3, %r34, 1;
	and.b32  	%r123, %r3, 3;
	setp.lt.u32 	%p2, %r33, 768;
	mov.f32 	%f46, 0fFF7FFFFF;
	mov.u32 	%r124, 0;
	mov.u32 	%r119, %r2;
	@%p2 bra 	$L__BB4_4;

	sub.s32 	%r117, %r3, %r123;
	mov.u32 	%r119, %r2;

$L__BB4_3:
	cvt.s64.s32 	%rd8, %r119;
	add.s64 	%rd9, %rd8, %rd2;
	shl.b64 	%rd10, %rd9, 2;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.nc.f32 	%f15, [%rd11];
	setp.gt.ftz.f32 	%p3, %f15, %f46;
	selp.f32 	%f16, %f15, %f46, %p3;
	selp.b32 	%r36, %r119, %r124, %p3;
	ld.global.nc.f32 	%f17, [%rd11+1024];
	setp.gt.ftz.f32 	%p4, %f17, %f16;
	selp.f32 	%f18, %f17, %f16, %p4;
	add.s32 	%r37, %r119, 256;
	selp.b32 	%r38, %r37, %r36, %p4;
	ld.global.nc.f32 	%f19, [%rd11+2048];
	setp.gt.ftz.f32 	%p5, %f19, %f18;
	selp.f32 	%f20, %f19, %f18, %p5;
	add.s32 	%r39, %r119, 512;
	selp.b32 	%r40, %r39, %r38, %p5;
	ld.global.nc.f32 	%f21, [%rd11+3072];
	setp.gt.ftz.f32 	%p6, %f21, %f20;
	selp.f32 	%f46, %f21, %f20, %p6;
	add.s32 	%r41, %r119, 768;
	selp.b32 	%r124, %r41, %r40, %p6;
	add.s32 	%r119, %r119, 1024;
	add.s32 	%r117, %r117, -4;
	setp.ne.s32 	%p7, %r117, 0;
	@%p7 bra 	$L__BB4_3;

$L__BB4_4:
	setp.eq.s32 	%p8, %r123, 0;
	@%p8 bra 	$L__BB4_7;

	cvt.s64.s32 	%rd12, %r119;
	add.s64 	%rd13, %rd12, %rd2;
	shl.b64 	%rd14, %rd13, 2;
	add.s64 	%rd18, %rd1, %rd14;

$L__BB4_6:
	.pragma "nounroll";
	ld.global.nc.f32 	%f22, [%rd18];
	setp.gt.ftz.f32 	%p9, %f22, %f46;
	selp.f32 	%f46, %f22, %f46, %p9;
	selp.b32 	%r124, %r119, %r124, %p9;
	add.s32 	%r119, %r119, 256;
	add.s64 	%rd18, %rd18, 1024;
	add.s32 	%r123, %r123, -1;
	setp.ne.s32 	%p10, %r123, 0;
	@%p10 bra 	$L__BB4_6;

$L__BB4_7:
	mov.b32 	%r42, %f46;
	mov.u32 	%r43, 31;
	mov.u32 	%r44, 16;
	mov.u32 	%r45, -1;
	shfl.sync.bfly.b32 	%r46|%p11, %r42, %r44, %r43, %r45;
	mov.b32 	%f23, %r46;
	shfl.sync.bfly.b32 	%r47|%p12, %r124, %r44, %r43, %r45;
	setp.lt.ftz.f32 	%p13, %f46, %f23;
	selp.f32 	%f24, %f23, %f46, %p13;
	selp.b32 	%r48, %r47, %r124, %p13;
	mov.b32 	%r49, %f24;
	mov.u32 	%r50, 8;
	shfl.sync.bfly.b32 	%r51|%p14, %r49, %r50, %r43, %r45;
	mov.b32 	%f25, %r51;
	shfl.sync.bfly.b32 	%r52|%p15, %r48, %r50, %r43, %r45;
	setp.lt.ftz.f32 	%p16, %f24, %f25;
	selp.f32 	%f26, %f25, %f24, %p16;
	selp.b32 	%r53, %r52, %r48, %p16;
	mov.b32 	%r54, %f26;
	mov.u32 	%r55, 4;
	shfl.sync.bfly.b32 	%r56|%p17, %r54, %r55, %r43, %r45;
	mov.b32 	%f27, %r56;
	shfl.sync.bfly.b32 	%r57|%p18, %r53, %r55, %r43, %r45;
	setp.lt.ftz.f32 	%p19, %f26, %f27;
	selp.f32 	%f28, %f27, %f26, %p19;
	selp.b32 	%r58, %r57, %r53, %p19;
	mov.b32 	%r59, %f28;
	mov.u32 	%r60, 2;
	shfl.sync.bfly.b32 	%r61|%p20, %r59, %r60, %r43, %r45;
	mov.b32 	%f29, %r61;
	shfl.sync.bfly.b32 	%r62|%p21, %r58, %r60, %r43, %r45;
	setp.lt.ftz.f32 	%p22, %f28, %f29;
	selp.f32 	%f30, %f29, %f28, %p22;
	selp.b32 	%r63, %r62, %r58, %p22;
	mov.b32 	%r64, %f30;
	mov.u32 	%r65, 1;
	shfl.sync.bfly.b32 	%r66|%p23, %r64, %r65, %r43, %r45;
	mov.b32 	%f31, %r66;
	shfl.sync.bfly.b32 	%r67|%p24, %r63, %r65, %r43, %r45;
	setp.lt.ftz.f32 	%p25, %f30, %f31;
	selp.f32 	%f8, %f31, %f30, %p25;
	selp.b32 	%r22, %r67, %r63, %p25;
	shr.s32 	%r68, %r2, 31;
	shr.u32 	%r69, %r68, 27;
	add.s32 	%r70, %r2, %r69;
	and.b32  	%r71, %r70, -32;
	sub.s32 	%r23, %r2, %r71;
	setp.ne.s32 	%p26, %r23, 0;
	@%p26 bra 	$L__BB4_9;

	shr.s32 	%r75, %r70, 5;
	shl.b32 	%r76, %r75, 2;
	mov.u32 	%r77, _ZZ10argmax_f32E9smem_vals;
	add.s32 	%r78, %r77, %r76;
	st.shared.f32 	[%r78], %f8;
	mov.u32 	%r79, _ZZ10argmax_f32E9smem_idxs;
	add.s32 	%r80, %r79, %r76;
	st.shared.u32 	[%r80], %r22;

$L__BB4_9:
	bar.sync 	0;
	add.s32 	%r81, %r2, 31;
	setp.gt.u32 	%p27, %r81, 62;
	@%p27 bra 	$L__BB4_16;

	setp.gt.s32 	%p28, %r23, 7;
	mov.f32 	%f47, 0fFF7FFFFF;
	@%p28 bra 	$L__BB4_12;

	shl.b32 	%r82, %r23, 2;
	mov.u32 	%r83, _ZZ10argmax_f32E9smem_vals;
	add.s32 	%r84, %r83, %r82;
	ld.shared.f32 	%f47, [%r84];

$L__BB4_12:
	mov.u32 	%r125, 0;
	@%p28 bra 	$L__BB4_14;

	shl.b32 	%r86, %r23, 2;
	mov.u32 	%r87, _ZZ10argmax_f32E9smem_idxs;
	add.s32 	%r88, %r87, %r86;
	ld.shared.u32 	%r125, [%r88];

$L__BB4_14:
	mov.b32 	%r89, %f47;
	mov.u32 	%r90, 31;
	mov.u32 	%r91, 16;
	mov.u32 	%r92, -1;
	shfl.sync.bfly.b32 	%r93|%p30, %r89, %r91, %r90, %r92;
	mov.b32 	%f33, %r93;
	shfl.sync.bfly.b32 	%r94|%p31, %r125, %r91, %r90, %r92;
	setp.lt.ftz.f32 	%p32, %f47, %f33;
	selp.f32 	%f34, %f33, %f47, %p32;
	selp.b32 	%r95, %r94, %r125, %p32;
	mov.b32 	%r96, %f34;
	mov.u32 	%r97, 8;
	shfl.sync.bfly.b32 	%r98|%p33, %r96, %r97, %r90, %r92;
	mov.b32 	%f35, %r98;
	shfl.sync.bfly.b32 	%r99|%p34, %r95, %r97, %r90, %r92;
	setp.lt.ftz.f32 	%p35, %f34, %f35;
	selp.f32 	%f36, %f35, %f34, %p35;
	selp.b32 	%r100, %r99, %r95, %p35;
	mov.b32 	%r101, %f36;
	mov.u32 	%r102, 4;
	shfl.sync.bfly.b32 	%r103|%p36, %r101, %r102, %r90, %r92;
	mov.b32 	%f37, %r103;
	shfl.sync.bfly.b32 	%r104|%p37, %r100, %r102, %r90, %r92;
	setp.lt.ftz.f32 	%p38, %f36, %f37;
	selp.f32 	%f38, %f37, %f36, %p38;
	selp.b32 	%r105, %r104, %r100, %p38;
	mov.b32 	%r106, %f38;
	mov.u32 	%r107, 2;
	shfl.sync.bfly.b32 	%r108|%p39, %r106, %r107, %r90, %r92;
	mov.b32 	%f39, %r108;
	shfl.sync.bfly.b32 	%r109|%p40, %r105, %r107, %r90, %r92;
	setp.lt.ftz.f32 	%p41, %f38, %f39;
	selp.f32 	%f40, %f39, %f38, %p41;
	selp.b32 	%r110, %r109, %r105, %p41;
	mov.b32 	%r111, %f40;
	mov.u32 	%r112, 1;
	shfl.sync.bfly.b32 	%r113|%p42, %r111, %r112, %r90, %r92;
	mov.b32 	%f41, %r113;
	shfl.sync.bfly.b32 	%r114|%p43, %r110, %r112, %r90, %r92;
	setp.lt.ftz.f32 	%p44, %f40, %f41;
	selp.b32 	%r26, %r114, %r110, %p44;
	@%p26 bra 	$L__BB4_16;

	cvta.to.global.u64 	%rd15, %rd6;
	mul.wide.s32 	%rd16, %r1, 4;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.u32 	[%rd17], %r26;

$L__BB4_16:
	ret;

}
	// .globl	softmax_to_probs_f32
.visible .entry softmax_to_probs_f32(
	.param .u64 softmax_to_probs_f32_param_0,
	.param .u64 softmax_to_probs_f32_param_1,
	.param .u32 softmax_to_probs_f32_param_2
)
{
	.reg .pred 	%p<44>;
	.reg .f32 	%f<127>;
	.reg .b32 	%r<140>;
	.reg .b64 	%rd<61>;
	// demoted variable
	.shared .align 4 .b8 _ZZ20softmax_to_probs_f32E4smem[32];

	ld.param.u64 	%rd29, [softmax_to_probs_f32_param_0];
	ld.param.u64 	%rd28, [softmax_to_probs_f32_param_1];
	ld.param.u32 	%r33, [softmax_to_probs_f32_param_2];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd29;
	mov.u32 	%r138, %tid.x;
	shr.s32 	%r34, %r138, 31;
	shr.u32 	%r35, %r34, 27;
	add.s32 	%r36, %r138, %r35;
	and.b32  	%r37, %r36, -32;
	sub.s32 	%r2, %r138, %r37;
	mov.u32 	%r38, %ctaid.x;
	mul.lo.s32 	%r39, %r38, %r33;
	cvt.s64.s32 	%rd3, %r39;
	setp.ge.s32 	%p1, %r138, %r33;
	mov.f32 	%f119, 0fFF7FFFFF;
	@%p1 bra 	$L__BB5_7;

	not.b32 	%r40, %r138;
	add.s32 	%r3, %r40, %r33;
	shr.u32 	%r41, %r3, 8;
	add.s32 	%r42, %r41, 1;
	and.b32  	%r129, %r42, 3;
	setp.eq.s32 	%p2, %r129, 0;
	mov.f32 	%f119, 0fFF7FFFFF;
	mov.u32 	%r130, %r138;
	@%p2 bra 	$L__BB5_4;

	cvt.s64.s32 	%rd30, %r138;
	add.s64 	%rd31, %rd30, %rd3;
	shl.b64 	%rd32, %rd31, 2;
	add.s64 	%rd53, %rd1, %rd32;
	mov.u32 	%r130, %r138;

$L__BB5_3:
	.pragma "nounroll";
	ld.global.nc.f32 	%f29, [%rd53];
	max.ftz.f32 	%f119, %f119, %f29;
	add.s32 	%r130, %r130, 256;
	add.s64 	%rd53, %rd53, 1024;
	add.s32 	%r129, %r129, -1;
	setp.ne.s32 	%p3, %r129, 0;
	@%p3 bra 	$L__BB5_3;

$L__BB5_4:
	setp.lt.u32 	%p4, %r3, 768;
	@%p4 bra 	$L__BB5_7;

	cvt.s64.s32 	%rd33, %r130;
	add.s64 	%rd34, %rd33, %rd3;
	shl.b64 	%rd35, %rd34, 2;
	add.s64 	%rd36, %rd1, %rd35;
	add.s64 	%rd54, %rd36, 2048;

$L__BB5_6:
	ld.global.nc.f32 	%f30, [%rd54+-2048];
	max.ftz.f32 	%f31, %f119, %f30;
	ld.global.nc.f32 	%f32, [%rd54+-1024];
	max.ftz.f32 	%f33, %f31, %f32;
	ld.global.nc.f32 	%f34, [%rd54];
	max.ftz.f32 	%f35, %f33, %f34;
	ld.global.nc.f32 	%f36, [%rd54+1024];
	max.ftz.f32 	%f119, %f35, %f36;
	add.s64 	%rd54, %rd54, 4096;
	add.s32 	%r130, %r130, 1024;
	setp.lt.s32 	%p5, %r130, %r33;
	@%p5 bra 	$L__BB5_6;

$L__BB5_7:
	mov.b32 	%r43, %f119;
	mov.u32 	%r44, 31;
	mov.u32 	%r45, 16;
	mov.u32 	%r46, -1;
	shfl.sync.bfly.b32 	%r47|%p6, %r43, %r45, %r44, %r46;
	mov.b32 	%f37, %r47;
	max.ftz.f32 	%f38, %f119, %f37;
	mov.b32 	%r48, %f38;
	mov.u32 	%r49, 8;
	shfl.sync.bfly.b32 	%r50|%p7, %r48, %r49, %r44, %r46;
	mov.b32 	%f39, %r50;
	max.ftz.f32 	%f40, %f38, %f39;
	mov.b32 	%r51, %f40;
	mov.u32 	%r52, 4;
	shfl.sync.bfly.b32 	%r53|%p8, %r51, %r52, %r44, %r46;
	mov.b32 	%f41, %r53;
	max.ftz.f32 	%f42, %f40, %f41;
	mov.b32 	%r54, %f42;
	mov.u32 	%r55, 2;
	shfl.sync.bfly.b32 	%r56|%p9, %r54, %r55, %r44, %r46;
	mov.b32 	%f43, %r56;
	max.ftz.f32 	%f44, %f42, %f43;
	mov.b32 	%r57, %f44;
	mov.u32 	%r58, 1;
	shfl.sync.bfly.b32 	%r59|%p10, %r57, %r58, %r44, %r46;
	mov.b32 	%f45, %r59;
	max.ftz.f32 	%f8, %f44, %f45;
	shr.s32 	%r63, %r36, 5;
	shl.b32 	%r64, %r63, 2;
	mov.u32 	%r65, _ZZ20softmax_to_probs_f32E4smem;
	add.s32 	%r12, %r65, %r64;
	setp.ne.s32 	%p11, %r2, 0;
	@%p11 bra 	$L__BB5_9;

	st.shared.f32 	[%r12], %f8;

$L__BB5_9:
	bar.sync 	0;
	add.s32 	%r13, %r138, 31;
	setp.gt.u32 	%p12, %r13, 62;
	shl.b32 	%r66, %r2, 2;
	add.s32 	%r14, %r65, %r66;
	@%p12 bra 	$L__BB5_14;

	setp.gt.s32 	%p13, %r2, 7;
	mov.f32 	%f120, 0fFF7FFFFF;
	@%p13 bra 	$L__BB5_12;

	ld.shared.f32 	%f120, [%r14];

$L__BB5_12:
	mov.b32 	%r68, %f120;
	mov.u32 	%r69, 31;
	mov.u32 	%r70, 16;
	mov.u32 	%r71, -1;
	shfl.sync.bfly.b32 	%r72|%p14, %r68, %r70, %r69, %r71;
	mov.b32 	%f47, %r72;
	max.ftz.f32 	%f48, %f120, %f47;
	mov.b32 	%r73, %f48;
	mov.u32 	%r74, 8;
	shfl.sync.bfly.b32 	%r75|%p15, %r73, %r74, %r69, %r71;
	mov.b32 	%f49, %r75;
	max.ftz.f32 	%f50, %f48, %f49;
	mov.b32 	%r76, %f50;
	mov.u32 	%r77, 4;
	shfl.sync.bfly.b32 	%r78|%p16, %r76, %r77, %r69, %r71;
	mov.b32 	%f51, %r78;
	max.ftz.f32 	%f52, %f50, %f51;
	mov.b32 	%r79, %f52;
	mov.u32 	%r80, 2;
	shfl.sync.bfly.b32 	%r81|%p17, %r79, %r80, %r69, %r71;
	mov.b32 	%f53, %r81;
	max.ftz.f32 	%f54, %f52, %f53;
	mov.b32 	%r82, %f54;
	mov.u32 	%r83, 1;
	shfl.sync.bfly.b32 	%r84|%p18, %r82, %r83, %r69, %r71;
	mov.b32 	%f55, %r84;
	max.ftz.f32 	%f11, %f54, %f55;
	@%p11 bra 	$L__BB5_14;

	st.shared.f32 	[_ZZ20softmax_to_probs_f32E4smem], %f11;

$L__BB5_14:
	bar.sync 	0;
	mov.f32 	%f125, 0f00000000;
	ld.shared.f32 	%f12, [_ZZ20softmax_to_probs_f32E4smem];
	@%p1 bra 	$L__BB5_21;

	not.b32 	%r85, %r138;
	add.s32 	%r15, %r85, %r33;
	shr.u32 	%r86, %r15, 8;
	add.s32 	%r87, %r86, 1;
	and.b32  	%r133, %r87, 3;
	setp.eq.s32 	%p21, %r133, 0;
	mov.f32 	%f125, 0f00000000;
	mov.u32 	%r134, %r138;
	@%p21 bra 	$L__BB5_18;

	cvt.s64.s32 	%rd37, %r138;
	add.s64 	%rd38, %rd37, %rd3;
	shl.b64 	%rd39, %rd38, 2;
	add.s64 	%rd56, %rd2, %rd39;
	add.s64 	%rd55, %rd1, %rd39;
	mov.u32 	%r134, %r138;

$L__BB5_17:
	.pragma "nounroll";
	ld.global.nc.f32 	%f60, [%rd55];
	sub.ftz.f32 	%f61, %f60, %f12;
	mul.ftz.f32 	%f62, %f61, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f63, %f62;
	st.global.f32 	[%rd56], %f63;
	add.ftz.f32 	%f125, %f125, %f63;
	add.s32 	%r134, %r134, 256;
	add.s64 	%rd56, %rd56, 1024;
	add.s64 	%rd55, %rd55, 1024;
	add.s32 	%r133, %r133, -1;
	setp.ne.s32 	%p22, %r133, 0;
	@%p22 bra 	$L__BB5_17;

$L__BB5_18:
	setp.lt.u32 	%p23, %r15, 768;
	@%p23 bra 	$L__BB5_21;

	cvt.s64.s32 	%rd41, %r134;
	add.s64 	%rd42, %rd41, %rd3;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd44, %rd43, 2048;
	add.s64 	%rd58, %rd2, %rd44;
	add.s64 	%rd57, %rd1, %rd44;

$L__BB5_20:
	ld.global.nc.f32 	%f64, [%rd57+-2048];
	sub.ftz.f32 	%f65, %f64, %f12;
	mul.ftz.f32 	%f66, %f65, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f67, %f66;
	st.global.f32 	[%rd58+-2048], %f67;
	add.ftz.f32 	%f68, %f125, %f67;
	ld.global.nc.f32 	%f69, [%rd57+-1024];
	sub.ftz.f32 	%f70, %f69, %f12;
	mul.ftz.f32 	%f71, %f70, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f72, %f71;
	st.global.f32 	[%rd58+-1024], %f72;
	add.ftz.f32 	%f73, %f68, %f72;
	ld.global.nc.f32 	%f74, [%rd57];
	sub.ftz.f32 	%f75, %f74, %f12;
	mul.ftz.f32 	%f76, %f75, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f77, %f76;
	st.global.f32 	[%rd58], %f77;
	add.ftz.f32 	%f78, %f73, %f77;
	ld.global.nc.f32 	%f79, [%rd57+1024];
	sub.ftz.f32 	%f80, %f79, %f12;
	mul.ftz.f32 	%f81, %f80, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f82, %f81;
	st.global.f32 	[%rd58+1024], %f82;
	add.ftz.f32 	%f125, %f78, %f82;
	add.s64 	%rd58, %rd58, 4096;
	add.s64 	%rd57, %rd57, 4096;
	add.s32 	%r134, %r134, 1024;
	setp.lt.s32 	%p24, %r134, %r33;
	@%p24 bra 	$L__BB5_20;

$L__BB5_21:
	mov.b32 	%r88, %f125;
	mov.u32 	%r89, 31;
	mov.u32 	%r90, 16;
	mov.u32 	%r91, -1;
	shfl.sync.bfly.b32 	%r92|%p25, %r88, %r90, %r89, %r91;
	mov.b32 	%f83, %r92;
	add.ftz.f32 	%f84, %f125, %f83;
	mov.b32 	%r93, %f84;
	mov.u32 	%r94, 8;
	shfl.sync.bfly.b32 	%r95|%p26, %r93, %r94, %r89, %r91;
	mov.b32 	%f85, %r95;
	add.ftz.f32 	%f86, %f84, %f85;
	mov.b32 	%r96, %f86;
	mov.u32 	%r97, 4;
	shfl.sync.bfly.b32 	%r98|%p27, %r96, %r97, %r89, %r91;
	mov.b32 	%f87, %r98;
	add.ftz.f32 	%f88, %f86, %f87;
	mov.b32 	%r99, %f88;
	mov.u32 	%r100, 2;
	shfl.sync.bfly.b32 	%r101|%p28, %r99, %r100, %r89, %r91;
	mov.b32 	%f89, %r101;
	add.ftz.f32 	%f90, %f88, %f89;
	mov.b32 	%r102, %f90;
	mov.u32 	%r103, 1;
	shfl.sync.bfly.b32 	%r104|%p29, %r102, %r103, %r89, %r91;
	mov.b32 	%f91, %r104;
	add.ftz.f32 	%f20, %f90, %f91;
	@%p11 bra 	$L__BB5_23;

	st.shared.f32 	[%r12], %f20;

$L__BB5_23:
	bar.sync 	0;
	@%p12 bra 	$L__BB5_28;

	setp.gt.s32 	%p32, %r2, 7;
	mov.f32 	%f126, 0f00000000;
	@%p32 bra 	$L__BB5_26;

	ld.shared.f32 	%f126, [%r14];

$L__BB5_26:
	mov.b32 	%r108, %f126;
	mov.u32 	%r109, 31;
	mov.u32 	%r110, 16;
	mov.u32 	%r111, -1;
	shfl.sync.bfly.b32 	%r112|%p33, %r108, %r110, %r109, %r111;
	mov.b32 	%f93, %r112;
	add.ftz.f32 	%f94, %f126, %f93;
	mov.b32 	%r113, %f94;
	mov.u32 	%r114, 8;
	shfl.sync.bfly.b32 	%r115|%p34, %r113, %r114, %r109, %r111;
	mov.b32 	%f95, %r115;
	add.ftz.f32 	%f96, %f94, %f95;
	mov.b32 	%r116, %f96;
	mov.u32 	%r117, 4;
	shfl.sync.bfly.b32 	%r118|%p35, %r116, %r117, %r109, %r111;
	mov.b32 	%f97, %r118;
	add.ftz.f32 	%f98, %f96, %f97;
	mov.b32 	%r119, %f98;
	mov.u32 	%r120, 2;
	shfl.sync.bfly.b32 	%r121|%p36, %r119, %r120, %r109, %r111;
	mov.b32 	%f99, %r121;
	add.ftz.f32 	%f100, %f98, %f99;
	mov.b32 	%r122, %f100;
	mov.u32 	%r123, 1;
	shfl.sync.bfly.b32 	%r124|%p37, %r122, %r123, %r109, %r111;
	mov.b32 	%f101, %r124;
	add.ftz.f32 	%f23, %f100, %f101;
	@%p11 bra 	$L__BB5_28;

	st.shared.f32 	[_ZZ20softmax_to_probs_f32E4smem], %f23;

$L__BB5_28:
	bar.sync 	0;
	ld.shared.f32 	%f102, [_ZZ20softmax_to_probs_f32E4smem];
	add.ftz.f32 	%f103, %f102, 0f358637BD;
	mov.f32 	%f104, 0f3F800000;
	div.approx.ftz.f32 	%f24, %f104, %f103;
	@%p1 bra 	$L__BB5_35;

	not.b32 	%r125, %r138;
	add.s32 	%r24, %r125, %r33;
	shr.u32 	%r126, %r24, 8;
	add.s32 	%r127, %r126, 1;
	and.b32  	%r137, %r127, 3;
	setp.eq.s32 	%p40, %r137, 0;
	@%p40 bra 	$L__BB5_32;

	cvt.s64.s32 	%rd46, %r138;
	add.s64 	%rd47, %rd46, %rd3;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd59, %rd2, %rd48;

$L__BB5_31:
	.pragma "nounroll";
	ld.global.f32 	%f105, [%rd59];
	mul.ftz.f32 	%f106, %f24, %f105;
	st.global.f32 	[%rd59], %f106;
	add.s32 	%r138, %r138, 256;
	add.s64 	%rd59, %rd59, 1024;
	add.s32 	%r137, %r137, -1;
	setp.ne.s32 	%p41, %r137, 0;
	@%p41 bra 	$L__BB5_31;

$L__BB5_32:
	setp.lt.u32 	%p42, %r24, 768;
	@%p42 bra 	$L__BB5_35;

	cvt.s64.s32 	%rd49, %r138;
	add.s64 	%rd50, %rd49, %rd3;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd60, %rd52, 2048;

$L__BB5_34:
	ld.global.f32 	%f107, [%rd60+-2048];
	mul.ftz.f32 	%f108, %f24, %f107;
	st.global.f32 	[%rd60+-2048], %f108;
	ld.global.f32 	%f109, [%rd60+-1024];
	mul.ftz.f32 	%f110, %f24, %f109;
	st.global.f32 	[%rd60+-1024], %f110;
	ld.global.f32 	%f111, [%rd60];
	mul.ftz.f32 	%f112, %f24, %f111;
	st.global.f32 	[%rd60], %f112;
	ld.global.f32 	%f113, [%rd60+1024];
	mul.ftz.f32 	%f114, %f24, %f113;
	st.global.f32 	[%rd60+1024], %f114;
	add.s64 	%rd60, %rd60, 4096;
	add.s32 	%r138, %r138, 1024;
	setp.lt.s32 	%p43, %r138, %r33;
	@%p43 bra 	$L__BB5_34;

$L__BB5_35:
	ret;

}
	// .globl	top_k_top_p_sample_per_seq
.visible .entry top_k_top_p_sample_per_seq(
	.param .u64 top_k_top_p_sample_per_seq_param_0,
	.param .u64 top_k_top_p_sample_per_seq_param_1,
	.param .u64 top_k_top_p_sample_per_seq_param_2,
	.param .u64 top_k_top_p_sample_per_seq_param_3,
	.param .u64 top_k_top_p_sample_per_seq_param_4,
	.param .u32 top_k_top_p_sample_per_seq_param_5
)
{
	.reg .pred 	%p<112>;
	.reg .f32 	%f<152>;
	.reg .b32 	%r<78>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd12, [top_k_top_p_sample_per_seq_param_0];
	ld.param.u64 	%rd13, [top_k_top_p_sample_per_seq_param_1];
	ld.param.u64 	%rd14, [top_k_top_p_sample_per_seq_param_2];
	ld.param.u64 	%rd15, [top_k_top_p_sample_per_seq_param_3];
	ld.param.u64 	%rd16, [top_k_top_p_sample_per_seq_param_4];
	ld.param.u32 	%r44, [top_k_top_p_sample_per_seq_param_5];
	cvta.to.global.u64 	%rd1, %rd13;
	mov.u32 	%r45, %ctaid.x;
	mul.lo.s32 	%r46, %r45, %r44;
	cvt.s64.s32 	%rd2, %r46;
	cvt.s64.s32 	%rd3, %r45;
	cvta.to.global.u64 	%rd17, %rd14;
	mul.wide.s32 	%rd18, %r45, 4;
	add.s64 	%rd19, %rd17, %rd18;
	ld.global.nc.f32 	%f1, [%rd19];
	cvta.to.global.u64 	%rd20, %rd15;
	add.s64 	%rd21, %rd20, %rd18;
	cvta.to.global.u64 	%rd22, %rd16;
	add.s64 	%rd23, %rd22, %rd18;
	ld.global.nc.f32 	%f2, [%rd23];
	ld.global.nc.u32 	%r1, [%rd21];
	setp.lt.s32 	%p1, %r1, 1;
	setp.ge.s32 	%p2, %r1, %r44;
	or.pred  	%p3, %p1, %p2;
	setp.lt.s32 	%p4, %r44, 1;
	mov.f32 	%f148, 0f00000000;
	or.pred  	%p5, %p3, %p4;
	mov.f32 	%f127, %f148;
	@%p5 bra 	$L__BB6_11;

	add.s32 	%r2, %r44, -1;
	and.b32  	%r3, %r44, 3;
	sub.s32 	%r4, %r44, %r3;
	mov.f32 	%f122, 0f7F7FFFFF;
	mov.u32 	%r47, 0;
	mov.u32 	%r60, %r47;

$L__BB6_2:
	setp.lt.u32 	%p6, %r2, 3;
	mov.f32 	%f126, 0fFF7FFFFF;
	mov.u32 	%r63, %r47;
	@%p6 bra 	$L__BB6_5;

	mov.u32 	%r63, 0;
	mov.u32 	%r62, %r4;

$L__BB6_4:
	cvt.s64.s32 	%rd24, %r63;
	add.s64 	%rd25, %rd24, %rd2;
	shl.b64 	%rd26, %rd25, 2;
	add.s64 	%rd27, %rd1, %rd26;
	ld.global.nc.f32 	%f58, [%rd27];
	setp.lt.ftz.f32 	%p7, %f58, %f122;
	setp.gt.ftz.f32 	%p8, %f58, %f126;
	and.pred  	%p9, %p7, %p8;
	selp.f32 	%f59, %f58, %f126, %p9;
	ld.global.nc.f32 	%f60, [%rd27+4];
	setp.lt.ftz.f32 	%p10, %f60, %f122;
	setp.gt.ftz.f32 	%p11, %f60, %f59;
	and.pred  	%p12, %p10, %p11;
	selp.f32 	%f61, %f60, %f59, %p12;
	ld.global.nc.f32 	%f62, [%rd27+8];
	setp.lt.ftz.f32 	%p13, %f62, %f122;
	setp.gt.ftz.f32 	%p14, %f62, %f61;
	and.pred  	%p15, %p13, %p14;
	selp.f32 	%f63, %f62, %f61, %p15;
	ld.global.nc.f32 	%f64, [%rd27+12];
	setp.lt.ftz.f32 	%p16, %f64, %f122;
	setp.gt.ftz.f32 	%p17, %f64, %f63;
	and.pred  	%p18, %p16, %p17;
	selp.f32 	%f126, %f64, %f63, %p18;
	add.s32 	%r63, %r63, 4;
	add.s32 	%r62, %r62, -4;
	setp.ne.s32 	%p19, %r62, 0;
	@%p19 bra 	$L__BB6_4;

$L__BB6_5:
	setp.eq.s32 	%p20, %r3, 0;
	@%p20 bra 	$L__BB6_9;

	setp.eq.s32 	%p21, %r3, 1;
	cvt.s64.s32 	%rd28, %r63;
	add.s64 	%rd29, %rd28, %rd2;
	shl.b64 	%rd30, %rd29, 2;
	add.s64 	%rd4, %rd1, %rd30;
	ld.global.nc.f32 	%f65, [%rd4];
	setp.lt.ftz.f32 	%p22, %f65, %f122;
	setp.gt.ftz.f32 	%p23, %f65, %f126;
	and.pred  	%p24, %p22, %p23;
	selp.f32 	%f126, %f65, %f126, %p24;
	@%p21 bra 	$L__BB6_9;

	setp.eq.s32 	%p25, %r3, 2;
	ld.global.nc.f32 	%f66, [%rd4+4];
	setp.lt.ftz.f32 	%p26, %f66, %f122;
	setp.gt.ftz.f32 	%p27, %f66, %f126;
	and.pred  	%p28, %p26, %p27;
	selp.f32 	%f126, %f66, %f126, %p28;
	@%p25 bra 	$L__BB6_9;

	ld.global.nc.f32 	%f67, [%rd4+8];
	setp.lt.ftz.f32 	%p29, %f67, %f122;
	setp.gt.ftz.f32 	%p30, %f67, %f126;
	and.pred  	%p31, %p29, %p30;
	selp.f32 	%f126, %f67, %f126, %p31;

$L__BB6_9:
	add.s32 	%r60, %r60, 1;
	setp.le.ftz.f32 	%p32, %f126, 0fFF7FFFFF;
	@%p32 bra 	$L__BB6_11;

	setp.lt.s32 	%p33, %r60, %r1;
	mov.f32 	%f127, %f126;
	mov.f32 	%f122, %f126;
	@%p33 bra 	$L__BB6_2;

$L__BB6_11:
	@%p4 bra 	$L__BB6_18;

	add.s32 	%r51, %r44, -1;
	and.b32  	%r67, %r44, 3;
	setp.lt.u32 	%p35, %r51, 3;
	mov.f32 	%f148, 0f00000000;
	mov.u32 	%r66, 0;
	@%p35 bra 	$L__BB6_15;

	sub.s32 	%r65, %r44, %r67;

$L__BB6_14:
	cvt.s64.s32 	%rd31, %r66;
	add.s64 	%rd32, %rd31, %rd2;
	shl.b64 	%rd33, %rd32, 2;
	add.s64 	%rd34, %rd1, %rd33;
	ld.global.nc.f32 	%f72, [%rd34];
	setp.ltu.ftz.f32 	%p36, %f72, %f127;
	add.ftz.f32 	%f73, %f148, %f72;
	selp.f32 	%f74, %f148, %f73, %p36;
	ld.global.nc.f32 	%f75, [%rd34+4];
	setp.ltu.ftz.f32 	%p37, %f75, %f127;
	add.ftz.f32 	%f76, %f74, %f75;
	selp.f32 	%f77, %f74, %f76, %p37;
	ld.global.nc.f32 	%f78, [%rd34+8];
	setp.ltu.ftz.f32 	%p38, %f78, %f127;
	add.ftz.f32 	%f79, %f77, %f78;
	selp.f32 	%f80, %f77, %f79, %p38;
	ld.global.nc.f32 	%f81, [%rd34+12];
	setp.ltu.ftz.f32 	%p39, %f81, %f127;
	add.ftz.f32 	%f82, %f80, %f81;
	selp.f32 	%f148, %f80, %f82, %p39;
	add.s32 	%r66, %r66, 4;
	add.s32 	%r65, %r65, -4;
	setp.ne.s32 	%p40, %r65, 0;
	@%p40 bra 	$L__BB6_14;

$L__BB6_15:
	setp.eq.s32 	%p41, %r67, 0;
	@%p41 bra 	$L__BB6_18;

	cvt.s64.s32 	%rd35, %r66;
	add.s64 	%rd36, %rd35, %rd2;
	shl.b64 	%rd37, %rd36, 2;
	add.s64 	%rd63, %rd1, %rd37;

$L__BB6_17:
	.pragma "nounroll";
	ld.global.nc.f32 	%f83, [%rd63];
	setp.ltu.ftz.f32 	%p42, %f83, %f127;
	add.ftz.f32 	%f84, %f148, %f83;
	selp.f32 	%f148, %f148, %f84, %p42;
	add.s64 	%rd63, %rd63, 4;
	add.s32 	%r67, %r67, -1;
	setp.ne.s32 	%p43, %r67, 0;
	@%p43 bra 	$L__BB6_17;

$L__BB6_18:
	setp.leu.ftz.f32 	%p44, %f2, 0f00000000;
	setp.geu.ftz.f32 	%p45, %f2, 0f3F800000;
	or.pred  	%p46, %p45, %p44;
	@%p46 bra 	$L__BB6_41;

	mul.ftz.f32 	%f21, %f2, %f148;
	setp.leu.ftz.f32 	%p47, %f21, 0f00000000;
	mov.f32 	%f148, 0f00000000;
	or.pred  	%p49, %p47, %p4;
	mov.f32 	%f143, %f148;
	@%p49 bra 	$L__BB6_34;

	add.s32 	%r21, %r44, -1;
	and.b32  	%r22, %r44, 3;
	sub.s32 	%r23, %r44, %r22;
	mov.f32 	%f133, 0f7F7FFFFF;
	mov.f32 	%f141, %f148;

$L__BB6_21:
	setp.lt.u32 	%p50, %r21, 3;
	mov.f32 	%f139, 0fFF7FFFFF;
	mov.u32 	%r70, 0;
	@%p50 bra 	$L__BB6_24;

	mov.u32 	%r69, %r23;

$L__BB6_23:
	cvt.s64.s32 	%rd38, %r70;
	add.s64 	%rd39, %rd38, %rd2;
	shl.b64 	%rd40, %rd39, 2;
	add.s64 	%rd41, %rd1, %rd40;
	ld.global.nc.f32 	%f92, [%rd41];
	setp.ge.ftz.f32 	%p51, %f92, %f127;
	setp.lt.ftz.f32 	%p52, %f92, %f133;
	and.pred  	%p53, %p51, %p52;
	setp.gt.ftz.f32 	%p54, %f92, %f139;
	and.pred  	%p55, %p54, %p53;
	selp.f32 	%f93, %f92, %f139, %p55;
	ld.global.nc.f32 	%f94, [%rd41+4];
	setp.ge.ftz.f32 	%p56, %f94, %f127;
	setp.lt.ftz.f32 	%p57, %f94, %f133;
	and.pred  	%p58, %p56, %p57;
	setp.gt.ftz.f32 	%p59, %f94, %f93;
	and.pred  	%p60, %p59, %p58;
	selp.f32 	%f95, %f94, %f93, %p60;
	ld.global.nc.f32 	%f96, [%rd41+8];
	setp.ge.ftz.f32 	%p61, %f96, %f127;
	setp.lt.ftz.f32 	%p62, %f96, %f133;
	and.pred  	%p63, %p61, %p62;
	setp.gt.ftz.f32 	%p64, %f96, %f95;
	and.pred  	%p65, %p64, %p63;
	selp.f32 	%f97, %f96, %f95, %p65;
	ld.global.nc.f32 	%f98, [%rd41+12];
	setp.ge.ftz.f32 	%p66, %f98, %f127;
	setp.lt.ftz.f32 	%p67, %f98, %f133;
	and.pred  	%p68, %p66, %p67;
	setp.gt.ftz.f32 	%p69, %f98, %f97;
	and.pred  	%p70, %p69, %p68;
	selp.f32 	%f139, %f98, %f97, %p70;
	add.s32 	%r70, %r70, 4;
	add.s32 	%r69, %r69, -4;
	setp.ne.s32 	%p71, %r69, 0;
	@%p71 bra 	$L__BB6_23;

$L__BB6_24:
	setp.eq.s32 	%p72, %r22, 0;
	@%p72 bra 	$L__BB6_28;

	setp.eq.s32 	%p73, %r22, 1;
	cvt.s64.s32 	%rd42, %r70;
	add.s64 	%rd43, %rd42, %rd2;
	shl.b64 	%rd44, %rd43, 2;
	add.s64 	%rd8, %rd1, %rd44;
	ld.global.nc.f32 	%f99, [%rd8];
	setp.ge.ftz.f32 	%p74, %f99, %f127;
	setp.lt.ftz.f32 	%p75, %f99, %f133;
	and.pred  	%p76, %p74, %p75;
	setp.gt.ftz.f32 	%p77, %f99, %f139;
	and.pred  	%p78, %p77, %p76;
	selp.f32 	%f139, %f99, %f139, %p78;
	@%p73 bra 	$L__BB6_28;

	setp.eq.s32 	%p79, %r22, 2;
	ld.global.nc.f32 	%f100, [%rd8+4];
	setp.ge.ftz.f32 	%p80, %f100, %f127;
	setp.lt.ftz.f32 	%p81, %f100, %f133;
	and.pred  	%p82, %p80, %p81;
	setp.gt.ftz.f32 	%p83, %f100, %f139;
	and.pred  	%p84, %p83, %p82;
	selp.f32 	%f139, %f100, %f139, %p84;
	@%p79 bra 	$L__BB6_28;

	ld.global.nc.f32 	%f101, [%rd8+8];
	setp.ge.ftz.f32 	%p85, %f101, %f127;
	setp.lt.ftz.f32 	%p86, %f101, %f133;
	and.pred  	%p87, %p85, %p86;
	setp.gt.ftz.f32 	%p88, %f101, %f139;
	and.pred  	%p89, %p88, %p87;
	selp.f32 	%f139, %f101, %f139, %p89;

$L__BB6_28:
	setp.le.ftz.f32 	%p90, %f139, 0fFF7FFFFF;
	@%p90 bra 	$L__BB6_34;

	mov.u32 	%r71, 0;

$L__BB6_30:
	cvt.s64.s32 	%rd45, %r71;
	add.s64 	%rd46, %rd45, %rd2;
	shl.b64 	%rd47, %rd46, 2;
	add.s64 	%rd48, %rd1, %rd47;
	ld.global.nc.f32 	%f102, [%rd48];
	setp.neu.ftz.f32 	%p91, %f102, %f139;
	setp.ltu.ftz.f32 	%p92, %f102, %f127;
	or.pred  	%p93, %p91, %p92;
	@%p93 bra 	$L__BB6_32;

	add.ftz.f32 	%f141, %f139, %f141;
	setp.ge.ftz.f32 	%p94, %f141, %f21;
	@%p94 bra 	$L__BB6_33;

$L__BB6_32:
	add.s32 	%r71, %r71, 1;
	setp.lt.s32 	%p95, %r71, %r44;
	@%p95 bra 	$L__BB6_30;

$L__BB6_33:
	setp.lt.ftz.f32 	%p96, %f141, %f21;
	mov.f32 	%f133, %f139;
	mov.f32 	%f143, %f139;
	@%p96 bra 	$L__BB6_21;

$L__BB6_34:
	setp.gt.ftz.f32 	%p97, %f143, %f127;
	selp.f32 	%f127, %f143, %f127, %p97;
	@%p4 bra 	$L__BB6_41;

	add.s32 	%r57, %r44, -1;
	and.b32  	%r75, %r44, 3;
	setp.lt.u32 	%p99, %r57, 3;
	mov.f32 	%f148, 0f00000000;
	mov.u32 	%r74, 0;
	@%p99 bra 	$L__BB6_38;

	sub.s32 	%r73, %r44, %r75;

$L__BB6_37:
	cvt.s64.s32 	%rd49, %r74;
	add.s64 	%rd50, %rd49, %rd2;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd1, %rd51;
	ld.global.nc.f32 	%f107, [%rd52];
	setp.ltu.ftz.f32 	%p100, %f107, %f127;
	add.ftz.f32 	%f108, %f148, %f107;
	selp.f32 	%f109, %f148, %f108, %p100;
	ld.global.nc.f32 	%f110, [%rd52+4];
	setp.ltu.ftz.f32 	%p101, %f110, %f127;
	add.ftz.f32 	%f111, %f109, %f110;
	selp.f32 	%f112, %f109, %f111, %p101;
	ld.global.nc.f32 	%f113, [%rd52+8];
	setp.ltu.ftz.f32 	%p102, %f113, %f127;
	add.ftz.f32 	%f114, %f112, %f113;
	selp.f32 	%f115, %f112, %f114, %p102;
	ld.global.nc.f32 	%f116, [%rd52+12];
	setp.ltu.ftz.f32 	%p103, %f116, %f127;
	add.ftz.f32 	%f117, %f115, %f116;
	selp.f32 	%f148, %f115, %f117, %p103;
	add.s32 	%r74, %r74, 4;
	add.s32 	%r73, %r73, -4;
	setp.ne.s32 	%p104, %r73, 0;
	@%p104 bra 	$L__BB6_37;

$L__BB6_38:
	setp.eq.s32 	%p105, %r75, 0;
	@%p105 bra 	$L__BB6_41;

	cvt.s64.s32 	%rd53, %r74;
	add.s64 	%rd54, %rd53, %rd2;
	shl.b64 	%rd55, %rd54, 2;
	add.s64 	%rd64, %rd1, %rd55;

$L__BB6_40:
	.pragma "nounroll";
	ld.global.nc.f32 	%f118, [%rd64];
	setp.ltu.ftz.f32 	%p106, %f118, %f127;
	add.ftz.f32 	%f119, %f148, %f118;
	selp.f32 	%f148, %f148, %f119, %p106;
	add.s64 	%rd64, %rd64, 4;
	add.s32 	%r75, %r75, -1;
	setp.ne.s32 	%p107, %r75, 0;
	@%p107 bra 	$L__BB6_40;

$L__BB6_41:
	mul.ftz.f32 	%f47, %f1, %f148;
	add.s32 	%r40, %r44, -1;
	mov.u32 	%r77, %r40;
	@%p4 bra 	$L__BB6_46;

	mov.f32 	%f151, 0f00000000;
	mov.u32 	%r76, 0;

$L__BB6_43:
	cvt.s64.s32 	%rd56, %r76;
	add.s64 	%rd57, %rd56, %rd2;
	shl.b64 	%rd58, %rd57, 2;
	add.s64 	%rd59, %rd1, %rd58;
	ld.global.nc.f32 	%f49, [%rd59];
	setp.ltu.ftz.f32 	%p109, %f49, %f127;
	@%p109 bra 	$L__BB6_45;

	add.ftz.f32 	%f151, %f151, %f49;
	setp.gt.ftz.f32 	%p110, %f151, %f47;
	mov.u32 	%r77, %r76;
	@%p110 bra 	$L__BB6_46;

$L__BB6_45:
	add.s32 	%r76, %r76, 1;
	setp.lt.s32 	%p111, %r76, %r44;
	mov.u32 	%r77, %r40;
	@%p111 bra 	$L__BB6_43;

$L__BB6_46:
	cvta.to.global.u64 	%rd60, %rd12;
	shl.b64 	%rd61, %rd3, 2;
	add.s64 	%rd62, %rd60, %rd61;
	st.global.u32 	[%rd62], %r77;
	ret;

}
	// .globl	temperature_scale_f32_per_seq
.visible .entry temperature_scale_f32_per_seq(
	.param .u64 temperature_scale_f32_per_seq_param_0,
	.param .u64 temperature_scale_f32_per_seq_param_1,
	.param .u32 temperature_scale_f32_per_seq_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<12>;
	.reg .b32 	%r<21>;
	.reg .b64 	%rd<23>;


	ld.param.u64 	%rd9, [temperature_scale_f32_per_seq_param_0];
	ld.param.u64 	%rd10, [temperature_scale_f32_per_seq_param_1];
	ld.param.u32 	%r11, [temperature_scale_f32_per_seq_param_2];
	cvta.to.global.u64 	%rd1, %rd9;
	mov.u32 	%r12, %ctaid.x;
	cvta.to.global.u64 	%rd11, %rd10;
	mul.wide.s32 	%rd12, %r12, 4;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.nc.f32 	%f1, [%rd13];
	mul.lo.s32 	%r13, %r12, %r11;
	cvt.s64.s32 	%rd2, %r13;
	mov.u32 	%r19, %tid.x;
	setp.ge.s32 	%p1, %r19, %r11;
	@%p1 bra 	$L__BB7_7;

	not.b32 	%r14, %r19;
	add.s32 	%r2, %r14, %r11;
	shr.u32 	%r15, %r2, 8;
	add.s32 	%r16, %r15, 1;
	and.b32  	%r18, %r16, 3;
	setp.eq.s32 	%p2, %r18, 0;
	@%p2 bra 	$L__BB7_4;

	cvt.s64.s32 	%rd14, %r19;
	add.s64 	%rd15, %rd14, %rd2;
	shl.b64 	%rd16, %rd15, 2;
	add.s64 	%rd21, %rd1, %rd16;

$L__BB7_3:
	.pragma "nounroll";
	ld.global.f32 	%f2, [%rd21];
	mul.ftz.f32 	%f3, %f1, %f2;
	st.global.f32 	[%rd21], %f3;
	add.s32 	%r19, %r19, 256;
	add.s64 	%rd21, %rd21, 1024;
	add.s32 	%r18, %r18, -1;
	setp.ne.s32 	%p3, %r18, 0;
	@%p3 bra 	$L__BB7_3;

$L__BB7_4:
	setp.lt.u32 	%p4, %r2, 768;
	@%p4 bra 	$L__BB7_7;

	cvt.s64.s32 	%rd17, %r19;
	add.s64 	%rd18, %rd17, %rd2;
	shl.b64 	%rd19, %rd18, 2;
	add.s64 	%rd20, %rd1, %rd19;
	add.s64 	%rd22, %rd20, 2048;

$L__BB7_6:
	ld.global.f32 	%f4, [%rd22+-2048];
	mul.ftz.f32 	%f5, %f1, %f4;
	st.global.f32 	[%rd22+-2048], %f5;
	ld.global.f32 	%f6, [%rd22+-1024];
	mul.ftz.f32 	%f7, %f1, %f6;
	st.global.f32 	[%rd22+-1024], %f7;
	ld.global.f32 	%f8, [%rd22];
	mul.ftz.f32 	%f9, %f1, %f8;
	st.global.f32 	[%rd22], %f9;
	ld.global.f32 	%f10, [%rd22+1024];
	mul.ftz.f32 	%f11, %f1, %f10;
	st.global.f32 	[%rd22+1024], %f11;
	add.s64 	%rd22, %rd22, 4096;
	add.s32 	%r19, %r19, 1024;
	setp.lt.s32 	%p5, %r19, %r11;
	@%p5 bra 	$L__BB7_6;

$L__BB7_7:
	ret;

}

