//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32267302
// Cuda compilation tools, release 12.0, V12.0.140
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_89
.address_size 64

	// .globl	rotary_embedding_neox_bf16

.visible .entry rotary_embedding_neox_bf16(
	.param .u64 rotary_embedding_neox_bf16_param_0,
	.param .u64 rotary_embedding_neox_bf16_param_1,
	.param .u64 rotary_embedding_neox_bf16_param_2,
	.param .u64 rotary_embedding_neox_bf16_param_3,
	.param .u32 rotary_embedding_neox_bf16_param_4,
	.param .u32 rotary_embedding_neox_bf16_param_5,
	.param .u32 rotary_embedding_neox_bf16_param_6,
	.param .u32 rotary_embedding_neox_bf16_param_7,
	.param .u32 rotary_embedding_neox_bf16_param_8,
	.param .u32 rotary_embedding_neox_bf16_param_9
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<9>;
	.reg .f32 	%f<19>;
	.reg .b32 	%r<36>;
	.reg .b64 	%rd<44>;


	ld.param.u64 	%rd8, [rotary_embedding_neox_bf16_param_0];
	ld.param.u64 	%rd6, [rotary_embedding_neox_bf16_param_1];
	ld.param.u64 	%rd7, [rotary_embedding_neox_bf16_param_2];
	ld.param.u64 	%rd9, [rotary_embedding_neox_bf16_param_3];
	ld.param.u32 	%r14, [rotary_embedding_neox_bf16_param_5];
	ld.param.u32 	%r15, [rotary_embedding_neox_bf16_param_6];
	ld.param.u32 	%r16, [rotary_embedding_neox_bf16_param_7];
	ld.param.u32 	%r18, [rotary_embedding_neox_bf16_param_8];
	ld.param.u32 	%r17, [rotary_embedding_neox_bf16_param_9];
	cvta.to.global.u64 	%rd1, %rd9;
	mov.u32 	%r1, %ctaid.x;
	cvta.to.global.u64 	%rd10, %rd8;
	mul.wide.s32 	%rd11, %r1, 4;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.nc.u32 	%r19, [%rd12];
	ld.param.u32 	%r20, [rotary_embedding_neox_bf16_param_4];
	mul.lo.s32 	%r21, %r19, %r20;
	cvt.s64.s32 	%rd2, %r21;
	shr.u32 	%r22, %r20, 31;
	add.s32 	%r23, %r20, %r22;
	shr.s32 	%r2, %r23, 1;
	cvt.s64.s32 	%rd13, %r2;
	add.s64 	%rd3, %rd2, %rd13;
	mul.lo.s32 	%r3, %r2, %r18;
	mov.u32 	%r35, %tid.x;
	setp.ge.s32 	%p1, %r35, %r3;
	@%p1 bra 	$L__BB0_3;

	cvta.to.global.u64 	%rd4, %rd6;
	mul.lo.s32 	%r5, %r1, %r14;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r34, %r35;

$L__BB0_2:
	div.s32 	%r24, %r34, %r2;
	mad.lo.s32 	%r25, %r24, %r16, %r5;
	cvt.s64.s32 	%rd14, %r25;
	mul.lo.s32 	%r26, %r24, %r2;
	sub.s32 	%r27, %r34, %r26;
	add.s32 	%r28, %r27, %r2;
	cvt.s64.s32 	%rd15, %r27;
	add.s64 	%rd16, %rd15, %rd2;
	shl.b64 	%rd17, %rd16, 2;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.nc.f32 	%f5, [%rd18];
	add.s64 	%rd19, %rd3, %rd15;
	shl.b64 	%rd20, %rd19, 2;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.nc.f32 	%f6, [%rd21];
	add.s64 	%rd22, %rd14, %rd15;
	shl.b64 	%rd23, %rd22, 1;
	add.s64 	%rd24, %rd4, %rd23;
	ld.global.u16 	%rs1, [%rd24];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs1};}

	// end inline asm
	cvt.s64.s32 	%rd25, %r28;
	add.s64 	%rd26, %rd14, %rd25;
	shl.b64 	%rd27, %rd26, 1;
	add.s64 	%rd28, %rd4, %rd27;
	ld.global.u16 	%rs2, [%rd28];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs2};}

	// end inline asm
	mul.ftz.f32 	%f7, %f5, %f1;
	mul.ftz.f32 	%f8, %f6, %f2;
	sub.ftz.f32 	%f3, %f7, %f8;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs3, %f3;}

	// end inline asm
	st.global.u16 	[%rd24], %rs3;
	mul.ftz.f32 	%f9, %f5, %f2;
	fma.rn.ftz.f32 	%f4, %f6, %f1, %f9;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs4, %f4;}

	// end inline asm
	st.global.u16 	[%rd28], %rs4;
	add.s32 	%r34, %r34, %r6;
	setp.lt.s32 	%p2, %r34, %r3;
	@%p2 bra 	$L__BB0_2;

$L__BB0_3:
	setp.eq.s64 	%p3, %rd7, 0;
	@%p3 bra 	$L__BB0_7;

	mul.lo.s32 	%r9, %r2, %r17;
	setp.ge.s32 	%p4, %r35, %r9;
	@%p4 bra 	$L__BB0_7;

	cvta.to.global.u64 	%rd5, %rd7;
	mul.lo.s32 	%r10, %r1, %r15;
	mov.u32 	%r11, %ntid.x;

$L__BB0_6:
	div.s32 	%r29, %r35, %r2;
	mad.lo.s32 	%r30, %r29, %r16, %r10;
	cvt.s64.s32 	%rd29, %r30;
	mul.lo.s32 	%r31, %r29, %r2;
	sub.s32 	%r32, %r35, %r31;
	add.s32 	%r33, %r32, %r2;
	cvt.s64.s32 	%rd30, %r32;
	add.s64 	%rd31, %rd30, %rd2;
	shl.b64 	%rd32, %rd31, 2;
	add.s64 	%rd33, %rd1, %rd32;
	ld.global.nc.f32 	%f14, [%rd33];
	add.s64 	%rd34, %rd3, %rd30;
	shl.b64 	%rd35, %rd34, 2;
	add.s64 	%rd36, %rd1, %rd35;
	ld.global.nc.f32 	%f15, [%rd36];
	add.s64 	%rd37, %rd29, %rd30;
	shl.b64 	%rd38, %rd37, 1;
	add.s64 	%rd39, %rd5, %rd38;
	ld.global.u16 	%rs5, [%rd39];
	// begin inline asm
	{ mov.b32 %f10, {0,%rs5};}

	// end inline asm
	cvt.s64.s32 	%rd40, %r33;
	add.s64 	%rd41, %rd29, %rd40;
	shl.b64 	%rd42, %rd41, 1;
	add.s64 	%rd43, %rd5, %rd42;
	ld.global.u16 	%rs6, [%rd43];
	// begin inline asm
	{ mov.b32 %f11, {0,%rs6};}

	// end inline asm
	mul.ftz.f32 	%f16, %f14, %f10;
	mul.ftz.f32 	%f17, %f15, %f11;
	sub.ftz.f32 	%f12, %f16, %f17;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs7, %f12;}

	// end inline asm
	st.global.u16 	[%rd39], %rs7;
	mul.ftz.f32 	%f18, %f14, %f11;
	fma.rn.ftz.f32 	%f13, %f15, %f10, %f18;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs8, %f13;}

	// end inline asm
	st.global.u16 	[%rd43], %rs8;
	add.s32 	%r35, %r35, %r11;
	setp.lt.s32 	%p5, %r35, %r9;
	@%p5 bra 	$L__BB0_6;

$L__BB0_7:
	ret;

}
	// .globl	rotary_embedding_gptj_bf16
.visible .entry rotary_embedding_gptj_bf16(
	.param .u64 rotary_embedding_gptj_bf16_param_0,
	.param .u64 rotary_embedding_gptj_bf16_param_1,
	.param .u64 rotary_embedding_gptj_bf16_param_2,
	.param .u64 rotary_embedding_gptj_bf16_param_3,
	.param .u32 rotary_embedding_gptj_bf16_param_4,
	.param .u32 rotary_embedding_gptj_bf16_param_5,
	.param .u32 rotary_embedding_gptj_bf16_param_6,
	.param .u32 rotary_embedding_gptj_bf16_param_7,
	.param .u32 rotary_embedding_gptj_bf16_param_8,
	.param .u32 rotary_embedding_gptj_bf16_param_9
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<9>;
	.reg .f32 	%f<19>;
	.reg .b32 	%r<36>;
	.reg .b64 	%rd<38>;


	ld.param.u64 	%rd8, [rotary_embedding_gptj_bf16_param_0];
	ld.param.u64 	%rd6, [rotary_embedding_gptj_bf16_param_1];
	ld.param.u64 	%rd7, [rotary_embedding_gptj_bf16_param_2];
	ld.param.u64 	%rd9, [rotary_embedding_gptj_bf16_param_3];
	ld.param.u32 	%r14, [rotary_embedding_gptj_bf16_param_5];
	ld.param.u32 	%r15, [rotary_embedding_gptj_bf16_param_6];
	ld.param.u32 	%r16, [rotary_embedding_gptj_bf16_param_7];
	ld.param.u32 	%r18, [rotary_embedding_gptj_bf16_param_8];
	ld.param.u32 	%r17, [rotary_embedding_gptj_bf16_param_9];
	cvta.to.global.u64 	%rd1, %rd9;
	mov.u32 	%r1, %ctaid.x;
	cvta.to.global.u64 	%rd10, %rd8;
	mul.wide.s32 	%rd11, %r1, 4;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.nc.u32 	%r19, [%rd12];
	ld.param.u32 	%r20, [rotary_embedding_gptj_bf16_param_4];
	mul.lo.s32 	%r21, %r19, %r20;
	cvt.s64.s32 	%rd2, %r21;
	shr.u32 	%r22, %r20, 31;
	add.s32 	%r23, %r20, %r22;
	shr.s32 	%r2, %r23, 1;
	cvt.s64.s32 	%rd13, %r2;
	add.s64 	%rd3, %rd2, %rd13;
	mul.lo.s32 	%r3, %r2, %r18;
	mov.u32 	%r35, %tid.x;
	setp.ge.s32 	%p1, %r35, %r3;
	@%p1 bra 	$L__BB1_3;

	mul.lo.s32 	%r5, %r1, %r14;
	mov.u32 	%r6, %ntid.x;
	cvta.to.global.u64 	%rd4, %rd6;
	mov.u32 	%r34, %r35;

$L__BB1_2:
	div.s32 	%r24, %r34, %r2;
	mad.lo.s32 	%r25, %r24, %r16, %r5;
	cvt.s64.s32 	%rd14, %r25;
	mul.lo.s32 	%r26, %r24, %r2;
	sub.s32 	%r27, %r34, %r26;
	shl.b32 	%r28, %r27, 1;
	cvt.s64.s32 	%rd15, %r27;
	add.s64 	%rd16, %rd15, %rd2;
	shl.b64 	%rd17, %rd16, 2;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.nc.f32 	%f5, [%rd18];
	add.s64 	%rd19, %rd3, %rd15;
	shl.b64 	%rd20, %rd19, 2;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.nc.f32 	%f6, [%rd21];
	cvt.s64.s32 	%rd22, %r28;
	add.s64 	%rd23, %rd14, %rd22;
	shl.b64 	%rd24, %rd23, 1;
	add.s64 	%rd25, %rd4, %rd24;
	ld.global.u16 	%rs1, [%rd25];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs1};}

	// end inline asm
	ld.global.u16 	%rs2, [%rd25+2];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs2};}

	// end inline asm
	mul.ftz.f32 	%f7, %f5, %f1;
	mul.ftz.f32 	%f8, %f6, %f2;
	sub.ftz.f32 	%f3, %f7, %f8;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs3, %f3;}

	// end inline asm
	st.global.u16 	[%rd25], %rs3;
	mul.ftz.f32 	%f9, %f5, %f2;
	fma.rn.ftz.f32 	%f4, %f6, %f1, %f9;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs4, %f4;}

	// end inline asm
	st.global.u16 	[%rd25+2], %rs4;
	add.s32 	%r34, %r34, %r6;
	setp.lt.s32 	%p2, %r34, %r3;
	@%p2 bra 	$L__BB1_2;

$L__BB1_3:
	setp.eq.s64 	%p3, %rd7, 0;
	@%p3 bra 	$L__BB1_7;

	mul.lo.s32 	%r9, %r2, %r17;
	setp.ge.s32 	%p4, %r35, %r9;
	@%p4 bra 	$L__BB1_7;

	mul.lo.s32 	%r10, %r1, %r15;
	mov.u32 	%r11, %ntid.x;
	cvta.to.global.u64 	%rd5, %rd7;

$L__BB1_6:
	div.s32 	%r29, %r35, %r2;
	mad.lo.s32 	%r30, %r29, %r16, %r10;
	cvt.s64.s32 	%rd26, %r30;
	mul.lo.s32 	%r31, %r29, %r2;
	sub.s32 	%r32, %r35, %r31;
	shl.b32 	%r33, %r32, 1;
	cvt.s64.s32 	%rd27, %r32;
	add.s64 	%rd28, %rd27, %rd2;
	shl.b64 	%rd29, %rd28, 2;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.nc.f32 	%f14, [%rd30];
	add.s64 	%rd31, %rd3, %rd27;
	shl.b64 	%rd32, %rd31, 2;
	add.s64 	%rd33, %rd1, %rd32;
	ld.global.nc.f32 	%f15, [%rd33];
	cvt.s64.s32 	%rd34, %r33;
	add.s64 	%rd35, %rd26, %rd34;
	shl.b64 	%rd36, %rd35, 1;
	add.s64 	%rd37, %rd5, %rd36;
	ld.global.u16 	%rs5, [%rd37];
	// begin inline asm
	{ mov.b32 %f10, {0,%rs5};}

	// end inline asm
	ld.global.u16 	%rs6, [%rd37+2];
	// begin inline asm
	{ mov.b32 %f11, {0,%rs6};}

	// end inline asm
	mul.ftz.f32 	%f16, %f14, %f10;
	mul.ftz.f32 	%f17, %f15, %f11;
	sub.ftz.f32 	%f12, %f16, %f17;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs7, %f12;}

	// end inline asm
	st.global.u16 	[%rd37], %rs7;
	mul.ftz.f32 	%f18, %f14, %f11;
	fma.rn.ftz.f32 	%f13, %f15, %f10, %f18;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs8, %f13;}

	// end inline asm
	st.global.u16 	[%rd37+2], %rs8;
	add.s32 	%r35, %r35, %r11;
	setp.lt.s32 	%p5, %r35, %r9;
	@%p5 bra 	$L__BB1_6;

$L__BB1_7:
	ret;

}
	// .globl	rotary_embedding_neox_fp16
.visible .entry rotary_embedding_neox_fp16(
	.param .u64 rotary_embedding_neox_fp16_param_0,
	.param .u64 rotary_embedding_neox_fp16_param_1,
	.param .u64 rotary_embedding_neox_fp16_param_2,
	.param .u64 rotary_embedding_neox_fp16_param_3,
	.param .u32 rotary_embedding_neox_fp16_param_4,
	.param .u32 rotary_embedding_neox_fp16_param_5,
	.param .u32 rotary_embedding_neox_fp16_param_6,
	.param .u32 rotary_embedding_neox_fp16_param_7,
	.param .u32 rotary_embedding_neox_fp16_param_8,
	.param .u32 rotary_embedding_neox_fp16_param_9
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<9>;
	.reg .f32 	%f<19>;
	.reg .b32 	%r<36>;
	.reg .b64 	%rd<44>;


	ld.param.u64 	%rd8, [rotary_embedding_neox_fp16_param_0];
	ld.param.u64 	%rd6, [rotary_embedding_neox_fp16_param_1];
	ld.param.u64 	%rd7, [rotary_embedding_neox_fp16_param_2];
	ld.param.u64 	%rd9, [rotary_embedding_neox_fp16_param_3];
	ld.param.u32 	%r14, [rotary_embedding_neox_fp16_param_5];
	ld.param.u32 	%r15, [rotary_embedding_neox_fp16_param_6];
	ld.param.u32 	%r16, [rotary_embedding_neox_fp16_param_7];
	ld.param.u32 	%r18, [rotary_embedding_neox_fp16_param_8];
	ld.param.u32 	%r17, [rotary_embedding_neox_fp16_param_9];
	cvta.to.global.u64 	%rd1, %rd9;
	mov.u32 	%r1, %ctaid.x;
	cvta.to.global.u64 	%rd10, %rd8;
	mul.wide.s32 	%rd11, %r1, 4;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.nc.u32 	%r19, [%rd12];
	ld.param.u32 	%r20, [rotary_embedding_neox_fp16_param_4];
	mul.lo.s32 	%r21, %r19, %r20;
	cvt.s64.s32 	%rd2, %r21;
	shr.u32 	%r22, %r20, 31;
	add.s32 	%r23, %r20, %r22;
	shr.s32 	%r2, %r23, 1;
	cvt.s64.s32 	%rd13, %r2;
	add.s64 	%rd3, %rd2, %rd13;
	mul.lo.s32 	%r3, %r2, %r18;
	mov.u32 	%r35, %tid.x;
	setp.ge.s32 	%p1, %r35, %r3;
	@%p1 bra 	$L__BB2_3;

	cvta.to.global.u64 	%rd4, %rd6;
	mul.lo.s32 	%r5, %r1, %r14;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r34, %r35;

$L__BB2_2:
	div.s32 	%r24, %r34, %r2;
	mad.lo.s32 	%r25, %r24, %r16, %r5;
	cvt.s64.s32 	%rd14, %r25;
	mul.lo.s32 	%r26, %r24, %r2;
	sub.s32 	%r27, %r34, %r26;
	add.s32 	%r28, %r27, %r2;
	cvt.s64.s32 	%rd15, %r27;
	add.s64 	%rd16, %rd15, %rd2;
	shl.b64 	%rd17, %rd16, 2;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.nc.f32 	%f5, [%rd18];
	add.s64 	%rd19, %rd3, %rd15;
	shl.b64 	%rd20, %rd19, 2;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.nc.f32 	%f6, [%rd21];
	add.s64 	%rd22, %rd14, %rd15;
	shl.b64 	%rd23, %rd22, 1;
	add.s64 	%rd24, %rd4, %rd23;
	ld.global.u16 	%rs1, [%rd24];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	cvt.s64.s32 	%rd25, %r28;
	add.s64 	%rd26, %rd14, %rd25;
	shl.b64 	%rd27, %rd26, 1;
	add.s64 	%rd28, %rd4, %rd27;
	ld.global.u16 	%rs2, [%rd28];
	// begin inline asm
	{  cvt.f32.f16 %f2, %rs2;}

	// end inline asm
	mul.ftz.f32 	%f7, %f5, %f1;
	mul.ftz.f32 	%f8, %f6, %f2;
	sub.ftz.f32 	%f3, %f7, %f8;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f3;}

	// end inline asm
	st.global.u16 	[%rd24], %rs3;
	mul.ftz.f32 	%f9, %f5, %f2;
	fma.rn.ftz.f32 	%f4, %f6, %f1, %f9;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs4, %f4;}

	// end inline asm
	st.global.u16 	[%rd28], %rs4;
	add.s32 	%r34, %r34, %r6;
	setp.lt.s32 	%p2, %r34, %r3;
	@%p2 bra 	$L__BB2_2;

$L__BB2_3:
	setp.eq.s64 	%p3, %rd7, 0;
	@%p3 bra 	$L__BB2_7;

	mul.lo.s32 	%r9, %r2, %r17;
	setp.ge.s32 	%p4, %r35, %r9;
	@%p4 bra 	$L__BB2_7;

	cvta.to.global.u64 	%rd5, %rd7;
	mul.lo.s32 	%r10, %r1, %r15;
	mov.u32 	%r11, %ntid.x;

$L__BB2_6:
	div.s32 	%r29, %r35, %r2;
	mad.lo.s32 	%r30, %r29, %r16, %r10;
	cvt.s64.s32 	%rd29, %r30;
	mul.lo.s32 	%r31, %r29, %r2;
	sub.s32 	%r32, %r35, %r31;
	add.s32 	%r33, %r32, %r2;
	cvt.s64.s32 	%rd30, %r32;
	add.s64 	%rd31, %rd30, %rd2;
	shl.b64 	%rd32, %rd31, 2;
	add.s64 	%rd33, %rd1, %rd32;
	ld.global.nc.f32 	%f14, [%rd33];
	add.s64 	%rd34, %rd3, %rd30;
	shl.b64 	%rd35, %rd34, 2;
	add.s64 	%rd36, %rd1, %rd35;
	ld.global.nc.f32 	%f15, [%rd36];
	add.s64 	%rd37, %rd29, %rd30;
	shl.b64 	%rd38, %rd37, 1;
	add.s64 	%rd39, %rd5, %rd38;
	ld.global.u16 	%rs5, [%rd39];
	// begin inline asm
	{  cvt.f32.f16 %f10, %rs5;}

	// end inline asm
	cvt.s64.s32 	%rd40, %r33;
	add.s64 	%rd41, %rd29, %rd40;
	shl.b64 	%rd42, %rd41, 1;
	add.s64 	%rd43, %rd5, %rd42;
	ld.global.u16 	%rs6, [%rd43];
	// begin inline asm
	{  cvt.f32.f16 %f11, %rs6;}

	// end inline asm
	mul.ftz.f32 	%f16, %f14, %f10;
	mul.ftz.f32 	%f17, %f15, %f11;
	sub.ftz.f32 	%f12, %f16, %f17;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs7, %f12;}

	// end inline asm
	st.global.u16 	[%rd39], %rs7;
	mul.ftz.f32 	%f18, %f14, %f11;
	fma.rn.ftz.f32 	%f13, %f15, %f10, %f18;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs8, %f13;}

	// end inline asm
	st.global.u16 	[%rd43], %rs8;
	add.s32 	%r35, %r35, %r11;
	setp.lt.s32 	%p5, %r35, %r9;
	@%p5 bra 	$L__BB2_6;

$L__BB2_7:
	ret;

}
	// .globl	rotary_embedding_neox_f32
.visible .entry rotary_embedding_neox_f32(
	.param .u64 rotary_embedding_neox_f32_param_0,
	.param .u64 rotary_embedding_neox_f32_param_1,
	.param .u64 rotary_embedding_neox_f32_param_2,
	.param .u64 rotary_embedding_neox_f32_param_3,
	.param .u32 rotary_embedding_neox_f32_param_4,
	.param .u32 rotary_embedding_neox_f32_param_5,
	.param .u32 rotary_embedding_neox_f32_param_6,
	.param .u32 rotary_embedding_neox_f32_param_7,
	.param .u32 rotary_embedding_neox_f32_param_8,
	.param .u32 rotary_embedding_neox_f32_param_9
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<19>;
	.reg .b32 	%r<36>;
	.reg .b64 	%rd<44>;


	ld.param.u64 	%rd8, [rotary_embedding_neox_f32_param_0];
	ld.param.u64 	%rd6, [rotary_embedding_neox_f32_param_1];
	ld.param.u64 	%rd7, [rotary_embedding_neox_f32_param_2];
	ld.param.u64 	%rd9, [rotary_embedding_neox_f32_param_3];
	ld.param.u32 	%r14, [rotary_embedding_neox_f32_param_5];
	ld.param.u32 	%r15, [rotary_embedding_neox_f32_param_6];
	ld.param.u32 	%r16, [rotary_embedding_neox_f32_param_7];
	ld.param.u32 	%r18, [rotary_embedding_neox_f32_param_8];
	ld.param.u32 	%r17, [rotary_embedding_neox_f32_param_9];
	cvta.to.global.u64 	%rd1, %rd9;
	mov.u32 	%r1, %ctaid.x;
	cvta.to.global.u64 	%rd10, %rd8;
	mul.wide.s32 	%rd11, %r1, 4;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.nc.u32 	%r19, [%rd12];
	ld.param.u32 	%r20, [rotary_embedding_neox_f32_param_4];
	mul.lo.s32 	%r21, %r19, %r20;
	cvt.s64.s32 	%rd2, %r21;
	shr.u32 	%r22, %r20, 31;
	add.s32 	%r23, %r20, %r22;
	shr.s32 	%r2, %r23, 1;
	cvt.s64.s32 	%rd13, %r2;
	add.s64 	%rd3, %rd2, %rd13;
	mul.lo.s32 	%r3, %r2, %r18;
	mov.u32 	%r35, %tid.x;
	setp.ge.s32 	%p1, %r35, %r3;
	@%p1 bra 	$L__BB3_3;

	cvta.to.global.u64 	%rd4, %rd6;
	mul.lo.s32 	%r5, %r1, %r14;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r34, %r35;

$L__BB3_2:
	div.s32 	%r24, %r34, %r2;
	mad.lo.s32 	%r25, %r24, %r16, %r5;
	cvt.s64.s32 	%rd14, %r25;
	mul.lo.s32 	%r26, %r24, %r2;
	sub.s32 	%r27, %r34, %r26;
	add.s32 	%r28, %r27, %r2;
	cvt.s64.s32 	%rd15, %r27;
	add.s64 	%rd16, %rd15, %rd2;
	shl.b64 	%rd17, %rd16, 2;
	add.s64 	%rd18, %rd1, %rd17;
	add.s64 	%rd19, %rd3, %rd15;
	shl.b64 	%rd20, %rd19, 2;
	add.s64 	%rd21, %rd1, %rd20;
	add.s64 	%rd22, %rd14, %rd15;
	shl.b64 	%rd23, %rd22, 2;
	add.s64 	%rd24, %rd4, %rd23;
	cvt.s64.s32 	%rd25, %r28;
	add.s64 	%rd26, %rd14, %rd25;
	shl.b64 	%rd27, %rd26, 2;
	add.s64 	%rd28, %rd4, %rd27;
	ld.global.f32 	%f1, [%rd24];
	ld.global.nc.f32 	%f2, [%rd18];
	mul.ftz.f32 	%f3, %f2, %f1;
	ld.global.f32 	%f4, [%rd28];
	ld.global.nc.f32 	%f5, [%rd21];
	mul.ftz.f32 	%f6, %f5, %f4;
	sub.ftz.f32 	%f7, %f3, %f6;
	st.global.f32 	[%rd24], %f7;
	mul.ftz.f32 	%f8, %f2, %f4;
	fma.rn.ftz.f32 	%f9, %f5, %f1, %f8;
	st.global.f32 	[%rd28], %f9;
	add.s32 	%r34, %r34, %r6;
	setp.lt.s32 	%p2, %r34, %r3;
	@%p2 bra 	$L__BB3_2;

$L__BB3_3:
	setp.eq.s64 	%p3, %rd7, 0;
	@%p3 bra 	$L__BB3_7;

	mul.lo.s32 	%r9, %r2, %r17;
	setp.ge.s32 	%p4, %r35, %r9;
	@%p4 bra 	$L__BB3_7;

	cvta.to.global.u64 	%rd5, %rd7;
	mul.lo.s32 	%r10, %r1, %r15;
	mov.u32 	%r11, %ntid.x;

$L__BB3_6:
	div.s32 	%r29, %r35, %r2;
	mad.lo.s32 	%r30, %r29, %r16, %r10;
	cvt.s64.s32 	%rd29, %r30;
	mul.lo.s32 	%r31, %r29, %r2;
	sub.s32 	%r32, %r35, %r31;
	add.s32 	%r33, %r32, %r2;
	cvt.s64.s32 	%rd30, %r32;
	add.s64 	%rd31, %rd30, %rd2;
	shl.b64 	%rd32, %rd31, 2;
	add.s64 	%rd33, %rd1, %rd32;
	add.s64 	%rd34, %rd3, %rd30;
	shl.b64 	%rd35, %rd34, 2;
	add.s64 	%rd36, %rd1, %rd35;
	add.s64 	%rd37, %rd29, %rd30;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd39, %rd5, %rd38;
	cvt.s64.s32 	%rd40, %r33;
	add.s64 	%rd41, %rd29, %rd40;
	shl.b64 	%rd42, %rd41, 2;
	add.s64 	%rd43, %rd5, %rd42;
	ld.global.f32 	%f10, [%rd39];
	ld.global.nc.f32 	%f11, [%rd33];
	mul.ftz.f32 	%f12, %f11, %f10;
	ld.global.f32 	%f13, [%rd43];
	ld.global.nc.f32 	%f14, [%rd36];
	mul.ftz.f32 	%f15, %f14, %f13;
	sub.ftz.f32 	%f16, %f12, %f15;
	st.global.f32 	[%rd39], %f16;
	mul.ftz.f32 	%f17, %f11, %f13;
	fma.rn.ftz.f32 	%f18, %f14, %f10, %f17;
	st.global.f32 	[%rd43], %f18;
	add.s32 	%r35, %r35, %r11;
	setp.lt.s32 	%p5, %r35, %r9;
	@%p5 bra 	$L__BB3_6;

$L__BB3_7:
	ret;

}

