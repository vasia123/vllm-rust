//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32267302
// Cuda compilation tools, release 12.0, V12.0.140
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_89
.address_size 64

	// .globl	gptq_dequant_int4_bf16
// _ZZ25gptq_gemm_int4_bf16_tiledE10input_tile has been demoted

.visible .entry gptq_dequant_int4_bf16(
	.param .u64 gptq_dequant_int4_bf16_param_0,
	.param .u64 gptq_dequant_int4_bf16_param_1,
	.param .u64 gptq_dequant_int4_bf16_param_2,
	.param .u64 gptq_dequant_int4_bf16_param_3,
	.param .u32 gptq_dequant_int4_bf16_param_4,
	.param .u32 gptq_dequant_int4_bf16_param_5,
	.param .u32 gptq_dequant_int4_bf16_param_6,
	.param .u32 gptq_dequant_int4_bf16_param_7
)
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<46>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd1, [gptq_dequant_int4_bf16_param_0];
	ld.param.u64 	%rd2, [gptq_dequant_int4_bf16_param_1];
	ld.param.u64 	%rd3, [gptq_dequant_int4_bf16_param_2];
	ld.param.u64 	%rd4, [gptq_dequant_int4_bf16_param_3];
	ld.param.u32 	%r7, [gptq_dequant_int4_bf16_param_4];
	ld.param.u32 	%r5, [gptq_dequant_int4_bf16_param_5];
	ld.param.u32 	%r6, [gptq_dequant_int4_bf16_param_6];
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r1, %r9, %r8, %r10;
	mov.u32 	%r11, %ntid.y;
	mov.u32 	%r12, %ctaid.y;
	mov.u32 	%r13, %tid.y;
	mad.lo.s32 	%r2, %r12, %r11, %r13;
	setp.ge.s32 	%p1, %r1, %r5;
	setp.ge.s32 	%p2, %r2, %r7;
	or.pred  	%p3, %p2, %p1;
	@%p3 bra 	$L__BB0_4;

	setp.lt.s32 	%p4, %r6, 1;
	mov.u32 	%r45, 0;
	@%p4 bra 	$L__BB0_3;

	div.s32 	%r45, %r2, %r6;

$L__BB0_3:
	mad.lo.s32 	%r15, %r45, %r5, %r1;
	cvta.to.global.u64 	%rd5, %rd3;
	mul.wide.s32 	%rd6, %r15, 2;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.nc.u16 	%rs1, [%rd7];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs1};}

	// end inline asm
	add.s32 	%r16, %r5, 7;
	shr.s32 	%r17, %r16, 31;
	shr.u32 	%r18, %r17, 29;
	add.s32 	%r19, %r16, %r18;
	shr.s32 	%r20, %r19, 3;
	shr.s32 	%r21, %r1, 31;
	shr.u32 	%r22, %r21, 29;
	add.s32 	%r23, %r1, %r22;
	shr.s32 	%r24, %r23, 3;
	mad.lo.s32 	%r25, %r45, %r20, %r24;
	cvta.to.global.u64 	%rd8, %rd4;
	mul.wide.s32 	%rd9, %r25, 4;
	add.s64 	%rd10, %rd8, %rd9;
	and.b32  	%r26, %r23, 1073741816;
	sub.s32 	%r27, %r1, %r26;
	shl.b32 	%r28, %r27, 2;
	ld.global.nc.u32 	%r29, [%rd10];
	shr.u32 	%r30, %r29, %r28;
	and.b32  	%r31, %r30, 15;
	shr.s32 	%r32, %r2, 31;
	shr.u32 	%r33, %r32, 29;
	add.s32 	%r34, %r2, %r33;
	shr.s32 	%r35, %r34, 3;
	mad.lo.s32 	%r36, %r35, %r5, %r1;
	cvta.to.global.u64 	%rd11, %rd2;
	mul.wide.s32 	%rd12, %r36, 4;
	add.s64 	%rd13, %rd11, %rd12;
	and.b32  	%r37, %r34, 1073741816;
	sub.s32 	%r38, %r2, %r37;
	shl.b32 	%r39, %r38, 2;
	ld.global.nc.u32 	%r40, [%rd13];
	shr.u32 	%r41, %r40, %r39;
	and.b32  	%r42, %r41, 15;
	sub.s32 	%r43, %r42, %r31;
	cvt.rn.f32.s32 	%f3, %r43;
	mul.ftz.f32 	%f2, %f1, %f3;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs2, %f2;}

	// end inline asm
	mad.lo.s32 	%r44, %r2, %r5, %r1;
	cvta.to.global.u64 	%rd14, %rd1;
	mul.wide.s32 	%rd15, %r44, 2;
	add.s64 	%rd16, %rd14, %rd15;
	st.global.u16 	[%rd16], %rs2;

$L__BB0_4:
	ret;

}
	// .globl	gptq_dequant_int4_fp16
.visible .entry gptq_dequant_int4_fp16(
	.param .u64 gptq_dequant_int4_fp16_param_0,
	.param .u64 gptq_dequant_int4_fp16_param_1,
	.param .u64 gptq_dequant_int4_fp16_param_2,
	.param .u64 gptq_dequant_int4_fp16_param_3,
	.param .u32 gptq_dequant_int4_fp16_param_4,
	.param .u32 gptq_dequant_int4_fp16_param_5,
	.param .u32 gptq_dequant_int4_fp16_param_6,
	.param .u32 gptq_dequant_int4_fp16_param_7
)
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<46>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd1, [gptq_dequant_int4_fp16_param_0];
	ld.param.u64 	%rd2, [gptq_dequant_int4_fp16_param_1];
	ld.param.u64 	%rd3, [gptq_dequant_int4_fp16_param_2];
	ld.param.u64 	%rd4, [gptq_dequant_int4_fp16_param_3];
	ld.param.u32 	%r7, [gptq_dequant_int4_fp16_param_4];
	ld.param.u32 	%r5, [gptq_dequant_int4_fp16_param_5];
	ld.param.u32 	%r6, [gptq_dequant_int4_fp16_param_6];
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r1, %r9, %r8, %r10;
	mov.u32 	%r11, %ntid.y;
	mov.u32 	%r12, %ctaid.y;
	mov.u32 	%r13, %tid.y;
	mad.lo.s32 	%r2, %r12, %r11, %r13;
	setp.ge.s32 	%p1, %r1, %r5;
	setp.ge.s32 	%p2, %r2, %r7;
	or.pred  	%p3, %p2, %p1;
	@%p3 bra 	$L__BB1_4;

	setp.lt.s32 	%p4, %r6, 1;
	mov.u32 	%r45, 0;
	@%p4 bra 	$L__BB1_3;

	div.s32 	%r45, %r2, %r6;

$L__BB1_3:
	mad.lo.s32 	%r15, %r45, %r5, %r1;
	cvta.to.global.u64 	%rd5, %rd3;
	mul.wide.s32 	%rd6, %r15, 2;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.nc.u16 	%rs1, [%rd7];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	add.s32 	%r16, %r5, 7;
	shr.s32 	%r17, %r16, 31;
	shr.u32 	%r18, %r17, 29;
	add.s32 	%r19, %r16, %r18;
	shr.s32 	%r20, %r19, 3;
	shr.s32 	%r21, %r1, 31;
	shr.u32 	%r22, %r21, 29;
	add.s32 	%r23, %r1, %r22;
	shr.s32 	%r24, %r23, 3;
	mad.lo.s32 	%r25, %r45, %r20, %r24;
	cvta.to.global.u64 	%rd8, %rd4;
	mul.wide.s32 	%rd9, %r25, 4;
	add.s64 	%rd10, %rd8, %rd9;
	and.b32  	%r26, %r23, 1073741816;
	sub.s32 	%r27, %r1, %r26;
	shl.b32 	%r28, %r27, 2;
	ld.global.nc.u32 	%r29, [%rd10];
	shr.u32 	%r30, %r29, %r28;
	and.b32  	%r31, %r30, 15;
	shr.s32 	%r32, %r2, 31;
	shr.u32 	%r33, %r32, 29;
	add.s32 	%r34, %r2, %r33;
	shr.s32 	%r35, %r34, 3;
	mad.lo.s32 	%r36, %r35, %r5, %r1;
	cvta.to.global.u64 	%rd11, %rd2;
	mul.wide.s32 	%rd12, %r36, 4;
	add.s64 	%rd13, %rd11, %rd12;
	and.b32  	%r37, %r34, 1073741816;
	sub.s32 	%r38, %r2, %r37;
	shl.b32 	%r39, %r38, 2;
	ld.global.nc.u32 	%r40, [%rd13];
	shr.u32 	%r41, %r40, %r39;
	and.b32  	%r42, %r41, 15;
	sub.s32 	%r43, %r42, %r31;
	cvt.rn.f32.s32 	%f3, %r43;
	mul.ftz.f32 	%f2, %f1, %f3;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f2;}

	// end inline asm
	mad.lo.s32 	%r44, %r2, %r5, %r1;
	cvta.to.global.u64 	%rd14, %rd1;
	mul.wide.s32 	%rd15, %r44, 2;
	add.s64 	%rd16, %rd14, %rd15;
	st.global.u16 	[%rd16], %rs2;

$L__BB1_4:
	ret;

}
	// .globl	gptq_dequant_int8_bf16
.visible .entry gptq_dequant_int8_bf16(
	.param .u64 gptq_dequant_int8_bf16_param_0,
	.param .u64 gptq_dequant_int8_bf16_param_1,
	.param .u64 gptq_dequant_int8_bf16_param_2,
	.param .u64 gptq_dequant_int8_bf16_param_3,
	.param .u32 gptq_dequant_int8_bf16_param_4,
	.param .u32 gptq_dequant_int8_bf16_param_5,
	.param .u32 gptq_dequant_int8_bf16_param_6,
	.param .u32 gptq_dequant_int8_bf16_param_7
)
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<46>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd1, [gptq_dequant_int8_bf16_param_0];
	ld.param.u64 	%rd2, [gptq_dequant_int8_bf16_param_1];
	ld.param.u64 	%rd3, [gptq_dequant_int8_bf16_param_2];
	ld.param.u64 	%rd4, [gptq_dequant_int8_bf16_param_3];
	ld.param.u32 	%r7, [gptq_dequant_int8_bf16_param_4];
	ld.param.u32 	%r5, [gptq_dequant_int8_bf16_param_5];
	ld.param.u32 	%r6, [gptq_dequant_int8_bf16_param_6];
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r1, %r9, %r8, %r10;
	mov.u32 	%r11, %ntid.y;
	mov.u32 	%r12, %ctaid.y;
	mov.u32 	%r13, %tid.y;
	mad.lo.s32 	%r2, %r12, %r11, %r13;
	setp.ge.s32 	%p1, %r1, %r5;
	setp.ge.s32 	%p2, %r2, %r7;
	or.pred  	%p3, %p2, %p1;
	@%p3 bra 	$L__BB2_4;

	setp.lt.s32 	%p4, %r6, 1;
	mov.u32 	%r45, 0;
	@%p4 bra 	$L__BB2_3;

	div.s32 	%r45, %r2, %r6;

$L__BB2_3:
	mad.lo.s32 	%r15, %r45, %r5, %r1;
	cvta.to.global.u64 	%rd5, %rd3;
	mul.wide.s32 	%rd6, %r15, 2;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.nc.u16 	%rs1, [%rd7];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs1};}

	// end inline asm
	add.s32 	%r16, %r5, 3;
	shr.s32 	%r17, %r16, 31;
	shr.u32 	%r18, %r17, 30;
	add.s32 	%r19, %r16, %r18;
	shr.s32 	%r20, %r19, 2;
	shr.s32 	%r21, %r1, 31;
	shr.u32 	%r22, %r21, 30;
	add.s32 	%r23, %r1, %r22;
	shr.s32 	%r24, %r23, 2;
	mad.lo.s32 	%r25, %r45, %r20, %r24;
	cvta.to.global.u64 	%rd8, %rd4;
	mul.wide.s32 	%rd9, %r25, 4;
	add.s64 	%rd10, %rd8, %rd9;
	and.b32  	%r26, %r23, 536870908;
	sub.s32 	%r27, %r1, %r26;
	shl.b32 	%r28, %r27, 3;
	ld.global.nc.u32 	%r29, [%rd10];
	shr.u32 	%r30, %r29, %r28;
	and.b32  	%r31, %r30, 255;
	shr.s32 	%r32, %r2, 31;
	shr.u32 	%r33, %r32, 30;
	add.s32 	%r34, %r2, %r33;
	shr.s32 	%r35, %r34, 2;
	mad.lo.s32 	%r36, %r35, %r5, %r1;
	cvta.to.global.u64 	%rd11, %rd2;
	mul.wide.s32 	%rd12, %r36, 4;
	add.s64 	%rd13, %rd11, %rd12;
	and.b32  	%r37, %r34, 536870908;
	sub.s32 	%r38, %r2, %r37;
	shl.b32 	%r39, %r38, 3;
	ld.global.nc.u32 	%r40, [%rd13];
	shr.u32 	%r41, %r40, %r39;
	and.b32  	%r42, %r41, 255;
	sub.s32 	%r43, %r42, %r31;
	cvt.rn.f32.s32 	%f3, %r43;
	mul.ftz.f32 	%f2, %f1, %f3;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs2, %f2;}

	// end inline asm
	mad.lo.s32 	%r44, %r2, %r5, %r1;
	cvta.to.global.u64 	%rd14, %rd1;
	mul.wide.s32 	%rd15, %r44, 2;
	add.s64 	%rd16, %rd14, %rd15;
	st.global.u16 	[%rd16], %rs2;

$L__BB2_4:
	ret;

}
	// .globl	gptq_gemm_int4_bf16_naive
.visible .entry gptq_gemm_int4_bf16_naive(
	.param .u64 gptq_gemm_int4_bf16_naive_param_0,
	.param .u64 gptq_gemm_int4_bf16_naive_param_1,
	.param .u64 gptq_gemm_int4_bf16_naive_param_2,
	.param .u64 gptq_gemm_int4_bf16_naive_param_3,
	.param .u64 gptq_gemm_int4_bf16_naive_param_4,
	.param .u64 gptq_gemm_int4_bf16_naive_param_5,
	.param .u32 gptq_gemm_int4_bf16_naive_param_6,
	.param .u32 gptq_gemm_int4_bf16_naive_param_7,
	.param .u32 gptq_gemm_int4_bf16_naive_param_8,
	.param .u32 gptq_gemm_int4_bf16_naive_param_9,
	.param .u32 gptq_gemm_int4_bf16_naive_param_10,
	.param .u8 gptq_gemm_int4_bf16_naive_param_11
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<24>;
	.reg .f32 	%f<81>;
	.reg .b32 	%r<180>;
	.reg .b64 	%rd<77>;


	ld.param.s8 	%rs1, [gptq_gemm_int4_bf16_naive_param_11];
	ld.param.u64 	%rd19, [gptq_gemm_int4_bf16_naive_param_0];
	ld.param.u64 	%rd21, [gptq_gemm_int4_bf16_naive_param_1];
	ld.param.u64 	%rd22, [gptq_gemm_int4_bf16_naive_param_2];
	ld.param.u64 	%rd23, [gptq_gemm_int4_bf16_naive_param_3];
	ld.param.u64 	%rd24, [gptq_gemm_int4_bf16_naive_param_4];
	ld.param.u64 	%rd20, [gptq_gemm_int4_bf16_naive_param_5];
	ld.param.u32 	%r32, [gptq_gemm_int4_bf16_naive_param_6];
	ld.param.u32 	%r29, [gptq_gemm_int4_bf16_naive_param_7];
	ld.param.u32 	%r30, [gptq_gemm_int4_bf16_naive_param_8];
	ld.param.u32 	%r31, [gptq_gemm_int4_bf16_naive_param_9];
	cvta.to.global.u64 	%rd1, %rd22;
	cvta.to.global.u64 	%rd2, %rd24;
	cvta.to.global.u64 	%rd3, %rd23;
	cvta.to.global.u64 	%rd4, %rd21;
	mov.u32 	%r33, %ntid.y;
	mov.u32 	%r34, %ctaid.y;
	mov.u32 	%r35, %tid.y;
	mad.lo.s32 	%r1, %r34, %r33, %r35;
	mov.u32 	%r36, %ntid.x;
	mov.u32 	%r37, %ctaid.x;
	mov.u32 	%r38, %tid.x;
	mad.lo.s32 	%r2, %r37, %r36, %r38;
	setp.ge.s32 	%p1, %r1, %r32;
	setp.ge.s32 	%p2, %r2, %r29;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB3_18;

	add.s32 	%r39, %r29, 7;
	shr.s32 	%r40, %r39, 31;
	shr.u32 	%r41, %r40, 29;
	add.s32 	%r42, %r39, %r41;
	shr.s32 	%r3, %r42, 3;
	shr.s32 	%r43, %r2, 31;
	shr.u32 	%r44, %r43, 29;
	add.s32 	%r45, %r2, %r44;
	shr.s32 	%r4, %r45, 3;
	setp.lt.s32 	%p4, %r30, 1;
	mov.f32 	%f79, 0f00000000;
	@%p4 bra 	$L__BB3_15;

	and.b32  	%r49, %r45, 1073741816;
	sub.s32 	%r50, %r2, %r49;
	shl.b32 	%r5, %r50, 2;
	add.s32 	%r6, %r30, -1;
	and.b32  	%r174, %r30, 3;
	setp.gt.s32 	%p5, %r31, 0;
	@%p5 bra 	$L__BB3_9;
	bra.uni 	$L__BB3_3;

$L__BB3_9:
	setp.lt.u32 	%p10, %r6, 3;
	mov.f32 	%f79, 0f00000000;
	mov.u32 	%r177, 0;
	@%p10 bra 	$L__BB3_12;

	sub.s32 	%r176, %r30, %r174;
	mul.lo.s32 	%r94, %r30, %r1;
	mul.wide.s32 	%rd34, %r94, 2;
	add.s64 	%rd35, %rd4, %rd34;
	add.s64 	%rd75, %rd35, 4;

$L__BB3_11:
	ld.global.nc.u16 	%rs12, [%rd75+-4];
	// begin inline asm
	{ mov.b32 %f46, {0,%rs12};}

	// end inline asm
	div.s32 	%r95, %r177, %r31;
	mad.lo.s32 	%r96, %r95, %r29, %r2;
	mul.wide.s32 	%rd36, %r96, 2;
	add.s64 	%rd37, %rd3, %rd36;
	ld.global.nc.u16 	%rs13, [%rd37];
	// begin inline asm
	{ mov.b32 %f47, {0,%rs13};}

	// end inline asm
	mad.lo.s32 	%r97, %r95, %r3, %r4;
	mul.wide.s32 	%rd38, %r97, 4;
	add.s64 	%rd39, %rd2, %rd38;
	ld.global.nc.u32 	%r98, [%rd39];
	shr.u32 	%r99, %r98, %r5;
	and.b32  	%r100, %r99, 15;
	shr.u32 	%r101, %r177, 3;
	mad.lo.s32 	%r102, %r101, %r29, %r2;
	mul.wide.s32 	%rd40, %r102, 4;
	add.s64 	%rd41, %rd1, %rd40;
	shl.b32 	%r103, %r177, 2;
	and.b32  	%r104, %r103, 16;
	ld.global.nc.u32 	%r105, [%rd41];
	shr.u32 	%r106, %r105, %r104;
	and.b32  	%r107, %r106, 15;
	sub.s32 	%r108, %r107, %r100;
	cvt.rn.f32.s32 	%f54, %r108;
	mul.ftz.f32 	%f55, %f47, %f54;
	fma.rn.ftz.f32 	%f56, %f46, %f55, %f79;
	ld.global.nc.u16 	%rs14, [%rd75+-2];
	// begin inline asm
	{ mov.b32 %f48, {0,%rs14};}

	// end inline asm
	add.s32 	%r109, %r177, 1;
	div.s32 	%r110, %r109, %r31;
	mad.lo.s32 	%r111, %r110, %r29, %r2;
	mul.wide.s32 	%rd42, %r111, 2;
	add.s64 	%rd43, %rd3, %rd42;
	ld.global.nc.u16 	%rs15, [%rd43];
	// begin inline asm
	{ mov.b32 %f49, {0,%rs15};}

	// end inline asm
	mad.lo.s32 	%r112, %r110, %r3, %r4;
	mul.wide.s32 	%rd44, %r112, 4;
	add.s64 	%rd45, %rd2, %rd44;
	ld.global.nc.u32 	%r113, [%rd45];
	shr.u32 	%r114, %r113, %r5;
	and.b32  	%r115, %r114, 15;
	shr.u32 	%r116, %r109, 3;
	mad.lo.s32 	%r117, %r116, %r29, %r2;
	mul.wide.s32 	%rd46, %r117, 4;
	add.s64 	%rd47, %rd1, %rd46;
	shl.b32 	%r118, %r109, 2;
	and.b32  	%r119, %r118, 20;
	ld.global.nc.u32 	%r120, [%rd47];
	shr.u32 	%r121, %r120, %r119;
	and.b32  	%r122, %r121, 15;
	sub.s32 	%r123, %r122, %r115;
	cvt.rn.f32.s32 	%f57, %r123;
	mul.ftz.f32 	%f58, %f49, %f57;
	fma.rn.ftz.f32 	%f59, %f48, %f58, %f56;
	ld.global.nc.u16 	%rs16, [%rd75];
	// begin inline asm
	{ mov.b32 %f50, {0,%rs16};}

	// end inline asm
	add.s32 	%r124, %r177, 2;
	div.s32 	%r125, %r124, %r31;
	mad.lo.s32 	%r126, %r125, %r29, %r2;
	mul.wide.s32 	%rd48, %r126, 2;
	add.s64 	%rd49, %rd3, %rd48;
	ld.global.nc.u16 	%rs17, [%rd49];
	// begin inline asm
	{ mov.b32 %f51, {0,%rs17};}

	// end inline asm
	mad.lo.s32 	%r127, %r125, %r3, %r4;
	mul.wide.s32 	%rd50, %r127, 4;
	add.s64 	%rd51, %rd2, %rd50;
	ld.global.nc.u32 	%r128, [%rd51];
	shr.u32 	%r129, %r128, %r5;
	and.b32  	%r130, %r129, 15;
	shr.u32 	%r131, %r124, 3;
	mad.lo.s32 	%r132, %r131, %r29, %r2;
	mul.wide.s32 	%rd52, %r132, 4;
	add.s64 	%rd53, %rd1, %rd52;
	shl.b32 	%r133, %r124, 2;
	and.b32  	%r134, %r133, 24;
	ld.global.nc.u32 	%r135, [%rd53];
	shr.u32 	%r136, %r135, %r134;
	and.b32  	%r137, %r136, 15;
	sub.s32 	%r138, %r137, %r130;
	cvt.rn.f32.s32 	%f60, %r138;
	mul.ftz.f32 	%f61, %f51, %f60;
	fma.rn.ftz.f32 	%f62, %f50, %f61, %f59;
	ld.global.nc.u16 	%rs18, [%rd75+2];
	// begin inline asm
	{ mov.b32 %f52, {0,%rs18};}

	// end inline asm
	add.s32 	%r139, %r177, 3;
	div.s32 	%r140, %r139, %r31;
	mad.lo.s32 	%r141, %r140, %r29, %r2;
	mul.wide.s32 	%rd54, %r141, 2;
	add.s64 	%rd55, %rd3, %rd54;
	ld.global.nc.u16 	%rs19, [%rd55];
	// begin inline asm
	{ mov.b32 %f53, {0,%rs19};}

	// end inline asm
	mad.lo.s32 	%r142, %r140, %r3, %r4;
	mul.wide.s32 	%rd56, %r142, 4;
	add.s64 	%rd57, %rd2, %rd56;
	ld.global.nc.u32 	%r143, [%rd57];
	shr.u32 	%r144, %r143, %r5;
	and.b32  	%r145, %r144, 15;
	shr.u32 	%r146, %r139, 3;
	mad.lo.s32 	%r147, %r146, %r29, %r2;
	mul.wide.s32 	%rd58, %r147, 4;
	add.s64 	%rd59, %rd1, %rd58;
	shl.b32 	%r148, %r139, 2;
	and.b32  	%r149, %r148, 28;
	ld.global.nc.u32 	%r150, [%rd59];
	shr.u32 	%r151, %r150, %r149;
	and.b32  	%r152, %r151, 15;
	sub.s32 	%r153, %r152, %r145;
	cvt.rn.f32.s32 	%f63, %r153;
	mul.ftz.f32 	%f64, %f53, %f63;
	fma.rn.ftz.f32 	%f79, %f52, %f64, %f62;
	add.s32 	%r177, %r177, 4;
	add.s64 	%rd75, %rd75, 8;
	add.s32 	%r176, %r176, -4;
	setp.ne.s32 	%p11, %r176, 0;
	@%p11 bra 	$L__BB3_11;

$L__BB3_12:
	setp.eq.s32 	%p12, %r174, 0;
	@%p12 bra 	$L__BB3_15;

	mad.lo.s32 	%r154, %r30, %r1, %r177;
	mul.wide.s32 	%rd60, %r154, 2;
	add.s64 	%rd76, %rd4, %rd60;

$L__BB3_14:
	.pragma "nounroll";
	ld.global.nc.u16 	%rs20, [%rd76];
	// begin inline asm
	{ mov.b32 %f65, {0,%rs20};}

	// end inline asm
	div.s32 	%r155, %r177, %r31;
	mad.lo.s32 	%r156, %r155, %r29, %r2;
	mul.wide.s32 	%rd61, %r156, 2;
	add.s64 	%rd62, %rd3, %rd61;
	ld.global.nc.u16 	%rs21, [%rd62];
	// begin inline asm
	{ mov.b32 %f66, {0,%rs21};}

	// end inline asm
	mad.lo.s32 	%r157, %r155, %r3, %r4;
	mul.wide.s32 	%rd63, %r157, 4;
	add.s64 	%rd64, %rd2, %rd63;
	ld.global.nc.u32 	%r158, [%rd64];
	shr.u32 	%r159, %r158, %r5;
	and.b32  	%r160, %r159, 15;
	shr.u32 	%r161, %r177, 3;
	mad.lo.s32 	%r162, %r161, %r29, %r2;
	mul.wide.s32 	%rd65, %r162, 4;
	add.s64 	%rd66, %rd1, %rd65;
	shl.b32 	%r163, %r177, 2;
	and.b32  	%r164, %r163, 28;
	ld.global.nc.u32 	%r165, [%rd66];
	shr.u32 	%r166, %r165, %r164;
	and.b32  	%r167, %r166, 15;
	sub.s32 	%r168, %r167, %r160;
	cvt.rn.f32.s32 	%f67, %r168;
	mul.ftz.f32 	%f68, %f66, %f67;
	fma.rn.ftz.f32 	%f79, %f65, %f68, %f79;
	add.s32 	%r177, %r177, 1;
	add.s64 	%rd76, %rd76, 2;
	add.s32 	%r174, %r174, -1;
	setp.ne.s32 	%p13, %r174, 0;
	@%p13 bra 	$L__BB3_14;
	bra.uni 	$L__BB3_15;

$L__BB3_3:
	setp.lt.u32 	%p6, %r6, 3;
	mul.wide.s32 	%rd25, %r2, 2;
	add.s64 	%rd5, %rd3, %rd25;
	mul.wide.s32 	%rd26, %r4, 4;
	add.s64 	%rd6, %rd2, %rd26;
	mov.f32 	%f79, 0f00000000;
	mov.u32 	%r172, 0;
	@%p6 bra 	$L__BB3_6;

	sub.s32 	%r171, %r30, %r174;
	mul.lo.s32 	%r53, %r30, %r1;
	mul.wide.s32 	%rd27, %r53, 2;
	add.s64 	%rd28, %rd4, %rd27;
	add.s64 	%rd73, %rd28, 4;

$L__BB3_5:
	ld.global.nc.u16 	%rs2, [%rd73+-4];
	// begin inline asm
	{ mov.b32 %f20, {0,%rs2};}

	// end inline asm
	ld.global.nc.u16 	%rs3, [%rd5];
	// begin inline asm
	{ mov.b32 %f21, {0,%rs3};}

	// end inline asm
	ld.global.nc.u32 	%r54, [%rd6];
	shr.u32 	%r55, %r54, %r5;
	and.b32  	%r56, %r55, 15;
	shr.u32 	%r57, %r172, 3;
	mad.lo.s32 	%r58, %r57, %r29, %r2;
	mul.wide.s32 	%rd29, %r58, 4;
	add.s64 	%rd30, %rd1, %rd29;
	shl.b32 	%r59, %r172, 2;
	and.b32  	%r60, %r59, 16;
	ld.global.nc.u32 	%r61, [%rd30];
	shr.u32 	%r62, %r61, %r60;
	and.b32  	%r63, %r62, 15;
	sub.s32 	%r64, %r63, %r56;
	cvt.rn.f32.s32 	%f28, %r64;
	mul.ftz.f32 	%f29, %f21, %f28;
	fma.rn.ftz.f32 	%f30, %f20, %f29, %f79;
	ld.global.nc.u16 	%rs4, [%rd73+-2];
	// begin inline asm
	{ mov.b32 %f22, {0,%rs4};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f23, {0,%rs3};}

	// end inline asm
	add.s32 	%r65, %r59, 4;
	and.b32  	%r66, %r65, 20;
	shr.u32 	%r67, %r61, %r66;
	and.b32  	%r68, %r67, 15;
	sub.s32 	%r69, %r68, %r56;
	cvt.rn.f32.s32 	%f31, %r69;
	mul.ftz.f32 	%f32, %f23, %f31;
	fma.rn.ftz.f32 	%f33, %f22, %f32, %f30;
	ld.global.nc.u16 	%rs6, [%rd73];
	// begin inline asm
	{ mov.b32 %f24, {0,%rs6};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f25, {0,%rs3};}

	// end inline asm
	add.s32 	%r70, %r59, 8;
	and.b32  	%r71, %r70, 24;
	shr.u32 	%r72, %r61, %r71;
	and.b32  	%r73, %r72, 15;
	sub.s32 	%r74, %r73, %r56;
	cvt.rn.f32.s32 	%f34, %r74;
	mul.ftz.f32 	%f35, %f25, %f34;
	fma.rn.ftz.f32 	%f36, %f24, %f35, %f33;
	ld.global.nc.u16 	%rs8, [%rd73+2];
	// begin inline asm
	{ mov.b32 %f26, {0,%rs8};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f27, {0,%rs3};}

	// end inline asm
	add.s32 	%r75, %r59, 12;
	and.b32  	%r76, %r75, 28;
	shr.u32 	%r77, %r61, %r76;
	and.b32  	%r78, %r77, 15;
	sub.s32 	%r79, %r78, %r56;
	cvt.rn.f32.s32 	%f37, %r79;
	mul.ftz.f32 	%f38, %f27, %f37;
	fma.rn.ftz.f32 	%f79, %f26, %f38, %f36;
	add.s32 	%r172, %r172, 4;
	add.s64 	%rd73, %rd73, 8;
	add.s32 	%r171, %r171, -4;
	setp.ne.s32 	%p7, %r171, 0;
	@%p7 bra 	$L__BB3_5;

$L__BB3_6:
	setp.eq.s32 	%p8, %r174, 0;
	@%p8 bra 	$L__BB3_15;

	mad.lo.s32 	%r80, %r30, %r1, %r172;
	mul.wide.s32 	%rd31, %r80, 2;
	add.s64 	%rd74, %rd4, %rd31;

$L__BB3_8:
	.pragma "nounroll";
	ld.global.nc.u16 	%rs10, [%rd74];
	// begin inline asm
	{ mov.b32 %f39, {0,%rs10};}

	// end inline asm
	ld.global.nc.u16 	%rs11, [%rd5];
	// begin inline asm
	{ mov.b32 %f40, {0,%rs11};}

	// end inline asm
	ld.global.nc.u32 	%r81, [%rd6];
	shr.u32 	%r82, %r81, %r5;
	and.b32  	%r83, %r82, 15;
	shr.u32 	%r84, %r172, 3;
	mad.lo.s32 	%r85, %r84, %r29, %r2;
	mul.wide.s32 	%rd32, %r85, 4;
	add.s64 	%rd33, %rd1, %rd32;
	shl.b32 	%r86, %r172, 2;
	and.b32  	%r87, %r86, 28;
	ld.global.nc.u32 	%r88, [%rd33];
	shr.u32 	%r89, %r88, %r87;
	and.b32  	%r90, %r89, 15;
	sub.s32 	%r91, %r90, %r83;
	cvt.rn.f32.s32 	%f41, %r91;
	mul.ftz.f32 	%f42, %f40, %f41;
	fma.rn.ftz.f32 	%f79, %f39, %f42, %f79;
	add.s32 	%r172, %r172, 1;
	add.s64 	%rd74, %rd74, 2;
	add.s32 	%r174, %r174, -1;
	setp.eq.s32 	%p9, %r174, 0;
	@%p9 bra 	$L__BB3_15;
	bra.uni 	$L__BB3_8;

$L__BB3_15:
	setp.eq.s64 	%p14, %rd20, 0;
	setp.eq.s16 	%p15, %rs1, 0;
	or.pred  	%p16, %p14, %p15;
	@%p16 bra 	$L__BB3_17;

	cvta.to.global.u64 	%rd67, %rd20;
	mul.wide.s32 	%rd68, %r2, 2;
	add.s64 	%rd69, %rd67, %rd68;
	ld.global.nc.u16 	%rs22, [%rd69];
	// begin inline asm
	{ mov.b32 %f69, {0,%rs22};}

	// end inline asm
	add.ftz.f32 	%f79, %f79, %f69;

$L__BB3_17:
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs23, %f79;}

	// end inline asm
	mad.lo.s32 	%r169, %r1, %r29, %r2;
	cvta.to.global.u64 	%rd70, %rd19;
	mul.wide.s32 	%rd71, %r169, 2;
	add.s64 	%rd72, %rd70, %rd71;
	st.global.u16 	[%rd72], %rs23;

$L__BB3_18:
	ret;

}
	// .globl	gptq_gemm_int4_bf16_tiled
.visible .entry gptq_gemm_int4_bf16_tiled(
	.param .u64 gptq_gemm_int4_bf16_tiled_param_0,
	.param .u64 gptq_gemm_int4_bf16_tiled_param_1,
	.param .u64 gptq_gemm_int4_bf16_tiled_param_2,
	.param .u64 gptq_gemm_int4_bf16_tiled_param_3,
	.param .u64 gptq_gemm_int4_bf16_tiled_param_4,
	.param .u64 gptq_gemm_int4_bf16_tiled_param_5,
	.param .u32 gptq_gemm_int4_bf16_tiled_param_6,
	.param .u32 gptq_gemm_int4_bf16_tiled_param_7,
	.param .u32 gptq_gemm_int4_bf16_tiled_param_8,
	.param .u32 gptq_gemm_int4_bf16_tiled_param_9,
	.param .u32 gptq_gemm_int4_bf16_tiled_param_10,
	.param .u8 gptq_gemm_int4_bf16_tiled_param_11
)
{
	.reg .pred 	%p<34>;
	.reg .b16 	%rs<21>;
	.reg .f32 	%f<103>;
	.reg .b32 	%r<272>;
	.reg .b64 	%rd<79>;
	// demoted variable
	.shared .align 4 .b8 _ZZ25gptq_gemm_int4_bf16_tiledE10input_tile[2112];

	ld.param.s8 	%rs3, [gptq_gemm_int4_bf16_tiled_param_11];
	ld.param.u64 	%rd5, [gptq_gemm_int4_bf16_tiled_param_0];
	ld.param.u64 	%rd6, [gptq_gemm_int4_bf16_tiled_param_1];
	ld.param.u64 	%rd10, [gptq_gemm_int4_bf16_tiled_param_2];
	ld.param.u64 	%rd7, [gptq_gemm_int4_bf16_tiled_param_3];
	ld.param.u64 	%rd8, [gptq_gemm_int4_bf16_tiled_param_4];
	ld.param.u64 	%rd9, [gptq_gemm_int4_bf16_tiled_param_5];
	ld.param.u32 	%r37, [gptq_gemm_int4_bf16_tiled_param_6];
	ld.param.u32 	%r38, [gptq_gemm_int4_bf16_tiled_param_7];
	ld.param.u32 	%r39, [gptq_gemm_int4_bf16_tiled_param_8];
	ld.param.u32 	%r40, [gptq_gemm_int4_bf16_tiled_param_9];
	cvta.to.global.u64 	%rd1, %rd10;
	mov.u32 	%r42, %ctaid.y;
	shl.b32 	%r43, %r42, 4;
	mov.u32 	%r1, %tid.y;
	add.s32 	%r2, %r43, %r1;
	mov.u32 	%r44, %ctaid.x;
	shl.b32 	%r45, %r44, 4;
	mov.u32 	%r3, %tid.x;
	add.s32 	%r4, %r45, %r3;
	add.s32 	%r46, %r38, 7;
	shr.s32 	%r47, %r46, 31;
	shr.u32 	%r48, %r47, 29;
	add.s32 	%r49, %r46, %r48;
	shr.s32 	%r5, %r49, 3;
	setp.ge.s32 	%p2, %r4, %r38;
	mov.u32 	%r262, 0;
	@%p2 bra 	$L__BB4_2;

	shr.s32 	%r50, %r4, 31;
	shr.u32 	%r51, %r50, 29;
	add.s32 	%r52, %r4, %r51;
	shr.s32 	%r262, %r52, 3;

$L__BB4_2:
	setp.lt.s32 	%p3, %r39, 1;
	mov.f32 	%f100, 0f00000000;
	@%p3 bra 	$L__BB4_26;

	setp.lt.s32 	%p4, %r4, %r38;
	setp.lt.s32 	%p5, %r2, %r37;
	and.pred  	%p1, %p5, %p4;
	mul.lo.s32 	%r8, %r2, %r39;
	shr.s32 	%r55, %r4, 31;
	shr.u32 	%r56, %r55, 29;
	add.s32 	%r57, %r4, %r56;
	and.b32  	%r58, %r57, 1073741816;
	sub.s32 	%r59, %r4, %r58;
	shl.b32 	%r9, %r59, 2;
	not.b32 	%r10, %r39;
	cvta.to.global.u64 	%rd11, %rd7;
	mul.wide.s32 	%rd12, %r4, 2;
	add.s64 	%rd2, %rd11, %rd12;
	cvta.to.global.u64 	%rd13, %rd8;
	mul.wide.s32 	%rd14, %r262, 4;
	add.s64 	%rd3, %rd13, %rd14;
	cvta.to.global.u64 	%rd4, %rd6;
	mov.u32 	%r263, 0;
	not.pred 	%p14, %p1;
	mov.u32 	%r264, %r263;

$L__BB4_4:
	add.s32 	%r60, %r263, %r10;
	max.s32 	%r61, %r60, -33;
	not.b32 	%r62, %r61;
	max.s32 	%r13, %r62, 1;
	add.s32 	%r63, %r3, %r264;
	setp.ge.s32 	%p6, %r63, %r39;
	setp.gt.s32 	%p7, %r3, 31;
	or.pred  	%p8, %p7, %p6;
	@%p8 bra 	$L__BB4_9;

	add.s32 	%r14, %r264, %r8;
	mov.u32 	%r265, %r3;

$L__BB4_6:
	setp.ge.s32 	%p9, %r2, %r37;
	mov.f32 	%f93, 0f00000000;
	@%p9 bra 	$L__BB4_8;

	add.s32 	%r64, %r14, %r265;
	mul.wide.s32 	%rd15, %r64, 2;
	add.s64 	%rd16, %rd4, %rd15;
	ld.global.nc.u16 	%rs4, [%rd16];
	// begin inline asm
	{ mov.b32 %f93, {0,%rs4};}

	// end inline asm

$L__BB4_8:
	mov.u32 	%r65, _ZZ25gptq_gemm_int4_bf16_tiledE10input_tile;
	mad.lo.s32 	%r66, %r1, 132, %r65;
	shl.b32 	%r67, %r265, 2;
	add.s32 	%r68, %r66, %r67;
	st.shared.f32 	[%r68], %f93;
	add.s32 	%r16, %r265, 16;
	add.s32 	%r69, %r16, %r264;
	setp.lt.s32 	%p10, %r69, %r39;
	setp.lt.s32 	%p11, %r265, 16;
	and.pred  	%p12, %p11, %p10;
	mov.u32 	%r265, %r16;
	@%p12 bra 	$L__BB4_6;

$L__BB4_9:
	bar.sync 	0;
	setp.ge.s32 	%p13, %r264, %r39;
	or.pred  	%p15, %p14, %p13;
	@%p15 bra 	$L__BB4_25;

	setp.gt.s32 	%p16, %r40, 0;
	and.b32  	%r17, %r13, 3;
	@%p16 bra 	$L__BB4_18;
	bra.uni 	$L__BB4_11;

$L__BB4_18:
	add.s32 	%r148, %r13, -1;
	setp.lt.u32 	%p22, %r148, 3;
	mov.u32 	%r271, 0;
	@%p22 bra 	$L__BB4_21;

	sub.s32 	%r270, %r13, %r17;

$L__BB4_20:
	add.s32 	%r150, %r271, %r264;
	div.s32 	%r151, %r150, %r40;
	mul.lo.s32 	%r152, %r151, %r38;
	mul.wide.s32 	%rd31, %r152, 2;
	add.s64 	%rd32, %rd2, %rd31;
	ld.global.nc.u16 	%rs12, [%rd32];
	// begin inline asm
	{ mov.b32 %f59, {0,%rs12};}

	// end inline asm
	mul.lo.s32 	%r153, %r151, %r5;
	mul.wide.s32 	%rd33, %r153, 4;
	add.s64 	%rd34, %rd3, %rd33;
	ld.global.nc.u32 	%r154, [%rd34];
	shr.u32 	%r155, %r154, %r9;
	and.b32  	%r156, %r155, 15;
	shr.u32 	%r157, %r150, 3;
	mad.lo.s32 	%r158, %r157, %r38, %r4;
	mul.wide.s32 	%rd35, %r158, 4;
	add.s64 	%rd36, %rd1, %rd35;
	shl.b32 	%r159, %r150, 2;
	and.b32  	%r160, %r159, 16;
	ld.global.nc.u32 	%r161, [%rd36];
	shr.u32 	%r162, %r161, %r160;
	and.b32  	%r163, %r162, 15;
	sub.s32 	%r164, %r163, %r156;
	cvt.rn.f32.s32 	%f63, %r164;
	mul.ftz.f32 	%f64, %f59, %f63;
	mov.u32 	%r165, _ZZ25gptq_gemm_int4_bf16_tiledE10input_tile;
	mad.lo.s32 	%r166, %r1, 132, %r165;
	shl.b32 	%r167, %r271, 2;
	add.s32 	%r168, %r166, %r167;
	ld.shared.f32 	%f65, [%r168];
	fma.rn.ftz.f32 	%f66, %f65, %f64, %f100;
	add.s32 	%r169, %r150, 1;
	div.s32 	%r170, %r169, %r40;
	mul.lo.s32 	%r171, %r170, %r38;
	mul.wide.s32 	%rd37, %r171, 2;
	add.s64 	%rd38, %rd2, %rd37;
	ld.global.nc.u16 	%rs13, [%rd38];
	// begin inline asm
	{ mov.b32 %f60, {0,%rs13};}

	// end inline asm
	mul.lo.s32 	%r172, %r170, %r5;
	mul.wide.s32 	%rd39, %r172, 4;
	add.s64 	%rd40, %rd3, %rd39;
	ld.global.nc.u32 	%r173, [%rd40];
	shr.u32 	%r174, %r173, %r9;
	and.b32  	%r175, %r174, 15;
	shr.u32 	%r176, %r169, 3;
	mad.lo.s32 	%r177, %r176, %r38, %r4;
	mul.wide.s32 	%rd41, %r177, 4;
	add.s64 	%rd42, %rd1, %rd41;
	shl.b32 	%r178, %r169, 2;
	and.b32  	%r179, %r178, 20;
	ld.global.nc.u32 	%r180, [%rd42];
	shr.u32 	%r181, %r180, %r179;
	and.b32  	%r182, %r181, 15;
	sub.s32 	%r183, %r182, %r175;
	cvt.rn.f32.s32 	%f67, %r183;
	mul.ftz.f32 	%f68, %f60, %f67;
	ld.shared.f32 	%f69, [%r168+4];
	fma.rn.ftz.f32 	%f70, %f69, %f68, %f66;
	add.s32 	%r184, %r150, 2;
	div.s32 	%r185, %r184, %r40;
	mul.lo.s32 	%r186, %r185, %r38;
	mul.wide.s32 	%rd43, %r186, 2;
	add.s64 	%rd44, %rd2, %rd43;
	ld.global.nc.u16 	%rs14, [%rd44];
	// begin inline asm
	{ mov.b32 %f61, {0,%rs14};}

	// end inline asm
	mul.lo.s32 	%r187, %r185, %r5;
	mul.wide.s32 	%rd45, %r187, 4;
	add.s64 	%rd46, %rd3, %rd45;
	ld.global.nc.u32 	%r188, [%rd46];
	shr.u32 	%r189, %r188, %r9;
	and.b32  	%r190, %r189, 15;
	shr.u32 	%r191, %r184, 3;
	mad.lo.s32 	%r192, %r191, %r38, %r4;
	mul.wide.s32 	%rd47, %r192, 4;
	add.s64 	%rd48, %rd1, %rd47;
	shl.b32 	%r193, %r184, 2;
	and.b32  	%r194, %r193, 24;
	ld.global.nc.u32 	%r195, [%rd48];
	shr.u32 	%r196, %r195, %r194;
	and.b32  	%r197, %r196, 15;
	sub.s32 	%r198, %r197, %r190;
	cvt.rn.f32.s32 	%f71, %r198;
	mul.ftz.f32 	%f72, %f61, %f71;
	ld.shared.f32 	%f73, [%r168+8];
	fma.rn.ftz.f32 	%f74, %f73, %f72, %f70;
	add.s32 	%r199, %r150, 3;
	div.s32 	%r200, %r199, %r40;
	mul.lo.s32 	%r201, %r200, %r38;
	mul.wide.s32 	%rd49, %r201, 2;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.nc.u16 	%rs15, [%rd50];
	// begin inline asm
	{ mov.b32 %f62, {0,%rs15};}

	// end inline asm
	mul.lo.s32 	%r202, %r200, %r5;
	mul.wide.s32 	%rd51, %r202, 4;
	add.s64 	%rd52, %rd3, %rd51;
	ld.global.nc.u32 	%r203, [%rd52];
	shr.u32 	%r204, %r203, %r9;
	and.b32  	%r205, %r204, 15;
	shr.u32 	%r206, %r199, 3;
	mad.lo.s32 	%r207, %r206, %r38, %r4;
	mul.wide.s32 	%rd53, %r207, 4;
	add.s64 	%rd54, %rd1, %rd53;
	shl.b32 	%r208, %r199, 2;
	and.b32  	%r209, %r208, 28;
	ld.global.nc.u32 	%r210, [%rd54];
	shr.u32 	%r211, %r210, %r209;
	and.b32  	%r212, %r211, 15;
	sub.s32 	%r213, %r212, %r205;
	cvt.rn.f32.s32 	%f75, %r213;
	mul.ftz.f32 	%f76, %f62, %f75;
	ld.shared.f32 	%f77, [%r168+12];
	fma.rn.ftz.f32 	%f100, %f77, %f76, %f74;
	add.s32 	%r271, %r271, 4;
	add.s32 	%r270, %r270, -4;
	setp.ne.s32 	%p23, %r270, 0;
	@%p23 bra 	$L__BB4_20;

$L__BB4_21:
	setp.eq.s32 	%p24, %r17, 0;
	@%p24 bra 	$L__BB4_25;

	add.s32 	%r33, %r271, %r264;
	div.s32 	%r214, %r33, %r40;
	mul.lo.s32 	%r215, %r214, %r38;
	mul.wide.s32 	%rd55, %r215, 2;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.nc.u16 	%rs16, [%rd56];
	// begin inline asm
	{ mov.b32 %f78, {0,%rs16};}

	// end inline asm
	mul.lo.s32 	%r216, %r214, %r5;
	mul.wide.s32 	%rd57, %r216, 4;
	add.s64 	%rd58, %rd3, %rd57;
	ld.global.nc.u32 	%r217, [%rd58];
	shr.u32 	%r218, %r217, %r9;
	and.b32  	%r219, %r218, 15;
	shr.u32 	%r220, %r33, 3;
	mad.lo.s32 	%r221, %r220, %r38, %r4;
	mul.wide.s32 	%rd59, %r221, 4;
	add.s64 	%rd60, %rd1, %rd59;
	shl.b32 	%r222, %r33, 2;
	and.b32  	%r223, %r222, 28;
	ld.global.nc.u32 	%r224, [%rd60];
	shr.u32 	%r225, %r224, %r223;
	and.b32  	%r226, %r225, 15;
	sub.s32 	%r227, %r226, %r219;
	cvt.rn.f32.s32 	%f79, %r227;
	mul.ftz.f32 	%f80, %f78, %f79;
	mov.u32 	%r228, _ZZ25gptq_gemm_int4_bf16_tiledE10input_tile;
	mad.lo.s32 	%r229, %r1, 132, %r228;
	shl.b32 	%r230, %r271, 2;
	add.s32 	%r34, %r229, %r230;
	ld.shared.f32 	%f81, [%r34];
	fma.rn.ftz.f32 	%f100, %f81, %f80, %f100;
	setp.eq.s32 	%p25, %r17, 1;
	@%p25 bra 	$L__BB4_25;

	add.s32 	%r231, %r33, 1;
	div.s32 	%r232, %r231, %r40;
	mul.lo.s32 	%r233, %r232, %r38;
	mul.wide.s32 	%rd61, %r233, 2;
	add.s64 	%rd62, %rd2, %rd61;
	ld.global.nc.u16 	%rs17, [%rd62];
	// begin inline asm
	{ mov.b32 %f82, {0,%rs17};}

	// end inline asm
	mul.lo.s32 	%r234, %r232, %r5;
	mul.wide.s32 	%rd63, %r234, 4;
	add.s64 	%rd64, %rd3, %rd63;
	ld.global.nc.u32 	%r235, [%rd64];
	shr.u32 	%r236, %r235, %r9;
	and.b32  	%r237, %r236, 15;
	shr.u32 	%r238, %r231, 3;
	mad.lo.s32 	%r239, %r238, %r38, %r4;
	mul.wide.s32 	%rd65, %r239, 4;
	add.s64 	%rd66, %rd1, %rd65;
	shl.b32 	%r240, %r231, 2;
	and.b32  	%r241, %r240, 28;
	ld.global.nc.u32 	%r242, [%rd66];
	shr.u32 	%r243, %r242, %r241;
	and.b32  	%r244, %r243, 15;
	sub.s32 	%r245, %r244, %r237;
	cvt.rn.f32.s32 	%f83, %r245;
	mul.ftz.f32 	%f84, %f82, %f83;
	ld.shared.f32 	%f85, [%r34+4];
	fma.rn.ftz.f32 	%f100, %f85, %f84, %f100;
	setp.eq.s32 	%p26, %r17, 2;
	@%p26 bra 	$L__BB4_25;

	add.s32 	%r246, %r33, 2;
	div.s32 	%r247, %r246, %r40;
	mul.lo.s32 	%r248, %r247, %r38;
	mul.wide.s32 	%rd67, %r248, 2;
	add.s64 	%rd68, %rd2, %rd67;
	ld.global.nc.u16 	%rs18, [%rd68];
	// begin inline asm
	{ mov.b32 %f86, {0,%rs18};}

	// end inline asm
	mul.lo.s32 	%r249, %r247, %r5;
	mul.wide.s32 	%rd69, %r249, 4;
	add.s64 	%rd70, %rd3, %rd69;
	ld.global.nc.u32 	%r250, [%rd70];
	shr.u32 	%r251, %r250, %r9;
	and.b32  	%r252, %r251, 15;
	shr.u32 	%r253, %r246, 3;
	mad.lo.s32 	%r254, %r253, %r38, %r4;
	mul.wide.s32 	%rd71, %r254, 4;
	add.s64 	%rd72, %rd1, %rd71;
	shl.b32 	%r255, %r246, 2;
	and.b32  	%r256, %r255, 28;
	ld.global.nc.u32 	%r257, [%rd72];
	shr.u32 	%r258, %r257, %r256;
	and.b32  	%r259, %r258, 15;
	sub.s32 	%r260, %r259, %r252;
	cvt.rn.f32.s32 	%f87, %r260;
	mul.ftz.f32 	%f88, %f86, %f87;
	ld.shared.f32 	%f89, [%r34+8];
	fma.rn.ftz.f32 	%f100, %f89, %f88, %f100;
	bra.uni 	$L__BB4_25;

$L__BB4_11:
	add.s32 	%r71, %r13, -1;
	setp.lt.u32 	%p17, %r71, 3;
	mov.u32 	%r268, 0;
	@%p17 bra 	$L__BB4_14;

	sub.s32 	%r267, %r13, %r17;
	ld.global.nc.u16 	%rs1, [%rd2];

$L__BB4_13:
	// begin inline asm
	{ mov.b32 %f27, {0,%rs1};}

	// end inline asm
	ld.global.nc.u32 	%r73, [%rd3];
	shr.u32 	%r74, %r73, %r9;
	and.b32  	%r75, %r74, 15;
	add.s32 	%r76, %r268, %r264;
	shr.u32 	%r77, %r76, 3;
	mad.lo.s32 	%r78, %r77, %r38, %r4;
	mul.wide.s32 	%rd17, %r78, 4;
	add.s64 	%rd18, %rd1, %rd17;
	shl.b32 	%r79, %r76, 2;
	and.b32  	%r80, %r79, 16;
	ld.global.nc.u32 	%r81, [%rd18];
	shr.u32 	%r82, %r81, %r80;
	and.b32  	%r83, %r82, 15;
	sub.s32 	%r84, %r83, %r75;
	cvt.rn.f32.s32 	%f31, %r84;
	mul.ftz.f32 	%f32, %f27, %f31;
	mov.u32 	%r85, _ZZ25gptq_gemm_int4_bf16_tiledE10input_tile;
	mad.lo.s32 	%r86, %r1, 132, %r85;
	shl.b32 	%r87, %r268, 2;
	add.s32 	%r88, %r86, %r87;
	ld.shared.f32 	%f33, [%r88];
	fma.rn.ftz.f32 	%f34, %f33, %f32, %f100;
	add.s32 	%r89, %r76, 1;
	// begin inline asm
	{ mov.b32 %f28, {0,%rs1};}

	// end inline asm
	shr.u32 	%r90, %r89, 3;
	mad.lo.s32 	%r91, %r90, %r38, %r4;
	mul.wide.s32 	%rd19, %r91, 4;
	add.s64 	%rd20, %rd1, %rd19;
	shl.b32 	%r92, %r89, 2;
	and.b32  	%r93, %r92, 20;
	ld.global.nc.u32 	%r94, [%rd20];
	shr.u32 	%r95, %r94, %r93;
	and.b32  	%r96, %r95, 15;
	sub.s32 	%r97, %r96, %r75;
	cvt.rn.f32.s32 	%f35, %r97;
	mul.ftz.f32 	%f36, %f28, %f35;
	ld.shared.f32 	%f37, [%r88+4];
	fma.rn.ftz.f32 	%f38, %f37, %f36, %f34;
	add.s32 	%r98, %r76, 2;
	// begin inline asm
	{ mov.b32 %f29, {0,%rs1};}

	// end inline asm
	shr.u32 	%r99, %r98, 3;
	mad.lo.s32 	%r100, %r99, %r38, %r4;
	mul.wide.s32 	%rd21, %r100, 4;
	add.s64 	%rd22, %rd1, %rd21;
	shl.b32 	%r101, %r98, 2;
	and.b32  	%r102, %r101, 24;
	ld.global.nc.u32 	%r103, [%rd22];
	shr.u32 	%r104, %r103, %r102;
	and.b32  	%r105, %r104, 15;
	sub.s32 	%r106, %r105, %r75;
	cvt.rn.f32.s32 	%f39, %r106;
	mul.ftz.f32 	%f40, %f29, %f39;
	ld.shared.f32 	%f41, [%r88+8];
	fma.rn.ftz.f32 	%f42, %f41, %f40, %f38;
	add.s32 	%r107, %r76, 3;
	// begin inline asm
	{ mov.b32 %f30, {0,%rs1};}

	// end inline asm
	shr.u32 	%r108, %r107, 3;
	mad.lo.s32 	%r109, %r108, %r38, %r4;
	mul.wide.s32 	%rd23, %r109, 4;
	add.s64 	%rd24, %rd1, %rd23;
	shl.b32 	%r110, %r107, 2;
	and.b32  	%r111, %r110, 28;
	ld.global.nc.u32 	%r112, [%rd24];
	shr.u32 	%r113, %r112, %r111;
	and.b32  	%r114, %r113, 15;
	sub.s32 	%r115, %r114, %r75;
	cvt.rn.f32.s32 	%f43, %r115;
	mul.ftz.f32 	%f44, %f30, %f43;
	ld.shared.f32 	%f45, [%r88+12];
	fma.rn.ftz.f32 	%f100, %f45, %f44, %f42;
	add.s32 	%r268, %r268, 4;
	add.s32 	%r267, %r267, -4;
	setp.ne.s32 	%p18, %r267, 0;
	@%p18 bra 	$L__BB4_13;

$L__BB4_14:
	setp.eq.s32 	%p19, %r17, 0;
	@%p19 bra 	$L__BB4_25;

	add.s32 	%r24, %r268, %r264;
	ld.global.nc.u16 	%rs2, [%rd2];
	// begin inline asm
	{ mov.b32 %f46, {0,%rs2};}

	// end inline asm
	ld.global.nc.u32 	%r116, [%rd3];
	shr.u32 	%r117, %r116, %r9;
	and.b32  	%r25, %r117, 15;
	shr.u32 	%r118, %r24, 3;
	mad.lo.s32 	%r119, %r118, %r38, %r4;
	mul.wide.s32 	%rd25, %r119, 4;
	add.s64 	%rd26, %rd1, %rd25;
	shl.b32 	%r120, %r24, 2;
	and.b32  	%r121, %r120, 28;
	ld.global.nc.u32 	%r122, [%rd26];
	shr.u32 	%r123, %r122, %r121;
	and.b32  	%r124, %r123, 15;
	sub.s32 	%r125, %r124, %r25;
	cvt.rn.f32.s32 	%f47, %r125;
	mul.ftz.f32 	%f48, %f46, %f47;
	mov.u32 	%r126, _ZZ25gptq_gemm_int4_bf16_tiledE10input_tile;
	mad.lo.s32 	%r127, %r1, 132, %r126;
	shl.b32 	%r128, %r268, 2;
	add.s32 	%r26, %r127, %r128;
	ld.shared.f32 	%f49, [%r26];
	fma.rn.ftz.f32 	%f100, %f49, %f48, %f100;
	setp.eq.s32 	%p20, %r17, 1;
	@%p20 bra 	$L__BB4_25;

	add.s32 	%r129, %r24, 1;
	// begin inline asm
	{ mov.b32 %f50, {0,%rs2};}

	// end inline asm
	shr.u32 	%r130, %r129, 3;
	mad.lo.s32 	%r131, %r130, %r38, %r4;
	mul.wide.s32 	%rd27, %r131, 4;
	add.s64 	%rd28, %rd1, %rd27;
	shl.b32 	%r132, %r129, 2;
	and.b32  	%r133, %r132, 28;
	ld.global.nc.u32 	%r134, [%rd28];
	shr.u32 	%r135, %r134, %r133;
	and.b32  	%r136, %r135, 15;
	sub.s32 	%r137, %r136, %r25;
	cvt.rn.f32.s32 	%f51, %r137;
	mul.ftz.f32 	%f52, %f50, %f51;
	ld.shared.f32 	%f53, [%r26+4];
	fma.rn.ftz.f32 	%f100, %f53, %f52, %f100;
	setp.eq.s32 	%p21, %r17, 2;
	@%p21 bra 	$L__BB4_25;

	add.s32 	%r138, %r24, 2;
	// begin inline asm
	{ mov.b32 %f54, {0,%rs2};}

	// end inline asm
	shr.u32 	%r139, %r138, 3;
	mad.lo.s32 	%r140, %r139, %r38, %r4;
	mul.wide.s32 	%rd29, %r140, 4;
	add.s64 	%rd30, %rd1, %rd29;
	shl.b32 	%r141, %r138, 2;
	and.b32  	%r142, %r141, 28;
	ld.global.nc.u32 	%r143, [%rd30];
	shr.u32 	%r144, %r143, %r142;
	and.b32  	%r145, %r144, 15;
	sub.s32 	%r146, %r145, %r25;
	cvt.rn.f32.s32 	%f55, %r146;
	mul.ftz.f32 	%f56, %f54, %f55;
	ld.shared.f32 	%f57, [%r26+8];
	fma.rn.ftz.f32 	%f100, %f57, %f56, %f100;

$L__BB4_25:
	bar.sync 	0;
	add.s32 	%r264, %r264, 32;
	setp.lt.s32 	%p27, %r264, %r39;
	add.s32 	%r263, %r263, 32;
	@%p27 bra 	$L__BB4_4;

$L__BB4_26:
	setp.ge.s32 	%p28, %r2, %r37;
	or.pred  	%p30, %p28, %p2;
	@%p30 bra 	$L__BB4_30;

	setp.eq.s16 	%p31, %rs3, 0;
	setp.eq.s64 	%p32, %rd9, 0;
	or.pred  	%p33, %p32, %p31;
	@%p33 bra 	$L__BB4_29;

	cvta.to.global.u64 	%rd73, %rd9;
	mul.wide.s32 	%rd74, %r4, 2;
	add.s64 	%rd75, %rd73, %rd74;
	ld.global.nc.u16 	%rs19, [%rd75];
	// begin inline asm
	{ mov.b32 %f90, {0,%rs19};}

	// end inline asm
	add.ftz.f32 	%f100, %f100, %f90;

$L__BB4_29:
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs20, %f100;}

	// end inline asm
	mad.lo.s32 	%r261, %r2, %r38, %r4;
	cvta.to.global.u64 	%rd76, %rd5;
	mul.wide.s32 	%rd77, %r261, 2;
	add.s64 	%rd78, %rd76, %rd77;
	st.global.u16 	[%rd78], %rs20;

$L__BB4_30:
	ret;

}

