//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32267302
// Cuda compilation tools, release 12.0, V12.0.140
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_89
.address_size 64

	// .globl	fused_swiglu_bf16

.visible .entry fused_swiglu_bf16(
	.param .u64 fused_swiglu_bf16_param_0,
	.param .u64 fused_swiglu_bf16_param_1,
	.param .u64 fused_swiglu_bf16_param_2,
	.param .u32 fused_swiglu_bf16_param_3
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<31>;
	.reg .f32 	%f<71>;
	.reg .b32 	%r<43>;
	.reg .b64 	%rd<35>;


	ld.param.u64 	%rd5, [fused_swiglu_bf16_param_0];
	ld.param.u64 	%rd6, [fused_swiglu_bf16_param_1];
	ld.param.u64 	%rd7, [fused_swiglu_bf16_param_2];
	ld.param.u32 	%r14, [fused_swiglu_bf16_param_3];
	mov.u32 	%r15, %ctaid.x;
	cvt.u64.u32 	%rd8, %r15;
	cvt.s64.s32 	%rd9, %r14;
	mul.lo.s64 	%rd1, %rd9, %rd8;
	shl.b64 	%rd10, %rd1, 1;
	add.s64 	%rd11, %rd6, %rd10;
	and.b64  	%rd12, %rd11, 15;
	setp.ne.s64 	%p4, %rd12, 0;
	mov.pred 	%p15, 0;
	@%p4 bra 	$L__BB0_3;

	add.s64 	%rd14, %rd7, %rd10;
	and.b64  	%rd15, %rd14, 15;
	setp.ne.s64 	%p6, %rd15, 0;
	@%p6 bra 	$L__BB0_3;

	add.s64 	%rd17, %rd5, %rd10;
	and.b64  	%rd18, %rd17, 15;
	setp.eq.s64 	%p15, %rd18, 0;

$L__BB0_3:
	cvta.to.global.u64 	%rd19, %rd6;
	add.s64 	%rd2, %rd19, %rd10;
	cvta.to.global.u64 	%rd21, %rd7;
	add.s64 	%rd3, %rd21, %rd10;
	cvta.to.global.u64 	%rd22, %rd5;
	add.s64 	%rd4, %rd22, %rd10;
	setp.gt.s32 	%p7, %r14, 7;
	and.pred  	%p8, %p7, %p15;
	@%p8 bra 	$L__BB0_7;
	bra.uni 	$L__BB0_4;

$L__BB0_7:
	shr.s32 	%r16, %r14, 31;
	shr.u32 	%r17, %r16, 29;
	add.s32 	%r18, %r14, %r17;
	shr.s32 	%r5, %r18, 3;
	mov.u32 	%r6, %tid.x;
	setp.ge.s32 	%p11, %r6, %r5;
	@%p11 bra 	$L__BB0_10;

	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r41, %r6;

$L__BB0_9:
	mul.wide.s32 	%rd27, %r41, 16;
	add.s64 	%rd28, %rd2, %rd27;
	ld.global.nc.v4.u32 	{%r19, %r20, %r21, %r22}, [%rd28];
	add.s64 	%rd29, %rd3, %rd27;
	ld.global.nc.v4.u32 	{%r27, %r28, %r29, %r30}, [%rd29];
	mov.b32 	{%rs4, %rs7}, %r19;
	// begin inline asm
	{ mov.b32 %f8, {0,%rs4};}

	// end inline asm
	mov.b32 	{%rs5, %rs8}, %r27;
	// begin inline asm
	{ mov.b32 %f9, {0,%rs5};}

	// end inline asm
	mul.ftz.f32 	%f32, %f8, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f33, %f32;
	add.ftz.f32 	%f34, %f33, 0f3F800000;
	div.approx.ftz.f32 	%f35, %f8, %f34;
	mul.ftz.f32 	%f10, %f9, %f35;
	// begin inline asm
	{ mov.b32 %f11, {0,%rs7};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f12, {0,%rs8};}

	// end inline asm
	mul.ftz.f32 	%f36, %f11, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f37, %f36;
	add.ftz.f32 	%f38, %f37, 0f3F800000;
	div.approx.ftz.f32 	%f39, %f11, %f38;
	mul.ftz.f32 	%f13, %f12, %f39;
	mov.b32 	{%rs10, %rs13}, %r20;
	// begin inline asm
	{ mov.b32 %f14, {0,%rs10};}

	// end inline asm
	mov.b32 	{%rs11, %rs14}, %r28;
	// begin inline asm
	{ mov.b32 %f15, {0,%rs11};}

	// end inline asm
	mul.ftz.f32 	%f40, %f14, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f41, %f40;
	add.ftz.f32 	%f42, %f41, 0f3F800000;
	div.approx.ftz.f32 	%f43, %f14, %f42;
	mul.ftz.f32 	%f16, %f15, %f43;
	// begin inline asm
	{ mov.b32 %f17, {0,%rs13};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f18, {0,%rs14};}

	// end inline asm
	mul.ftz.f32 	%f44, %f17, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f45, %f44;
	add.ftz.f32 	%f46, %f45, 0f3F800000;
	div.approx.ftz.f32 	%f47, %f17, %f46;
	mul.ftz.f32 	%f19, %f18, %f47;
	mov.b32 	{%rs16, %rs19}, %r21;
	// begin inline asm
	{ mov.b32 %f20, {0,%rs16};}

	// end inline asm
	mov.b32 	{%rs17, %rs20}, %r29;
	// begin inline asm
	{ mov.b32 %f21, {0,%rs17};}

	// end inline asm
	mul.ftz.f32 	%f48, %f20, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f49, %f48;
	add.ftz.f32 	%f50, %f49, 0f3F800000;
	div.approx.ftz.f32 	%f51, %f20, %f50;
	mul.ftz.f32 	%f22, %f21, %f51;
	// begin inline asm
	{ mov.b32 %f23, {0,%rs19};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f24, {0,%rs20};}

	// end inline asm
	mul.ftz.f32 	%f52, %f23, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f53, %f52;
	add.ftz.f32 	%f54, %f53, 0f3F800000;
	div.approx.ftz.f32 	%f55, %f23, %f54;
	mul.ftz.f32 	%f25, %f24, %f55;
	mov.b32 	{%rs22, %rs25}, %r22;
	// begin inline asm
	{ mov.b32 %f26, {0,%rs22};}

	// end inline asm
	mov.b32 	{%rs23, %rs26}, %r30;
	// begin inline asm
	{ mov.b32 %f27, {0,%rs23};}

	// end inline asm
	mul.ftz.f32 	%f56, %f26, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f57, %f56;
	add.ftz.f32 	%f58, %f57, 0f3F800000;
	div.approx.ftz.f32 	%f59, %f26, %f58;
	mul.ftz.f32 	%f28, %f27, %f59;
	// begin inline asm
	{ mov.b32 %f29, {0,%rs25};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f30, {0,%rs26};}

	// end inline asm
	mul.ftz.f32 	%f60, %f29, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f61, %f60;
	add.ftz.f32 	%f62, %f61, 0f3F800000;
	div.approx.ftz.f32 	%f63, %f29, %f62;
	mul.ftz.f32 	%f31, %f30, %f63;
	add.s64 	%rd30, %rd4, %rd27;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs9, %f13;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs6, %f10;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs15, %f19;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs12, %f16;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs21, %f25;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs18, %f22;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs27, %f31;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs24, %f28;}

	// end inline asm
	mov.b32 	%r35, {%rs24, %rs27};
	mov.b32 	%r36, {%rs18, %rs21};
	mov.b32 	%r37, {%rs12, %rs15};
	mov.b32 	%r38, {%rs6, %rs9};
	st.global.v4.u32 	[%rd30], {%r38, %r37, %r36, %r35};
	add.s32 	%r41, %r41, %r7;
	setp.lt.s32 	%p12, %r41, %r5;
	@%p12 bra 	$L__BB0_9;

$L__BB0_10:
	shl.b32 	%r39, %r5, 3;
	add.s32 	%r42, %r39, %r6;
	setp.ge.s32 	%p13, %r42, %r14;
	@%p13 bra 	$L__BB0_13;

	mov.u32 	%r11, %ntid.x;

$L__BB0_12:
	mul.wide.s32 	%rd31, %r42, 2;
	add.s64 	%rd32, %rd2, %rd31;
	ld.global.nc.u16 	%rs28, [%rd32];
	// begin inline asm
	{ mov.b32 %f64, {0,%rs28};}

	// end inline asm
	add.s64 	%rd33, %rd3, %rd31;
	ld.global.nc.u16 	%rs29, [%rd33];
	// begin inline asm
	{ mov.b32 %f65, {0,%rs29};}

	// end inline asm
	mul.ftz.f32 	%f67, %f64, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f68, %f67;
	add.ftz.f32 	%f69, %f68, 0f3F800000;
	div.approx.ftz.f32 	%f70, %f64, %f69;
	mul.ftz.f32 	%f66, %f65, %f70;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs30, %f66;}

	// end inline asm
	add.s64 	%rd34, %rd4, %rd31;
	st.global.u16 	[%rd34], %rs30;
	add.s32 	%r42, %r42, %r11;
	setp.lt.s32 	%p14, %r42, %r14;
	@%p14 bra 	$L__BB0_12;
	bra.uni 	$L__BB0_13;

$L__BB0_4:
	mov.u32 	%r40, %tid.x;
	setp.ge.s32 	%p9, %r40, %r14;
	@%p9 bra 	$L__BB0_13;

	mov.u32 	%r2, %ntid.x;

$L__BB0_6:
	mul.wide.s32 	%rd23, %r40, 2;
	add.s64 	%rd24, %rd2, %rd23;
	ld.global.nc.u16 	%rs1, [%rd24];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs1};}

	// end inline asm
	add.s64 	%rd25, %rd3, %rd23;
	ld.global.nc.u16 	%rs2, [%rd25];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs2};}

	// end inline asm
	mul.ftz.f32 	%f4, %f1, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f5, %f4;
	add.ftz.f32 	%f6, %f5, 0f3F800000;
	div.approx.ftz.f32 	%f7, %f1, %f6;
	mul.ftz.f32 	%f3, %f2, %f7;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs3, %f3;}

	// end inline asm
	add.s64 	%rd26, %rd4, %rd23;
	st.global.u16 	[%rd26], %rs3;
	add.s32 	%r40, %r40, %r2;
	setp.lt.s32 	%p10, %r40, %r14;
	@%p10 bra 	$L__BB0_6;

$L__BB0_13:
	ret;

}
	// .globl	fused_swiglu_fp16
.visible .entry fused_swiglu_fp16(
	.param .u64 fused_swiglu_fp16_param_0,
	.param .u64 fused_swiglu_fp16_param_1,
	.param .u64 fused_swiglu_fp16_param_2,
	.param .u32 fused_swiglu_fp16_param_3
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<31>;
	.reg .f32 	%f<71>;
	.reg .b32 	%r<43>;
	.reg .b64 	%rd<35>;


	ld.param.u64 	%rd5, [fused_swiglu_fp16_param_0];
	ld.param.u64 	%rd6, [fused_swiglu_fp16_param_1];
	ld.param.u64 	%rd7, [fused_swiglu_fp16_param_2];
	ld.param.u32 	%r14, [fused_swiglu_fp16_param_3];
	mov.u32 	%r15, %ctaid.x;
	cvt.u64.u32 	%rd8, %r15;
	cvt.s64.s32 	%rd9, %r14;
	mul.lo.s64 	%rd1, %rd9, %rd8;
	shl.b64 	%rd10, %rd1, 1;
	add.s64 	%rd11, %rd6, %rd10;
	and.b64  	%rd12, %rd11, 15;
	setp.ne.s64 	%p4, %rd12, 0;
	mov.pred 	%p15, 0;
	@%p4 bra 	$L__BB1_3;

	add.s64 	%rd14, %rd7, %rd10;
	and.b64  	%rd15, %rd14, 15;
	setp.ne.s64 	%p6, %rd15, 0;
	@%p6 bra 	$L__BB1_3;

	add.s64 	%rd17, %rd5, %rd10;
	and.b64  	%rd18, %rd17, 15;
	setp.eq.s64 	%p15, %rd18, 0;

$L__BB1_3:
	cvta.to.global.u64 	%rd19, %rd6;
	add.s64 	%rd2, %rd19, %rd10;
	cvta.to.global.u64 	%rd21, %rd7;
	add.s64 	%rd3, %rd21, %rd10;
	cvta.to.global.u64 	%rd22, %rd5;
	add.s64 	%rd4, %rd22, %rd10;
	setp.gt.s32 	%p7, %r14, 7;
	and.pred  	%p8, %p7, %p15;
	@%p8 bra 	$L__BB1_7;
	bra.uni 	$L__BB1_4;

$L__BB1_7:
	shr.s32 	%r16, %r14, 31;
	shr.u32 	%r17, %r16, 29;
	add.s32 	%r18, %r14, %r17;
	shr.s32 	%r5, %r18, 3;
	mov.u32 	%r6, %tid.x;
	setp.ge.s32 	%p11, %r6, %r5;
	@%p11 bra 	$L__BB1_10;

	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r41, %r6;

$L__BB1_9:
	mul.wide.s32 	%rd27, %r41, 16;
	add.s64 	%rd28, %rd2, %rd27;
	ld.global.nc.v4.u32 	{%r19, %r20, %r21, %r22}, [%rd28];
	add.s64 	%rd29, %rd3, %rd27;
	ld.global.nc.v4.u32 	{%r27, %r28, %r29, %r30}, [%rd29];
	mov.b32 	{%rs4, %rs7}, %r19;
	// begin inline asm
	{  cvt.f32.f16 %f8, %rs4;}

	// end inline asm
	mov.b32 	{%rs5, %rs8}, %r27;
	// begin inline asm
	{  cvt.f32.f16 %f9, %rs5;}

	// end inline asm
	mul.ftz.f32 	%f32, %f8, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f33, %f32;
	add.ftz.f32 	%f34, %f33, 0f3F800000;
	div.approx.ftz.f32 	%f35, %f8, %f34;
	mul.ftz.f32 	%f10, %f9, %f35;
	// begin inline asm
	{  cvt.f32.f16 %f11, %rs7;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f12, %rs8;}

	// end inline asm
	mul.ftz.f32 	%f36, %f11, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f37, %f36;
	add.ftz.f32 	%f38, %f37, 0f3F800000;
	div.approx.ftz.f32 	%f39, %f11, %f38;
	mul.ftz.f32 	%f13, %f12, %f39;
	mov.b32 	{%rs10, %rs13}, %r20;
	// begin inline asm
	{  cvt.f32.f16 %f14, %rs10;}

	// end inline asm
	mov.b32 	{%rs11, %rs14}, %r28;
	// begin inline asm
	{  cvt.f32.f16 %f15, %rs11;}

	// end inline asm
	mul.ftz.f32 	%f40, %f14, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f41, %f40;
	add.ftz.f32 	%f42, %f41, 0f3F800000;
	div.approx.ftz.f32 	%f43, %f14, %f42;
	mul.ftz.f32 	%f16, %f15, %f43;
	// begin inline asm
	{  cvt.f32.f16 %f17, %rs13;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f18, %rs14;}

	// end inline asm
	mul.ftz.f32 	%f44, %f17, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f45, %f44;
	add.ftz.f32 	%f46, %f45, 0f3F800000;
	div.approx.ftz.f32 	%f47, %f17, %f46;
	mul.ftz.f32 	%f19, %f18, %f47;
	mov.b32 	{%rs16, %rs19}, %r21;
	// begin inline asm
	{  cvt.f32.f16 %f20, %rs16;}

	// end inline asm
	mov.b32 	{%rs17, %rs20}, %r29;
	// begin inline asm
	{  cvt.f32.f16 %f21, %rs17;}

	// end inline asm
	mul.ftz.f32 	%f48, %f20, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f49, %f48;
	add.ftz.f32 	%f50, %f49, 0f3F800000;
	div.approx.ftz.f32 	%f51, %f20, %f50;
	mul.ftz.f32 	%f22, %f21, %f51;
	// begin inline asm
	{  cvt.f32.f16 %f23, %rs19;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f24, %rs20;}

	// end inline asm
	mul.ftz.f32 	%f52, %f23, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f53, %f52;
	add.ftz.f32 	%f54, %f53, 0f3F800000;
	div.approx.ftz.f32 	%f55, %f23, %f54;
	mul.ftz.f32 	%f25, %f24, %f55;
	mov.b32 	{%rs22, %rs25}, %r22;
	// begin inline asm
	{  cvt.f32.f16 %f26, %rs22;}

	// end inline asm
	mov.b32 	{%rs23, %rs26}, %r30;
	// begin inline asm
	{  cvt.f32.f16 %f27, %rs23;}

	// end inline asm
	mul.ftz.f32 	%f56, %f26, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f57, %f56;
	add.ftz.f32 	%f58, %f57, 0f3F800000;
	div.approx.ftz.f32 	%f59, %f26, %f58;
	mul.ftz.f32 	%f28, %f27, %f59;
	// begin inline asm
	{  cvt.f32.f16 %f29, %rs25;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f30, %rs26;}

	// end inline asm
	mul.ftz.f32 	%f60, %f29, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f61, %f60;
	add.ftz.f32 	%f62, %f61, 0f3F800000;
	div.approx.ftz.f32 	%f63, %f29, %f62;
	mul.ftz.f32 	%f31, %f30, %f63;
	add.s64 	%rd30, %rd4, %rd27;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs9, %f13;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f10;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs15, %f19;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs12, %f16;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs21, %f25;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs18, %f22;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs27, %f31;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs24, %f28;}

	// end inline asm
	mov.b32 	%r35, {%rs24, %rs27};
	mov.b32 	%r36, {%rs18, %rs21};
	mov.b32 	%r37, {%rs12, %rs15};
	mov.b32 	%r38, {%rs6, %rs9};
	st.global.v4.u32 	[%rd30], {%r38, %r37, %r36, %r35};
	add.s32 	%r41, %r41, %r7;
	setp.lt.s32 	%p12, %r41, %r5;
	@%p12 bra 	$L__BB1_9;

$L__BB1_10:
	shl.b32 	%r39, %r5, 3;
	add.s32 	%r42, %r39, %r6;
	setp.ge.s32 	%p13, %r42, %r14;
	@%p13 bra 	$L__BB1_13;

	mov.u32 	%r11, %ntid.x;

$L__BB1_12:
	mul.wide.s32 	%rd31, %r42, 2;
	add.s64 	%rd32, %rd2, %rd31;
	ld.global.nc.u16 	%rs28, [%rd32];
	// begin inline asm
	{  cvt.f32.f16 %f64, %rs28;}

	// end inline asm
	add.s64 	%rd33, %rd3, %rd31;
	ld.global.nc.u16 	%rs29, [%rd33];
	// begin inline asm
	{  cvt.f32.f16 %f65, %rs29;}

	// end inline asm
	mul.ftz.f32 	%f67, %f64, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f68, %f67;
	add.ftz.f32 	%f69, %f68, 0f3F800000;
	div.approx.ftz.f32 	%f70, %f64, %f69;
	mul.ftz.f32 	%f66, %f65, %f70;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs30, %f66;}

	// end inline asm
	add.s64 	%rd34, %rd4, %rd31;
	st.global.u16 	[%rd34], %rs30;
	add.s32 	%r42, %r42, %r11;
	setp.lt.s32 	%p14, %r42, %r14;
	@%p14 bra 	$L__BB1_12;
	bra.uni 	$L__BB1_13;

$L__BB1_4:
	mov.u32 	%r40, %tid.x;
	setp.ge.s32 	%p9, %r40, %r14;
	@%p9 bra 	$L__BB1_13;

	mov.u32 	%r2, %ntid.x;

$L__BB1_6:
	mul.wide.s32 	%rd23, %r40, 2;
	add.s64 	%rd24, %rd2, %rd23;
	ld.global.nc.u16 	%rs1, [%rd24];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	add.s64 	%rd25, %rd3, %rd23;
	ld.global.nc.u16 	%rs2, [%rd25];
	// begin inline asm
	{  cvt.f32.f16 %f2, %rs2;}

	// end inline asm
	mul.ftz.f32 	%f4, %f1, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f5, %f4;
	add.ftz.f32 	%f6, %f5, 0f3F800000;
	div.approx.ftz.f32 	%f7, %f1, %f6;
	mul.ftz.f32 	%f3, %f2, %f7;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f3;}

	// end inline asm
	add.s64 	%rd26, %rd4, %rd23;
	st.global.u16 	[%rd26], %rs3;
	add.s32 	%r40, %r40, %r2;
	setp.lt.s32 	%p10, %r40, %r14;
	@%p10 bra 	$L__BB1_6;

$L__BB1_13:
	ret;

}
	// .globl	fused_swiglu_fp32
.visible .entry fused_swiglu_fp32(
	.param .u64 fused_swiglu_fp32_param_0,
	.param .u64 fused_swiglu_fp32_param_1,
	.param .u64 fused_swiglu_fp32_param_2,
	.param .u32 fused_swiglu_fp32_param_3
)
{
	.reg .pred 	%p<16>;
	.reg .f32 	%f<51>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<35>;


	ld.param.u64 	%rd5, [fused_swiglu_fp32_param_0];
	ld.param.u64 	%rd6, [fused_swiglu_fp32_param_1];
	ld.param.u64 	%rd7, [fused_swiglu_fp32_param_2];
	ld.param.u32 	%r14, [fused_swiglu_fp32_param_3];
	mov.u32 	%r15, %ctaid.x;
	cvt.u64.u32 	%rd8, %r15;
	cvt.s64.s32 	%rd9, %r14;
	mul.lo.s64 	%rd1, %rd9, %rd8;
	shl.b64 	%rd10, %rd1, 2;
	add.s64 	%rd11, %rd6, %rd10;
	and.b64  	%rd12, %rd11, 15;
	setp.ne.s64 	%p4, %rd12, 0;
	mov.pred 	%p15, 0;
	@%p4 bra 	$L__BB2_3;

	add.s64 	%rd14, %rd7, %rd10;
	and.b64  	%rd15, %rd14, 15;
	setp.ne.s64 	%p6, %rd15, 0;
	@%p6 bra 	$L__BB2_3;

	add.s64 	%rd17, %rd5, %rd10;
	and.b64  	%rd18, %rd17, 15;
	setp.eq.s64 	%p15, %rd18, 0;

$L__BB2_3:
	cvta.to.global.u64 	%rd19, %rd6;
	add.s64 	%rd2, %rd19, %rd10;
	cvta.to.global.u64 	%rd21, %rd7;
	add.s64 	%rd3, %rd21, %rd10;
	cvta.to.global.u64 	%rd22, %rd5;
	add.s64 	%rd4, %rd22, %rd10;
	setp.gt.s32 	%p7, %r14, 3;
	and.pred  	%p8, %p7, %p15;
	@%p8 bra 	$L__BB2_7;
	bra.uni 	$L__BB2_4;

$L__BB2_7:
	shr.s32 	%r16, %r14, 31;
	shr.u32 	%r17, %r16, 30;
	add.s32 	%r18, %r14, %r17;
	shr.s32 	%r5, %r18, 2;
	mov.u32 	%r6, %tid.x;
	setp.ge.s32 	%p11, %r6, %r5;
	@%p11 bra 	$L__BB2_10;

	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r21, %r6;

$L__BB2_9:
	mul.wide.s32 	%rd27, %r21, 16;
	add.s64 	%rd28, %rd2, %rd27;
	ld.global.nc.v4.f32 	{%f8, %f9, %f10, %f11}, [%rd28];
	add.s64 	%rd29, %rd3, %rd27;
	ld.global.nc.v4.f32 	{%f16, %f17, %f18, %f19}, [%rd29];
	mul.ftz.f32 	%f24, %f8, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f25, %f24;
	add.ftz.f32 	%f26, %f25, 0f3F800000;
	div.approx.ftz.f32 	%f27, %f8, %f26;
	mul.ftz.f32 	%f28, %f9, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f29, %f28;
	add.ftz.f32 	%f30, %f29, 0f3F800000;
	div.approx.ftz.f32 	%f31, %f9, %f30;
	mul.ftz.f32 	%f32, %f10, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f33, %f32;
	add.ftz.f32 	%f34, %f33, 0f3F800000;
	div.approx.ftz.f32 	%f35, %f10, %f34;
	mul.ftz.f32 	%f36, %f11, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f37, %f36;
	add.ftz.f32 	%f38, %f37, 0f3F800000;
	div.approx.ftz.f32 	%f39, %f11, %f38;
	add.s64 	%rd30, %rd4, %rd27;
	mul.ftz.f32 	%f40, %f19, %f39;
	mul.ftz.f32 	%f41, %f18, %f35;
	mul.ftz.f32 	%f42, %f17, %f31;
	mul.ftz.f32 	%f43, %f16, %f27;
	st.global.v4.f32 	[%rd30], {%f43, %f42, %f41, %f40};
	add.s32 	%r21, %r21, %r7;
	setp.lt.s32 	%p12, %r21, %r5;
	@%p12 bra 	$L__BB2_9;

$L__BB2_10:
	shl.b32 	%r19, %r5, 2;
	add.s32 	%r22, %r19, %r6;
	setp.ge.s32 	%p13, %r22, %r14;
	@%p13 bra 	$L__BB2_13;

	mov.u32 	%r11, %ntid.x;

$L__BB2_12:
	mul.wide.s32 	%rd31, %r22, 4;
	add.s64 	%rd32, %rd2, %rd31;
	add.s64 	%rd33, %rd3, %rd31;
	ld.global.nc.f32 	%f44, [%rd32];
	mul.ftz.f32 	%f45, %f44, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f46, %f45;
	add.ftz.f32 	%f47, %f46, 0f3F800000;
	div.approx.ftz.f32 	%f48, %f44, %f47;
	ld.global.nc.f32 	%f49, [%rd33];
	mul.ftz.f32 	%f50, %f49, %f48;
	add.s64 	%rd34, %rd4, %rd31;
	st.global.f32 	[%rd34], %f50;
	add.s32 	%r22, %r22, %r11;
	setp.lt.s32 	%p14, %r22, %r14;
	@%p14 bra 	$L__BB2_12;
	bra.uni 	$L__BB2_13;

$L__BB2_4:
	mov.u32 	%r20, %tid.x;
	setp.ge.s32 	%p9, %r20, %r14;
	@%p9 bra 	$L__BB2_13;

	mov.u32 	%r2, %ntid.x;

$L__BB2_6:
	mul.wide.s32 	%rd23, %r20, 4;
	add.s64 	%rd24, %rd2, %rd23;
	add.s64 	%rd25, %rd3, %rd23;
	ld.global.nc.f32 	%f1, [%rd24];
	mul.ftz.f32 	%f2, %f1, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f3, %f2;
	add.ftz.f32 	%f4, %f3, 0f3F800000;
	div.approx.ftz.f32 	%f5, %f1, %f4;
	ld.global.nc.f32 	%f6, [%rd25];
	mul.ftz.f32 	%f7, %f6, %f5;
	add.s64 	%rd26, %rd4, %rd23;
	st.global.f32 	[%rd26], %f7;
	add.s32 	%r20, %r20, %r2;
	setp.lt.s32 	%p10, %r20, %r14;
	@%p10 bra 	$L__BB2_6;

$L__BB2_13:
	ret;

}
	// .globl	fused_swiglu_packed_bf16
.visible .entry fused_swiglu_packed_bf16(
	.param .u64 fused_swiglu_packed_bf16_param_0,
	.param .u64 fused_swiglu_packed_bf16_param_1,
	.param .u32 fused_swiglu_packed_bf16_param_2
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<31>;
	.reg .f32 	%f<71>;
	.reg .b32 	%r<43>;
	.reg .b64 	%rd<36>;


	ld.param.u64 	%rd8, [fused_swiglu_packed_bf16_param_0];
	ld.param.u64 	%rd9, [fused_swiglu_packed_bf16_param_1];
	ld.param.u32 	%r14, [fused_swiglu_packed_bf16_param_2];
	mov.u32 	%r15, %ctaid.x;
	cvt.u64.u32 	%rd10, %r15;
	cvt.s64.s32 	%rd1, %r14;
	mul.lo.s64 	%rd4, %rd1, %rd10;
	shl.b64 	%rd2, %rd4, 1;
	shl.b64 	%rd11, %rd4, 2;
	add.s64 	%rd3, %rd9, %rd11;
	and.b64  	%rd12, %rd3, 15;
	setp.ne.s64 	%p4, %rd12, 0;
	mov.pred 	%p15, 0;
	@%p4 bra 	$L__BB3_3;

	shl.b64 	%rd13, %rd1, 1;
	add.s64 	%rd14, %rd3, %rd13;
	and.b64  	%rd15, %rd14, 15;
	setp.ne.s64 	%p6, %rd15, 0;
	@%p6 bra 	$L__BB3_3;

	add.s64 	%rd17, %rd8, %rd2;
	and.b64  	%rd18, %rd17, 15;
	setp.eq.s64 	%p15, %rd18, 0;

$L__BB3_3:
	cvta.to.global.u64 	%rd19, %rd9;
	shl.b64 	%rd20, %rd2, 1;
	add.s64 	%rd5, %rd19, %rd20;
	shl.b64 	%rd21, %rd1, 1;
	add.s64 	%rd6, %rd5, %rd21;
	cvta.to.global.u64 	%rd22, %rd8;
	add.s64 	%rd7, %rd22, %rd2;
	setp.gt.s32 	%p7, %r14, 7;
	and.pred  	%p8, %p7, %p15;
	@%p8 bra 	$L__BB3_7;
	bra.uni 	$L__BB3_4;

$L__BB3_7:
	shr.s32 	%r16, %r14, 31;
	shr.u32 	%r17, %r16, 29;
	add.s32 	%r18, %r14, %r17;
	shr.s32 	%r5, %r18, 3;
	mov.u32 	%r6, %tid.x;
	setp.ge.s32 	%p11, %r6, %r5;
	@%p11 bra 	$L__BB3_10;

	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r41, %r6;

$L__BB3_9:
	mul.wide.s32 	%rd28, %r41, 16;
	add.s64 	%rd29, %rd5, %rd28;
	ld.global.nc.v4.u32 	{%r19, %r20, %r21, %r22}, [%rd29];
	add.s64 	%rd30, %rd6, %rd28;
	ld.global.nc.v4.u32 	{%r27, %r28, %r29, %r30}, [%rd30];
	mov.b32 	{%rs4, %rs7}, %r19;
	// begin inline asm
	{ mov.b32 %f8, {0,%rs4};}

	// end inline asm
	mov.b32 	{%rs5, %rs8}, %r27;
	// begin inline asm
	{ mov.b32 %f9, {0,%rs5};}

	// end inline asm
	mul.ftz.f32 	%f32, %f8, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f33, %f32;
	add.ftz.f32 	%f34, %f33, 0f3F800000;
	div.approx.ftz.f32 	%f35, %f8, %f34;
	mul.ftz.f32 	%f10, %f9, %f35;
	// begin inline asm
	{ mov.b32 %f11, {0,%rs7};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f12, {0,%rs8};}

	// end inline asm
	mul.ftz.f32 	%f36, %f11, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f37, %f36;
	add.ftz.f32 	%f38, %f37, 0f3F800000;
	div.approx.ftz.f32 	%f39, %f11, %f38;
	mul.ftz.f32 	%f13, %f12, %f39;
	mov.b32 	{%rs10, %rs13}, %r20;
	// begin inline asm
	{ mov.b32 %f14, {0,%rs10};}

	// end inline asm
	mov.b32 	{%rs11, %rs14}, %r28;
	// begin inline asm
	{ mov.b32 %f15, {0,%rs11};}

	// end inline asm
	mul.ftz.f32 	%f40, %f14, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f41, %f40;
	add.ftz.f32 	%f42, %f41, 0f3F800000;
	div.approx.ftz.f32 	%f43, %f14, %f42;
	mul.ftz.f32 	%f16, %f15, %f43;
	// begin inline asm
	{ mov.b32 %f17, {0,%rs13};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f18, {0,%rs14};}

	// end inline asm
	mul.ftz.f32 	%f44, %f17, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f45, %f44;
	add.ftz.f32 	%f46, %f45, 0f3F800000;
	div.approx.ftz.f32 	%f47, %f17, %f46;
	mul.ftz.f32 	%f19, %f18, %f47;
	mov.b32 	{%rs16, %rs19}, %r21;
	// begin inline asm
	{ mov.b32 %f20, {0,%rs16};}

	// end inline asm
	mov.b32 	{%rs17, %rs20}, %r29;
	// begin inline asm
	{ mov.b32 %f21, {0,%rs17};}

	// end inline asm
	mul.ftz.f32 	%f48, %f20, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f49, %f48;
	add.ftz.f32 	%f50, %f49, 0f3F800000;
	div.approx.ftz.f32 	%f51, %f20, %f50;
	mul.ftz.f32 	%f22, %f21, %f51;
	// begin inline asm
	{ mov.b32 %f23, {0,%rs19};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f24, {0,%rs20};}

	// end inline asm
	mul.ftz.f32 	%f52, %f23, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f53, %f52;
	add.ftz.f32 	%f54, %f53, 0f3F800000;
	div.approx.ftz.f32 	%f55, %f23, %f54;
	mul.ftz.f32 	%f25, %f24, %f55;
	mov.b32 	{%rs22, %rs25}, %r22;
	// begin inline asm
	{ mov.b32 %f26, {0,%rs22};}

	// end inline asm
	mov.b32 	{%rs23, %rs26}, %r30;
	// begin inline asm
	{ mov.b32 %f27, {0,%rs23};}

	// end inline asm
	mul.ftz.f32 	%f56, %f26, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f57, %f56;
	add.ftz.f32 	%f58, %f57, 0f3F800000;
	div.approx.ftz.f32 	%f59, %f26, %f58;
	mul.ftz.f32 	%f28, %f27, %f59;
	// begin inline asm
	{ mov.b32 %f29, {0,%rs25};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f30, {0,%rs26};}

	// end inline asm
	mul.ftz.f32 	%f60, %f29, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f61, %f60;
	add.ftz.f32 	%f62, %f61, 0f3F800000;
	div.approx.ftz.f32 	%f63, %f29, %f62;
	mul.ftz.f32 	%f31, %f30, %f63;
	add.s64 	%rd31, %rd7, %rd28;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs9, %f13;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs6, %f10;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs15, %f19;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs12, %f16;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs21, %f25;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs18, %f22;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs27, %f31;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs24, %f28;}

	// end inline asm
	mov.b32 	%r35, {%rs24, %rs27};
	mov.b32 	%r36, {%rs18, %rs21};
	mov.b32 	%r37, {%rs12, %rs15};
	mov.b32 	%r38, {%rs6, %rs9};
	st.global.v4.u32 	[%rd31], {%r38, %r37, %r36, %r35};
	add.s32 	%r41, %r41, %r7;
	setp.lt.s32 	%p12, %r41, %r5;
	@%p12 bra 	$L__BB3_9;

$L__BB3_10:
	shl.b32 	%r39, %r5, 3;
	add.s32 	%r42, %r39, %r6;
	setp.ge.s32 	%p13, %r42, %r14;
	@%p13 bra 	$L__BB3_13;

	mov.u32 	%r11, %ntid.x;

$L__BB3_12:
	mul.wide.s32 	%rd32, %r42, 2;
	add.s64 	%rd33, %rd5, %rd32;
	ld.global.nc.u16 	%rs28, [%rd33];
	// begin inline asm
	{ mov.b32 %f64, {0,%rs28};}

	// end inline asm
	add.s64 	%rd34, %rd6, %rd32;
	ld.global.nc.u16 	%rs29, [%rd34];
	// begin inline asm
	{ mov.b32 %f65, {0,%rs29};}

	// end inline asm
	mul.ftz.f32 	%f67, %f64, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f68, %f67;
	add.ftz.f32 	%f69, %f68, 0f3F800000;
	div.approx.ftz.f32 	%f70, %f64, %f69;
	mul.ftz.f32 	%f66, %f65, %f70;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs30, %f66;}

	// end inline asm
	add.s64 	%rd35, %rd7, %rd32;
	st.global.u16 	[%rd35], %rs30;
	add.s32 	%r42, %r42, %r11;
	setp.lt.s32 	%p14, %r42, %r14;
	@%p14 bra 	$L__BB3_12;
	bra.uni 	$L__BB3_13;

$L__BB3_4:
	mov.u32 	%r40, %tid.x;
	setp.ge.s32 	%p9, %r40, %r14;
	@%p9 bra 	$L__BB3_13;

	mov.u32 	%r2, %ntid.x;

$L__BB3_6:
	mul.wide.s32 	%rd24, %r40, 2;
	add.s64 	%rd25, %rd5, %rd24;
	ld.global.nc.u16 	%rs1, [%rd25];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs1};}

	// end inline asm
	add.s64 	%rd26, %rd6, %rd24;
	ld.global.nc.u16 	%rs2, [%rd26];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs2};}

	// end inline asm
	mul.ftz.f32 	%f4, %f1, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f5, %f4;
	add.ftz.f32 	%f6, %f5, 0f3F800000;
	div.approx.ftz.f32 	%f7, %f1, %f6;
	mul.ftz.f32 	%f3, %f2, %f7;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs3, %f3;}

	// end inline asm
	add.s64 	%rd27, %rd7, %rd24;
	st.global.u16 	[%rd27], %rs3;
	add.s32 	%r40, %r40, %r2;
	setp.lt.s32 	%p10, %r40, %r14;
	@%p10 bra 	$L__BB3_6;

$L__BB3_13:
	ret;

}
	// .globl	fused_swiglu_packed_fp16
.visible .entry fused_swiglu_packed_fp16(
	.param .u64 fused_swiglu_packed_fp16_param_0,
	.param .u64 fused_swiglu_packed_fp16_param_1,
	.param .u32 fused_swiglu_packed_fp16_param_2
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<31>;
	.reg .f32 	%f<71>;
	.reg .b32 	%r<43>;
	.reg .b64 	%rd<36>;


	ld.param.u64 	%rd8, [fused_swiglu_packed_fp16_param_0];
	ld.param.u64 	%rd9, [fused_swiglu_packed_fp16_param_1];
	ld.param.u32 	%r14, [fused_swiglu_packed_fp16_param_2];
	mov.u32 	%r15, %ctaid.x;
	cvt.u64.u32 	%rd10, %r15;
	cvt.s64.s32 	%rd1, %r14;
	mul.lo.s64 	%rd4, %rd1, %rd10;
	shl.b64 	%rd2, %rd4, 1;
	shl.b64 	%rd11, %rd4, 2;
	add.s64 	%rd3, %rd9, %rd11;
	and.b64  	%rd12, %rd3, 15;
	setp.ne.s64 	%p4, %rd12, 0;
	mov.pred 	%p15, 0;
	@%p4 bra 	$L__BB4_3;

	shl.b64 	%rd13, %rd1, 1;
	add.s64 	%rd14, %rd3, %rd13;
	and.b64  	%rd15, %rd14, 15;
	setp.ne.s64 	%p6, %rd15, 0;
	@%p6 bra 	$L__BB4_3;

	add.s64 	%rd17, %rd8, %rd2;
	and.b64  	%rd18, %rd17, 15;
	setp.eq.s64 	%p15, %rd18, 0;

$L__BB4_3:
	cvta.to.global.u64 	%rd19, %rd9;
	shl.b64 	%rd20, %rd2, 1;
	add.s64 	%rd5, %rd19, %rd20;
	shl.b64 	%rd21, %rd1, 1;
	add.s64 	%rd6, %rd5, %rd21;
	cvta.to.global.u64 	%rd22, %rd8;
	add.s64 	%rd7, %rd22, %rd2;
	setp.gt.s32 	%p7, %r14, 7;
	and.pred  	%p8, %p7, %p15;
	@%p8 bra 	$L__BB4_7;
	bra.uni 	$L__BB4_4;

$L__BB4_7:
	shr.s32 	%r16, %r14, 31;
	shr.u32 	%r17, %r16, 29;
	add.s32 	%r18, %r14, %r17;
	shr.s32 	%r5, %r18, 3;
	mov.u32 	%r6, %tid.x;
	setp.ge.s32 	%p11, %r6, %r5;
	@%p11 bra 	$L__BB4_10;

	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r41, %r6;

$L__BB4_9:
	mul.wide.s32 	%rd28, %r41, 16;
	add.s64 	%rd29, %rd5, %rd28;
	ld.global.nc.v4.u32 	{%r19, %r20, %r21, %r22}, [%rd29];
	add.s64 	%rd30, %rd6, %rd28;
	ld.global.nc.v4.u32 	{%r27, %r28, %r29, %r30}, [%rd30];
	mov.b32 	{%rs4, %rs7}, %r19;
	// begin inline asm
	{  cvt.f32.f16 %f8, %rs4;}

	// end inline asm
	mov.b32 	{%rs5, %rs8}, %r27;
	// begin inline asm
	{  cvt.f32.f16 %f9, %rs5;}

	// end inline asm
	mul.ftz.f32 	%f32, %f8, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f33, %f32;
	add.ftz.f32 	%f34, %f33, 0f3F800000;
	div.approx.ftz.f32 	%f35, %f8, %f34;
	mul.ftz.f32 	%f10, %f9, %f35;
	// begin inline asm
	{  cvt.f32.f16 %f11, %rs7;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f12, %rs8;}

	// end inline asm
	mul.ftz.f32 	%f36, %f11, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f37, %f36;
	add.ftz.f32 	%f38, %f37, 0f3F800000;
	div.approx.ftz.f32 	%f39, %f11, %f38;
	mul.ftz.f32 	%f13, %f12, %f39;
	mov.b32 	{%rs10, %rs13}, %r20;
	// begin inline asm
	{  cvt.f32.f16 %f14, %rs10;}

	// end inline asm
	mov.b32 	{%rs11, %rs14}, %r28;
	// begin inline asm
	{  cvt.f32.f16 %f15, %rs11;}

	// end inline asm
	mul.ftz.f32 	%f40, %f14, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f41, %f40;
	add.ftz.f32 	%f42, %f41, 0f3F800000;
	div.approx.ftz.f32 	%f43, %f14, %f42;
	mul.ftz.f32 	%f16, %f15, %f43;
	// begin inline asm
	{  cvt.f32.f16 %f17, %rs13;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f18, %rs14;}

	// end inline asm
	mul.ftz.f32 	%f44, %f17, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f45, %f44;
	add.ftz.f32 	%f46, %f45, 0f3F800000;
	div.approx.ftz.f32 	%f47, %f17, %f46;
	mul.ftz.f32 	%f19, %f18, %f47;
	mov.b32 	{%rs16, %rs19}, %r21;
	// begin inline asm
	{  cvt.f32.f16 %f20, %rs16;}

	// end inline asm
	mov.b32 	{%rs17, %rs20}, %r29;
	// begin inline asm
	{  cvt.f32.f16 %f21, %rs17;}

	// end inline asm
	mul.ftz.f32 	%f48, %f20, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f49, %f48;
	add.ftz.f32 	%f50, %f49, 0f3F800000;
	div.approx.ftz.f32 	%f51, %f20, %f50;
	mul.ftz.f32 	%f22, %f21, %f51;
	// begin inline asm
	{  cvt.f32.f16 %f23, %rs19;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f24, %rs20;}

	// end inline asm
	mul.ftz.f32 	%f52, %f23, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f53, %f52;
	add.ftz.f32 	%f54, %f53, 0f3F800000;
	div.approx.ftz.f32 	%f55, %f23, %f54;
	mul.ftz.f32 	%f25, %f24, %f55;
	mov.b32 	{%rs22, %rs25}, %r22;
	// begin inline asm
	{  cvt.f32.f16 %f26, %rs22;}

	// end inline asm
	mov.b32 	{%rs23, %rs26}, %r30;
	// begin inline asm
	{  cvt.f32.f16 %f27, %rs23;}

	// end inline asm
	mul.ftz.f32 	%f56, %f26, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f57, %f56;
	add.ftz.f32 	%f58, %f57, 0f3F800000;
	div.approx.ftz.f32 	%f59, %f26, %f58;
	mul.ftz.f32 	%f28, %f27, %f59;
	// begin inline asm
	{  cvt.f32.f16 %f29, %rs25;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f30, %rs26;}

	// end inline asm
	mul.ftz.f32 	%f60, %f29, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f61, %f60;
	add.ftz.f32 	%f62, %f61, 0f3F800000;
	div.approx.ftz.f32 	%f63, %f29, %f62;
	mul.ftz.f32 	%f31, %f30, %f63;
	add.s64 	%rd31, %rd7, %rd28;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs9, %f13;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f10;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs15, %f19;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs12, %f16;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs21, %f25;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs18, %f22;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs27, %f31;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs24, %f28;}

	// end inline asm
	mov.b32 	%r35, {%rs24, %rs27};
	mov.b32 	%r36, {%rs18, %rs21};
	mov.b32 	%r37, {%rs12, %rs15};
	mov.b32 	%r38, {%rs6, %rs9};
	st.global.v4.u32 	[%rd31], {%r38, %r37, %r36, %r35};
	add.s32 	%r41, %r41, %r7;
	setp.lt.s32 	%p12, %r41, %r5;
	@%p12 bra 	$L__BB4_9;

$L__BB4_10:
	shl.b32 	%r39, %r5, 3;
	add.s32 	%r42, %r39, %r6;
	setp.ge.s32 	%p13, %r42, %r14;
	@%p13 bra 	$L__BB4_13;

	mov.u32 	%r11, %ntid.x;

$L__BB4_12:
	mul.wide.s32 	%rd32, %r42, 2;
	add.s64 	%rd33, %rd5, %rd32;
	ld.global.nc.u16 	%rs28, [%rd33];
	// begin inline asm
	{  cvt.f32.f16 %f64, %rs28;}

	// end inline asm
	add.s64 	%rd34, %rd6, %rd32;
	ld.global.nc.u16 	%rs29, [%rd34];
	// begin inline asm
	{  cvt.f32.f16 %f65, %rs29;}

	// end inline asm
	mul.ftz.f32 	%f67, %f64, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f68, %f67;
	add.ftz.f32 	%f69, %f68, 0f3F800000;
	div.approx.ftz.f32 	%f70, %f64, %f69;
	mul.ftz.f32 	%f66, %f65, %f70;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs30, %f66;}

	// end inline asm
	add.s64 	%rd35, %rd7, %rd32;
	st.global.u16 	[%rd35], %rs30;
	add.s32 	%r42, %r42, %r11;
	setp.lt.s32 	%p14, %r42, %r14;
	@%p14 bra 	$L__BB4_12;
	bra.uni 	$L__BB4_13;

$L__BB4_4:
	mov.u32 	%r40, %tid.x;
	setp.ge.s32 	%p9, %r40, %r14;
	@%p9 bra 	$L__BB4_13;

	mov.u32 	%r2, %ntid.x;

$L__BB4_6:
	mul.wide.s32 	%rd24, %r40, 2;
	add.s64 	%rd25, %rd5, %rd24;
	ld.global.nc.u16 	%rs1, [%rd25];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	add.s64 	%rd26, %rd6, %rd24;
	ld.global.nc.u16 	%rs2, [%rd26];
	// begin inline asm
	{  cvt.f32.f16 %f2, %rs2;}

	// end inline asm
	mul.ftz.f32 	%f4, %f1, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f5, %f4;
	add.ftz.f32 	%f6, %f5, 0f3F800000;
	div.approx.ftz.f32 	%f7, %f1, %f6;
	mul.ftz.f32 	%f3, %f2, %f7;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f3;}

	// end inline asm
	add.s64 	%rd27, %rd7, %rd24;
	st.global.u16 	[%rd27], %rs3;
	add.s32 	%r40, %r40, %r2;
	setp.lt.s32 	%p10, %r40, %r14;
	@%p10 bra 	$L__BB4_6;

$L__BB4_13:
	ret;

}
	// .globl	fused_swiglu_packed_fp32
.visible .entry fused_swiglu_packed_fp32(
	.param .u64 fused_swiglu_packed_fp32_param_0,
	.param .u64 fused_swiglu_packed_fp32_param_1,
	.param .u32 fused_swiglu_packed_fp32_param_2
)
{
	.reg .pred 	%p<16>;
	.reg .f32 	%f<51>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<36>;


	ld.param.u64 	%rd8, [fused_swiglu_packed_fp32_param_0];
	ld.param.u64 	%rd9, [fused_swiglu_packed_fp32_param_1];
	ld.param.u32 	%r14, [fused_swiglu_packed_fp32_param_2];
	mov.u32 	%r15, %ctaid.x;
	cvt.u64.u32 	%rd10, %r15;
	cvt.s64.s32 	%rd1, %r14;
	mul.lo.s64 	%rd4, %rd1, %rd10;
	shl.b64 	%rd2, %rd4, 1;
	shl.b64 	%rd11, %rd4, 3;
	add.s64 	%rd3, %rd9, %rd11;
	and.b64  	%rd12, %rd3, 15;
	setp.ne.s64 	%p4, %rd12, 0;
	mov.pred 	%p15, 0;
	@%p4 bra 	$L__BB5_3;

	shl.b64 	%rd13, %rd1, 2;
	add.s64 	%rd14, %rd3, %rd13;
	and.b64  	%rd15, %rd14, 15;
	setp.ne.s64 	%p6, %rd15, 0;
	@%p6 bra 	$L__BB5_3;

	shl.b64 	%rd16, %rd4, 2;
	add.s64 	%rd17, %rd8, %rd16;
	and.b64  	%rd18, %rd17, 15;
	setp.eq.s64 	%p15, %rd18, 0;

$L__BB5_3:
	cvta.to.global.u64 	%rd19, %rd9;
	shl.b64 	%rd20, %rd2, 2;
	add.s64 	%rd5, %rd19, %rd20;
	shl.b64 	%rd21, %rd1, 2;
	add.s64 	%rd6, %rd5, %rd21;
	cvta.to.global.u64 	%rd22, %rd8;
	shl.b64 	%rd23, %rd4, 2;
	add.s64 	%rd7, %rd22, %rd23;
	setp.gt.s32 	%p7, %r14, 3;
	and.pred  	%p8, %p7, %p15;
	@%p8 bra 	$L__BB5_7;
	bra.uni 	$L__BB5_4;

$L__BB5_7:
	shr.s32 	%r16, %r14, 31;
	shr.u32 	%r17, %r16, 30;
	add.s32 	%r18, %r14, %r17;
	shr.s32 	%r5, %r18, 2;
	mov.u32 	%r6, %tid.x;
	setp.ge.s32 	%p11, %r6, %r5;
	@%p11 bra 	$L__BB5_10;

	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r21, %r6;

$L__BB5_9:
	mul.wide.s32 	%rd28, %r21, 16;
	add.s64 	%rd29, %rd5, %rd28;
	ld.global.nc.v4.f32 	{%f8, %f9, %f10, %f11}, [%rd29];
	add.s64 	%rd30, %rd6, %rd28;
	ld.global.nc.v4.f32 	{%f16, %f17, %f18, %f19}, [%rd30];
	mul.ftz.f32 	%f24, %f8, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f25, %f24;
	add.ftz.f32 	%f26, %f25, 0f3F800000;
	div.approx.ftz.f32 	%f27, %f8, %f26;
	mul.ftz.f32 	%f28, %f9, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f29, %f28;
	add.ftz.f32 	%f30, %f29, 0f3F800000;
	div.approx.ftz.f32 	%f31, %f9, %f30;
	mul.ftz.f32 	%f32, %f10, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f33, %f32;
	add.ftz.f32 	%f34, %f33, 0f3F800000;
	div.approx.ftz.f32 	%f35, %f10, %f34;
	mul.ftz.f32 	%f36, %f11, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f37, %f36;
	add.ftz.f32 	%f38, %f37, 0f3F800000;
	div.approx.ftz.f32 	%f39, %f11, %f38;
	add.s64 	%rd31, %rd7, %rd28;
	mul.ftz.f32 	%f40, %f19, %f39;
	mul.ftz.f32 	%f41, %f18, %f35;
	mul.ftz.f32 	%f42, %f17, %f31;
	mul.ftz.f32 	%f43, %f16, %f27;
	st.global.v4.f32 	[%rd31], {%f43, %f42, %f41, %f40};
	add.s32 	%r21, %r21, %r7;
	setp.lt.s32 	%p12, %r21, %r5;
	@%p12 bra 	$L__BB5_9;

$L__BB5_10:
	shl.b32 	%r19, %r5, 2;
	add.s32 	%r22, %r19, %r6;
	setp.ge.s32 	%p13, %r22, %r14;
	@%p13 bra 	$L__BB5_13;

	mov.u32 	%r11, %ntid.x;

$L__BB5_12:
	mul.wide.s32 	%rd32, %r22, 4;
	add.s64 	%rd33, %rd5, %rd32;
	add.s64 	%rd34, %rd6, %rd32;
	ld.global.nc.f32 	%f44, [%rd33];
	mul.ftz.f32 	%f45, %f44, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f46, %f45;
	add.ftz.f32 	%f47, %f46, 0f3F800000;
	div.approx.ftz.f32 	%f48, %f44, %f47;
	ld.global.nc.f32 	%f49, [%rd34];
	mul.ftz.f32 	%f50, %f49, %f48;
	add.s64 	%rd35, %rd7, %rd32;
	st.global.f32 	[%rd35], %f50;
	add.s32 	%r22, %r22, %r11;
	setp.lt.s32 	%p14, %r22, %r14;
	@%p14 bra 	$L__BB5_12;
	bra.uni 	$L__BB5_13;

$L__BB5_4:
	mov.u32 	%r20, %tid.x;
	setp.ge.s32 	%p9, %r20, %r14;
	@%p9 bra 	$L__BB5_13;

	mov.u32 	%r2, %ntid.x;

$L__BB5_6:
	mul.wide.s32 	%rd24, %r20, 4;
	add.s64 	%rd25, %rd5, %rd24;
	add.s64 	%rd26, %rd6, %rd24;
	ld.global.nc.f32 	%f1, [%rd25];
	mul.ftz.f32 	%f2, %f1, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f3, %f2;
	add.ftz.f32 	%f4, %f3, 0f3F800000;
	div.approx.ftz.f32 	%f5, %f1, %f4;
	ld.global.nc.f32 	%f6, [%rd26];
	mul.ftz.f32 	%f7, %f6, %f5;
	add.s64 	%rd27, %rd7, %rd24;
	st.global.f32 	[%rd27], %f7;
	add.s32 	%r20, %r20, %r2;
	setp.lt.s32 	%p10, %r20, %r14;
	@%p10 bra 	$L__BB5_6;

$L__BB5_13:
	ret;

}

