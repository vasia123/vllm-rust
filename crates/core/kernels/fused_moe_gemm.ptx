//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32267302
// Cuda compilation tools, release 12.0, V12.0.140
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_89
.address_size 64

	// .globl	fused_moe_gate_up_silu_kernel
.extern .shared .align 16 .b8 smem[];
.extern .shared .align 16 .b8 simple_smem_raw[];
.extern .shared .align 16 .b8 smem_raw[];

.visible .entry fused_moe_gate_up_silu_kernel(
	.param .u64 fused_moe_gate_up_silu_kernel_param_0,
	.param .u64 fused_moe_gate_up_silu_kernel_param_1,
	.param .u64 fused_moe_gate_up_silu_kernel_param_2,
	.param .u64 fused_moe_gate_up_silu_kernel_param_3,
	.param .u64 fused_moe_gate_up_silu_kernel_param_4,
	.param .u64 fused_moe_gate_up_silu_kernel_param_5,
	.param .u32 fused_moe_gate_up_silu_kernel_param_6,
	.param .u32 fused_moe_gate_up_silu_kernel_param_7,
	.param .u32 fused_moe_gate_up_silu_kernel_param_8,
	.param .u32 fused_moe_gate_up_silu_kernel_param_9,
	.param .u32 fused_moe_gate_up_silu_kernel_param_10
)
{
	.reg .pred 	%p<39>;
	.reg .b16 	%rs<6>;
	.reg .f32 	%f<74>;
	.reg .b32 	%r<119>;
	.reg .b64 	%rd<27>;


	ld.param.u64 	%rd7, [fused_moe_gate_up_silu_kernel_param_0];
	ld.param.u64 	%rd8, [fused_moe_gate_up_silu_kernel_param_1];
	ld.param.u64 	%rd4, [fused_moe_gate_up_silu_kernel_param_2];
	ld.param.u64 	%rd5, [fused_moe_gate_up_silu_kernel_param_3];
	ld.param.u64 	%rd6, [fused_moe_gate_up_silu_kernel_param_4];
	ld.param.u64 	%rd9, [fused_moe_gate_up_silu_kernel_param_5];
	ld.param.u32 	%r23, [fused_moe_gate_up_silu_kernel_param_6];
	ld.param.u32 	%r24, [fused_moe_gate_up_silu_kernel_param_7];
	ld.param.u32 	%r25, [fused_moe_gate_up_silu_kernel_param_8];
	ld.param.u32 	%r26, [fused_moe_gate_up_silu_kernel_param_9];
	ld.param.u32 	%r27, [fused_moe_gate_up_silu_kernel_param_10];
	cvta.to.global.u64 	%rd1, %rd8;
	cvta.to.global.u64 	%rd2, %rd7;
	cvta.to.global.u64 	%rd10, %rd9;
	ld.global.nc.u32 	%r28, [%rd10];
	mov.u32 	%r1, %ctaid.x;
	setp.ge.s32 	%p1, %r1, %r28;
	@%p1 bra 	$L__BB0_28;

	cvta.to.global.u64 	%rd11, %rd5;
	mul.wide.s32 	%rd12, %r1, 4;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.nc.u32 	%r2, [%rd13];
	setp.ge.s32 	%p2, %r2, %r25;
	@%p2 bra 	$L__BB0_28;

	div.s32 	%r29, %r1, %r27;
	cvta.to.global.u64 	%rd14, %rd6;
	mul.wide.s32 	%rd15, %r29, 4;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.nc.u32 	%r3, [%rd16];
	setp.lt.s32 	%p3, %r3, 0;
	@%p3 bra 	$L__BB0_28;

	mov.u32 	%r4, %tid.x;
	setp.lt.s32 	%p4, %r24, 1;
	@%p4 bra 	$L__BB0_28;

	shr.s32 	%r31, %r4, 31;
	shr.u32 	%r32, %r31, 27;
	add.s32 	%r33, %r4, %r32;
	shr.s32 	%r34, %r33, 5;
	mov.u32 	%r5, %ntid.x;
	shr.u32 	%r6, %r5, 5;
	div.s32 	%r35, %r2, %r26;
	mul.lo.s32 	%r7, %r35, %r23;
	mul.lo.s32 	%r36, %r24, %r3;
	shl.b32 	%r8, %r36, 1;
	shl.b32 	%r37, %r34, 2;
	mov.u32 	%r38, smem_raw;
	add.s32 	%r9, %r38, %r37;
	add.s32 	%r10, %r4, 31;
	and.b32  	%r39, %r33, -32;
	sub.s32 	%r11, %r4, %r39;
	shl.b32 	%r40, %r11, 2;
	add.s32 	%r12, %r38, %r40;
	add.s32 	%r13, %r8, %r24;
	mul.lo.s32 	%r14, %r2, %r24;
	cvta.to.global.u64 	%rd3, %rd4;
	mov.u32 	%r116, 0;

$L__BB0_5:
	setp.ge.s32 	%p5, %r4, %r23;
	mov.f32 	%f69, 0f00000000;
	@%p5 bra 	$L__BB0_8;

	add.s32 	%r41, %r116, %r8;
	mul.lo.s32 	%r16, %r41, %r23;
	mov.u32 	%r117, %r4;

$L__BB0_7:
	add.s32 	%r42, %r117, %r7;
	mul.wide.s32 	%rd17, %r42, 2;
	add.s64 	%rd18, %rd2, %rd17;
	ld.global.nc.u16 	%rs1, [%rd18];
	// begin inline asm
	{ mov.b32 %f19, {0,%rs1};}

	// end inline asm
	add.s32 	%r43, %r117, %r16;
	mul.wide.s32 	%rd19, %r43, 2;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.nc.u16 	%rs2, [%rd20];
	// begin inline asm
	{ mov.b32 %f20, {0,%rs2};}

	// end inline asm
	fma.rn.ftz.f32 	%f69, %f19, %f20, %f69;
	add.s32 	%r117, %r117, %r5;
	setp.lt.s32 	%p6, %r117, %r23;
	@%p6 bra 	$L__BB0_7;

$L__BB0_8:
	mov.b32 	%r44, %f69;
	mov.u32 	%r45, 31;
	mov.u32 	%r46, 16;
	mov.u32 	%r47, -1;
	shfl.sync.bfly.b32 	%r48|%p7, %r44, %r46, %r45, %r47;
	mov.b32 	%f21, %r48;
	add.ftz.f32 	%f22, %f69, %f21;
	mov.b32 	%r49, %f22;
	mov.u32 	%r50, 8;
	shfl.sync.bfly.b32 	%r51|%p8, %r49, %r50, %r45, %r47;
	mov.b32 	%f23, %r51;
	add.ftz.f32 	%f24, %f22, %f23;
	mov.b32 	%r52, %f24;
	mov.u32 	%r53, 4;
	shfl.sync.bfly.b32 	%r54|%p9, %r52, %r53, %r45, %r47;
	mov.b32 	%f25, %r54;
	add.ftz.f32 	%f26, %f24, %f25;
	mov.b32 	%r55, %f26;
	mov.u32 	%r56, 2;
	shfl.sync.bfly.b32 	%r57|%p10, %r55, %r56, %r45, %r47;
	mov.b32 	%f27, %r57;
	add.ftz.f32 	%f28, %f26, %f27;
	mov.b32 	%r58, %f28;
	mov.u32 	%r59, 1;
	shfl.sync.bfly.b32 	%r60|%p11, %r58, %r59, %r45, %r47;
	mov.b32 	%f29, %r60;
	add.ftz.f32 	%f4, %f28, %f29;
	setp.ne.s32 	%p12, %r11, 0;
	@%p12 bra 	$L__BB0_10;

	st.shared.f32 	[%r9], %f4;

$L__BB0_10:
	setp.gt.u32 	%p13, %r10, 62;
	bar.sync 	0;
	@%p13 bra 	$L__BB0_15;

	setp.ge.s32 	%p14, %r11, %r6;
	mov.f32 	%f70, 0f00000000;
	@%p14 bra 	$L__BB0_13;

	ld.shared.f32 	%f70, [%r12];

$L__BB0_13:
	mov.b32 	%r61, %f70;
	mov.u32 	%r62, 31;
	mov.u32 	%r63, 16;
	mov.u32 	%r64, -1;
	shfl.sync.bfly.b32 	%r65|%p15, %r61, %r63, %r62, %r64;
	mov.b32 	%f31, %r65;
	add.ftz.f32 	%f32, %f70, %f31;
	mov.b32 	%r66, %f32;
	mov.u32 	%r67, 8;
	shfl.sync.bfly.b32 	%r68|%p16, %r66, %r67, %r62, %r64;
	mov.b32 	%f33, %r68;
	add.ftz.f32 	%f34, %f32, %f33;
	mov.b32 	%r69, %f34;
	mov.u32 	%r70, 4;
	shfl.sync.bfly.b32 	%r71|%p17, %r69, %r70, %r62, %r64;
	mov.b32 	%f35, %r71;
	add.ftz.f32 	%f36, %f34, %f35;
	mov.b32 	%r72, %f36;
	mov.u32 	%r73, 2;
	shfl.sync.bfly.b32 	%r74|%p18, %r72, %r73, %r62, %r64;
	mov.b32 	%f37, %r74;
	add.ftz.f32 	%f38, %f36, %f37;
	mov.b32 	%r75, %f38;
	mov.u32 	%r76, 1;
	shfl.sync.bfly.b32 	%r77|%p19, %r75, %r76, %r62, %r64;
	mov.b32 	%f39, %r77;
	add.ftz.f32 	%f7, %f38, %f39;
	@%p12 bra 	$L__BB0_15;

	st.shared.f32 	[smem_raw], %f7;

$L__BB0_15:
	bar.sync 	0;
	ld.shared.f32 	%f8, [smem_raw];
	bar.sync 	0;
	mov.f32 	%f72, 0f00000000;
	@%p5 bra 	$L__BB0_18;

	add.s32 	%r78, %r13, %r116;
	mul.lo.s32 	%r19, %r78, %r23;
	mov.u32 	%r118, %r4;

$L__BB0_17:
	add.s32 	%r79, %r118, %r7;
	mul.wide.s32 	%rd21, %r79, 2;
	add.s64 	%rd22, %rd2, %rd21;
	ld.global.nc.u16 	%rs3, [%rd22];
	// begin inline asm
	{ mov.b32 %f42, {0,%rs3};}

	// end inline asm
	add.s32 	%r80, %r118, %r19;
	mul.wide.s32 	%rd23, %r80, 2;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.nc.u16 	%rs4, [%rd24];
	// begin inline asm
	{ mov.b32 %f43, {0,%rs4};}

	// end inline asm
	fma.rn.ftz.f32 	%f72, %f42, %f43, %f72;
	add.s32 	%r118, %r118, %r5;
	setp.lt.s32 	%p22, %r118, %r23;
	@%p22 bra 	$L__BB0_17;

$L__BB0_18:
	mov.b32 	%r81, %f72;
	mov.u32 	%r82, 31;
	mov.u32 	%r83, 16;
	mov.u32 	%r84, -1;
	shfl.sync.bfly.b32 	%r85|%p23, %r81, %r83, %r82, %r84;
	mov.b32 	%f44, %r85;
	add.ftz.f32 	%f45, %f72, %f44;
	mov.b32 	%r86, %f45;
	mov.u32 	%r87, 8;
	shfl.sync.bfly.b32 	%r88|%p24, %r86, %r87, %r82, %r84;
	mov.b32 	%f46, %r88;
	add.ftz.f32 	%f47, %f45, %f46;
	mov.b32 	%r89, %f47;
	mov.u32 	%r90, 4;
	shfl.sync.bfly.b32 	%r91|%p25, %r89, %r90, %r82, %r84;
	mov.b32 	%f48, %r91;
	add.ftz.f32 	%f49, %f47, %f48;
	mov.b32 	%r92, %f49;
	mov.u32 	%r93, 2;
	shfl.sync.bfly.b32 	%r94|%p26, %r92, %r93, %r82, %r84;
	mov.b32 	%f50, %r94;
	add.ftz.f32 	%f51, %f49, %f50;
	mov.b32 	%r95, %f51;
	mov.u32 	%r96, 1;
	shfl.sync.bfly.b32 	%r97|%p27, %r95, %r96, %r82, %r84;
	mov.b32 	%f52, %r97;
	add.ftz.f32 	%f12, %f51, %f52;
	@%p12 bra 	$L__BB0_20;

	st.shared.f32 	[%r9], %f12;

$L__BB0_20:
	bar.sync 	0;
	@%p13 bra 	$L__BB0_25;

	setp.ge.s32 	%p30, %r11, %r6;
	mov.f32 	%f73, 0f00000000;
	@%p30 bra 	$L__BB0_23;

	ld.shared.f32 	%f73, [%r12];

$L__BB0_23:
	mov.b32 	%r98, %f73;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p31, %r98, %r100, %r99, %r101;
	mov.b32 	%f54, %r102;
	add.ftz.f32 	%f55, %f73, %f54;
	mov.b32 	%r103, %f55;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p32, %r103, %r104, %r99, %r101;
	mov.b32 	%f56, %r105;
	add.ftz.f32 	%f57, %f55, %f56;
	mov.b32 	%r106, %f57;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p33, %r106, %r107, %r99, %r101;
	mov.b32 	%f58, %r108;
	add.ftz.f32 	%f59, %f57, %f58;
	mov.b32 	%r109, %f59;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p34, %r109, %r110, %r99, %r101;
	mov.b32 	%f60, %r111;
	add.ftz.f32 	%f61, %f59, %f60;
	mov.b32 	%r112, %f61;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p35, %r112, %r113, %r99, %r101;
	mov.b32 	%f62, %r114;
	add.ftz.f32 	%f15, %f61, %f62;
	@%p12 bra 	$L__BB0_25;

	st.shared.f32 	[smem_raw], %f15;

$L__BB0_25:
	setp.ne.s32 	%p37, %r4, 0;
	bar.sync 	0;
	ld.shared.f32 	%f16, [smem_raw];
	bar.sync 	0;
	@%p37 bra 	$L__BB0_27;

	mul.ftz.f32 	%f64, %f8, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f65, %f64;
	add.ftz.f32 	%f66, %f65, 0f3F800000;
	div.approx.ftz.f32 	%f67, %f8, %f66;
	mul.ftz.f32 	%f63, %f67, %f16;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs5, %f63;}

	// end inline asm
	add.s32 	%r115, %r116, %r14;
	mul.wide.s32 	%rd25, %r115, 2;
	add.s64 	%rd26, %rd3, %rd25;
	st.global.u16 	[%rd26], %rs5;

$L__BB0_27:
	bar.sync 	0;
	add.s32 	%r116, %r116, 1;
	setp.lt.s32 	%p38, %r116, %r24;
	@%p38 bra 	$L__BB0_5;

$L__BB0_28:
	ret;

}
	// .globl	fused_moe_down_reduce_kernel
.visible .entry fused_moe_down_reduce_kernel(
	.param .u64 fused_moe_down_reduce_kernel_param_0,
	.param .u64 fused_moe_down_reduce_kernel_param_1,
	.param .u64 fused_moe_down_reduce_kernel_param_2,
	.param .u64 fused_moe_down_reduce_kernel_param_3,
	.param .u64 fused_moe_down_reduce_kernel_param_4,
	.param .u64 fused_moe_down_reduce_kernel_param_5,
	.param .u64 fused_moe_down_reduce_kernel_param_6,
	.param .u32 fused_moe_down_reduce_kernel_param_7,
	.param .u32 fused_moe_down_reduce_kernel_param_8,
	.param .u32 fused_moe_down_reduce_kernel_param_9,
	.param .u32 fused_moe_down_reduce_kernel_param_10,
	.param .u32 fused_moe_down_reduce_kernel_param_11,
	.param .u32 fused_moe_down_reduce_kernel_param_12
)
{
	.reg .pred 	%p<28>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<48>;
	.reg .b32 	%r<84>;
	.reg .b64 	%rd<28>;


	ld.param.u64 	%rd10, [fused_moe_down_reduce_kernel_param_0];
	ld.param.u64 	%rd11, [fused_moe_down_reduce_kernel_param_1];
	ld.param.u64 	%rd12, [fused_moe_down_reduce_kernel_param_2];
	ld.param.u64 	%rd13, [fused_moe_down_reduce_kernel_param_3];
	ld.param.u64 	%rd14, [fused_moe_down_reduce_kernel_param_4];
	ld.param.u64 	%rd15, [fused_moe_down_reduce_kernel_param_5];
	ld.param.u64 	%rd16, [fused_moe_down_reduce_kernel_param_6];
	ld.param.u32 	%r24, [fused_moe_down_reduce_kernel_param_7];
	ld.param.u32 	%r25, [fused_moe_down_reduce_kernel_param_8];
	ld.param.u32 	%r28, [fused_moe_down_reduce_kernel_param_10];
	ld.param.u32 	%r26, [fused_moe_down_reduce_kernel_param_11];
	ld.param.u32 	%r27, [fused_moe_down_reduce_kernel_param_12];
	mov.u32 	%r1, %ctaid.x;
	setp.ge.s32 	%p1, %r1, %r28;
	@%p1 bra 	$L__BB1_27;

	mov.u32 	%r2, %tid.x;
	setp.lt.s32 	%p2, %r24, 1;
	@%p2 bra 	$L__BB1_27;

	mul.lo.s32 	%r3, %r1, %r26;
	mov.u32 	%r4, %ntid.x;
	shr.u32 	%r5, %r4, 5;
	shr.s32 	%r30, %r2, 31;
	shr.u32 	%r31, %r30, 27;
	add.s32 	%r32, %r2, %r31;
	shr.s32 	%r33, %r32, 5;
	shl.b32 	%r34, %r33, 2;
	mov.u32 	%r35, smem_raw;
	add.s32 	%r6, %r35, %r34;
	add.s32 	%r7, %r2, 31;
	and.b32  	%r36, %r32, -32;
	sub.s32 	%r8, %r2, %r36;
	shl.b32 	%r37, %r8, 2;
	add.s32 	%r9, %r35, %r37;
	mul.lo.s32 	%r10, %r1, %r24;
	cvta.to.global.u64 	%rd1, %rd11;
	cvta.to.global.u64 	%rd2, %rd10;
	cvta.to.global.u64 	%rd3, %rd13;
	cvta.to.global.u64 	%rd4, %rd15;
	cvta.to.global.u64 	%rd5, %rd14;
	cvta.to.global.u64 	%rd6, %rd16;
	cvta.to.global.u64 	%rd7, %rd12;
	mov.u32 	%r80, 0;

$L__BB1_3:
	setp.lt.s32 	%p3, %r26, 1;
	mov.f32 	%f46, 0f00000000;
	@%p3 bra 	$L__BB1_24;

	ld.global.nc.u32 	%r12, [%rd6];
	mov.u32 	%r81, 0;

$L__BB1_5:
	add.s32 	%r14, %r81, %r3;
	setp.lt.s32 	%p4, %r12, 1;
	@%p4 bra 	$L__BB1_23;

	mov.u32 	%r82, 0;
	mov.u64 	%rd27, %rd5;

$L__BB1_7:
	ld.global.nc.u32 	%r40, [%rd27];
	setp.ne.s32 	%p5, %r40, %r14;
	@%p5 bra 	$L__BB1_22;

	div.s32 	%r41, %r82, %r27;
	mul.wide.s32 	%rd17, %r41, 4;
	add.s64 	%rd18, %rd4, %rd17;
	ld.global.nc.u32 	%r16, [%rd18];
	setp.lt.s32 	%p6, %r16, 0;
	@%p6 bra 	$L__BB1_22;
	bra.uni 	$L__BB1_9;

$L__BB1_22:
	add.s32 	%r82, %r82, 1;
	add.s64 	%rd27, %rd27, 4;
	setp.lt.s32 	%p24, %r82, %r12;
	@%p24 bra 	$L__BB1_7;
	bra.uni 	$L__BB1_23;

$L__BB1_9:
	setp.ge.s32 	%p7, %r2, %r25;
	mov.f32 	%f43, 0f00000000;
	@%p7 bra 	$L__BB1_12;

	mul.lo.s32 	%r17, %r14, %r25;
	mad.lo.s32 	%r42, %r16, %r24, %r80;
	mul.lo.s32 	%r18, %r42, %r25;
	mov.u32 	%r83, %r2;

$L__BB1_11:
	add.s32 	%r43, %r83, %r17;
	mul.wide.s32 	%rd19, %r43, 2;
	add.s64 	%rd20, %rd2, %rd19;
	ld.global.nc.u16 	%rs1, [%rd20];
	// begin inline asm
	{ mov.b32 %f17, {0,%rs1};}

	// end inline asm
	add.s32 	%r44, %r83, %r18;
	mul.wide.s32 	%rd21, %r44, 2;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.nc.u16 	%rs2, [%rd22];
	// begin inline asm
	{ mov.b32 %f18, {0,%rs2};}

	// end inline asm
	fma.rn.ftz.f32 	%f43, %f17, %f18, %f43;
	add.s32 	%r83, %r83, %r4;
	setp.lt.s32 	%p8, %r83, %r25;
	@%p8 bra 	$L__BB1_11;

$L__BB1_12:
	mov.b32 	%r45, %f43;
	mov.u32 	%r46, 31;
	mov.u32 	%r47, 16;
	mov.u32 	%r48, -1;
	shfl.sync.bfly.b32 	%r49|%p9, %r45, %r47, %r46, %r48;
	mov.b32 	%f19, %r49;
	add.ftz.f32 	%f20, %f43, %f19;
	mov.b32 	%r50, %f20;
	mov.u32 	%r51, 8;
	shfl.sync.bfly.b32 	%r52|%p10, %r50, %r51, %r46, %r48;
	mov.b32 	%f21, %r52;
	add.ftz.f32 	%f22, %f20, %f21;
	mov.b32 	%r53, %f22;
	mov.u32 	%r54, 4;
	shfl.sync.bfly.b32 	%r55|%p11, %r53, %r54, %r46, %r48;
	mov.b32 	%f23, %r55;
	add.ftz.f32 	%f24, %f22, %f23;
	mov.b32 	%r56, %f24;
	mov.u32 	%r57, 2;
	shfl.sync.bfly.b32 	%r58|%p12, %r56, %r57, %r46, %r48;
	mov.b32 	%f25, %r58;
	add.ftz.f32 	%f26, %f24, %f25;
	mov.b32 	%r59, %f26;
	mov.u32 	%r60, 1;
	shfl.sync.bfly.b32 	%r61|%p13, %r59, %r60, %r46, %r48;
	mov.b32 	%f27, %r61;
	add.ftz.f32 	%f5, %f26, %f27;
	setp.ne.s32 	%p14, %r8, 0;
	@%p14 bra 	$L__BB1_14;

	st.shared.f32 	[%r6], %f5;

$L__BB1_14:
	setp.gt.u32 	%p15, %r7, 62;
	bar.sync 	0;
	@%p15 bra 	$L__BB1_19;

	setp.ge.s32 	%p16, %r8, %r5;
	mov.f32 	%f44, 0f00000000;
	@%p16 bra 	$L__BB1_17;

	ld.shared.f32 	%f44, [%r9];

$L__BB1_17:
	mov.b32 	%r62, %f44;
	mov.u32 	%r63, 31;
	mov.u32 	%r64, 16;
	mov.u32 	%r65, -1;
	shfl.sync.bfly.b32 	%r66|%p17, %r62, %r64, %r63, %r65;
	mov.b32 	%f29, %r66;
	add.ftz.f32 	%f30, %f44, %f29;
	mov.b32 	%r67, %f30;
	mov.u32 	%r68, 8;
	shfl.sync.bfly.b32 	%r69|%p18, %r67, %r68, %r63, %r65;
	mov.b32 	%f31, %r69;
	add.ftz.f32 	%f32, %f30, %f31;
	mov.b32 	%r70, %f32;
	mov.u32 	%r71, 4;
	shfl.sync.bfly.b32 	%r72|%p19, %r70, %r71, %r63, %r65;
	mov.b32 	%f33, %r72;
	add.ftz.f32 	%f34, %f32, %f33;
	mov.b32 	%r73, %f34;
	mov.u32 	%r74, 2;
	shfl.sync.bfly.b32 	%r75|%p20, %r73, %r74, %r63, %r65;
	mov.b32 	%f35, %r75;
	add.ftz.f32 	%f36, %f34, %f35;
	mov.b32 	%r76, %f36;
	mov.u32 	%r77, 1;
	shfl.sync.bfly.b32 	%r78|%p21, %r76, %r77, %r63, %r65;
	mov.b32 	%f37, %r78;
	add.ftz.f32 	%f8, %f36, %f37;
	@%p14 bra 	$L__BB1_19;

	st.shared.f32 	[smem_raw], %f8;

$L__BB1_19:
	setp.ne.s32 	%p23, %r2, 0;
	bar.sync 	0;
	@%p23 bra 	$L__BB1_21;

	ld.shared.f32 	%f38, [smem_raw];
	mul.wide.s32 	%rd23, %r14, 4;
	add.s64 	%rd24, %rd3, %rd23;
	ld.global.nc.f32 	%f39, [%rd24];
	fma.rn.ftz.f32 	%f46, %f38, %f39, %f46;

$L__BB1_21:
	bar.sync 	0;

$L__BB1_23:
	add.s32 	%r81, %r81, 1;
	setp.lt.s32 	%p25, %r81, %r26;
	@%p25 bra 	$L__BB1_5;

$L__BB1_24:
	setp.ne.s32 	%p26, %r2, 0;
	@%p26 bra 	$L__BB1_26;

	// begin inline asm
	{  cvt.rn.bf16.f32 %rs3, %f46;}

	// end inline asm
	add.s32 	%r79, %r80, %r10;
	mul.wide.s32 	%rd25, %r79, 2;
	add.s64 	%rd26, %rd7, %rd25;
	st.global.u16 	[%rd26], %rs3;

$L__BB1_26:
	bar.sync 	0;
	add.s32 	%r80, %r80, 1;
	setp.lt.s32 	%p27, %r80, %r24;
	@%p27 bra 	$L__BB1_3;

$L__BB1_27:
	ret;

}
	// .globl	fused_moe_gemm_64_64_32_weighted
.visible .entry fused_moe_gemm_64_64_32_weighted(
	.param .u64 fused_moe_gemm_64_64_32_weighted_param_0,
	.param .u64 fused_moe_gemm_64_64_32_weighted_param_1,
	.param .u64 fused_moe_gemm_64_64_32_weighted_param_2,
	.param .u64 fused_moe_gemm_64_64_32_weighted_param_3,
	.param .u64 fused_moe_gemm_64_64_32_weighted_param_4,
	.param .u64 fused_moe_gemm_64_64_32_weighted_param_5,
	.param .u64 fused_moe_gemm_64_64_32_weighted_param_6,
	.param .u32 fused_moe_gemm_64_64_32_weighted_param_7,
	.param .u32 fused_moe_gemm_64_64_32_weighted_param_8,
	.param .u32 fused_moe_gemm_64_64_32_weighted_param_9,
	.param .u32 fused_moe_gemm_64_64_32_weighted_param_10,
	.param .u32 fused_moe_gemm_64_64_32_weighted_param_11,
	.param .u32 fused_moe_gemm_64_64_32_weighted_param_12,
	.param .u32 fused_moe_gemm_64_64_32_weighted_param_13,
	.param .u32 fused_moe_gemm_64_64_32_weighted_param_14,
	.param .u32 fused_moe_gemm_64_64_32_weighted_param_15,
	.param .u32 fused_moe_gemm_64_64_32_weighted_param_16
)
{
	.local .align 16 .b8 	__local_depot2[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<38>;
	.reg .b16 	%rs<72>;
	.reg .f32 	%f<116>;
	.reg .b32 	%r<152>;
	.reg .b64 	%rd<45>;


	mov.u64 	%SPL, __local_depot2;
	ld.param.u64 	%rd9, [fused_moe_gemm_64_64_32_weighted_param_0];
	ld.param.u64 	%rd10, [fused_moe_gemm_64_64_32_weighted_param_1];
	ld.param.u64 	%rd13, [fused_moe_gemm_64_64_32_weighted_param_2];
	ld.param.u64 	%rd11, [fused_moe_gemm_64_64_32_weighted_param_3];
	ld.param.u64 	%rd14, [fused_moe_gemm_64_64_32_weighted_param_4];
	ld.param.u64 	%rd12, [fused_moe_gemm_64_64_32_weighted_param_5];
	ld.param.u64 	%rd15, [fused_moe_gemm_64_64_32_weighted_param_6];
	ld.param.u32 	%r72, [fused_moe_gemm_64_64_32_weighted_param_7];
	ld.param.u32 	%r73, [fused_moe_gemm_64_64_32_weighted_param_8];
	ld.param.u32 	%r74, [fused_moe_gemm_64_64_32_weighted_param_9];
	ld.param.u32 	%r75, [fused_moe_gemm_64_64_32_weighted_param_10];
	ld.param.u32 	%r76, [fused_moe_gemm_64_64_32_weighted_param_11];
	ld.param.u32 	%r77, [fused_moe_gemm_64_64_32_weighted_param_12];
	ld.param.u32 	%r78, [fused_moe_gemm_64_64_32_weighted_param_13];
	ld.param.u32 	%r79, [fused_moe_gemm_64_64_32_weighted_param_14];
	ld.param.u32 	%r80, [fused_moe_gemm_64_64_32_weighted_param_15];
	ld.param.u32 	%r81, [fused_moe_gemm_64_64_32_weighted_param_16];
	cvta.to.global.u64 	%rd1, %rd13;
	cvta.to.global.u64 	%rd2, %rd14;
	add.u64 	%rd3, %SPL, 0;
	cvta.to.global.u64 	%rd17, %rd15;
	ld.global.nc.u32 	%r1, [%rd17];
	add.s32 	%r82, %r1, 63;
	shr.s32 	%r83, %r82, 31;
	shr.u32 	%r84, %r83, 26;
	add.s32 	%r85, %r82, %r84;
	shr.s32 	%r86, %r85, 6;
	add.s32 	%r87, %r72, 63;
	shr.s32 	%r88, %r87, 31;
	shr.u32 	%r89, %r88, 26;
	add.s32 	%r90, %r87, %r89;
	shr.s32 	%r91, %r90, 6;
	shl.b32 	%r92, %r91, 3;
	mov.u32 	%r93, %ctaid.x;
	div.s32 	%r94, %r93, %r92;
	shl.b32 	%r95, %r94, 3;
	sub.s32 	%r96, %r86, %r95;
	min.s32 	%r97, %r96, 8;
	mul.lo.s32 	%r98, %r94, %r92;
	sub.s32 	%r99, %r93, %r98;
	div.s32 	%r3, %r99, %r97;
	mul.lo.s32 	%r100, %r3, %r97;
	sub.s32 	%r101, %r99, %r100;
	add.s32 	%r2, %r101, %r95;
	shl.b32 	%r4, %r2, 6;
	setp.ge.s32 	%p1, %r4, %r1;
	@%p1 bra 	$L__BB2_48;

	cvta.to.global.u64 	%rd18, %rd12;
	mul.wide.s32 	%rd19, %r2, 4;
	add.s64 	%rd20, %rd18, %rd19;
	ld.global.nc.u32 	%r5, [%rd20];
	setp.eq.s32 	%p2, %r5, -1;
	@%p2 bra 	$L__BB2_39;

	mov.f32 	%f7, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f7, %f7, %f7, %f7};
	shl.b32 	%r6, %r3, 6;
	setp.gt.s32 	%p3, %r73, 0;
	@%p3 bra 	$L__BB2_3;
	bra.uni 	$L__BB2_29;

$L__BB2_3:
	mov.u32 	%r7, %tid.y;
	mov.u32 	%r8, %ntid.y;
	mul.lo.s32 	%r9, %r5, %r77;
	mov.u32 	%r10, %tid.x;
	shl.b32 	%r103, %r10, 1;
	mov.u32 	%r104, smem;
	add.s32 	%r11, %r104, %r103;
	mov.u32 	%r12, %ntid.x;
	shl.b32 	%r13, %r12, 1;
	cvta.to.global.u64 	%rd4, %rd10;
	cvta.to.global.u64 	%rd5, %rd9;
	add.s32 	%r105, %r73, 31;
	shr.s32 	%r106, %r105, 31;
	shr.u32 	%r107, %r106, 27;
	add.s32 	%r108, %r105, %r107;
	shr.s32 	%r14, %r108, 5;
	mov.u32 	%r137, 0;

$L__BB2_4:
	shl.b32 	%r16, %r137, 5;
	setp.gt.s32 	%p4, %r7, 63;
	@%p4 bra 	$L__BB2_14;

	add.s32 	%r17, %r10, %r16;
	mov.u32 	%r138, %r7;

$L__BB2_6:
	add.s32 	%r19, %r138, %r4;
	setp.ge.s32 	%p5, %r19, %r1;
	mov.u32 	%r139, %r74;
	@%p5 bra 	$L__BB2_8;

	mul.wide.s32 	%rd21, %r19, 4;
	add.s64 	%rd22, %rd2, %rd21;
	ld.global.nc.u32 	%r139, [%rd22];

$L__BB2_8:
	setp.gt.s32 	%p6, %r10, 31;
	@%p6 bra 	$L__BB2_13;

	div.s32 	%r109, %r139, %r75;
	mad.lo.s32 	%r142, %r76, %r109, %r17;
	shl.b32 	%r110, %r138, 6;
	add.s32 	%r140, %r11, %r110;
	mov.u32 	%r141, %r17;
	mov.u32 	%r143, %r10;

$L__BB2_10:
	setp.ge.s32 	%p7, %r141, %r73;
	setp.ge.s32 	%p8, %r139, %r74;
	mov.f32 	%f114, 0f00000000;
	or.pred  	%p9, %p8, %p7;
	@%p9 bra 	$L__BB2_12;

	mul.wide.s32 	%rd23, %r142, 2;
	add.s64 	%rd24, %rd5, %rd23;
	ld.global.nc.u16 	%rs2, [%rd24];
	// begin inline asm
	{ mov.b32 %f114, {0,%rs2};}

	// end inline asm

$L__BB2_12:
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs3, %f114;}

	// end inline asm
	st.shared.u16 	[%r140], %rs3;
	add.s32 	%r142, %r142, %r12;
	add.s32 	%r141, %r141, %r12;
	add.s32 	%r140, %r140, %r13;
	add.s32 	%r143, %r143, %r12;
	setp.lt.s32 	%p10, %r143, 32;
	@%p10 bra 	$L__BB2_10;

$L__BB2_13:
	add.s32 	%r138, %r138, %r8;
	setp.lt.s32 	%p11, %r138, 64;
	@%p11 bra 	$L__BB2_6;

$L__BB2_14:
	setp.gt.s32 	%p12, %r7, 31;
	@%p12 bra 	$L__BB2_22;

	mov.u32 	%r144, %r7;

$L__BB2_16:
	add.s32 	%r34, %r144, %r16;
	setp.gt.s32 	%p13, %r10, 63;
	@%p13 bra 	$L__BB2_21;

	mad.lo.s32 	%r35, %r34, %r78, %r9;
	shl.b32 	%r36, %r144, 6;
	mov.u32 	%r145, %r10;

$L__BB2_18:
	add.s32 	%r38, %r145, %r6;
	setp.ge.s32 	%p14, %r38, %r72;
	setp.ge.s32 	%p15, %r34, %r73;
	mov.f32 	%f115, 0f00000000;
	or.pred  	%p16, %p15, %p14;
	@%p16 bra 	$L__BB2_20;

	mad.lo.s32 	%r111, %r38, %r79, %r35;
	mul.wide.s32 	%rd25, %r111, 2;
	add.s64 	%rd26, %rd4, %rd25;
	ld.global.nc.u16 	%rs4, [%rd26];
	// begin inline asm
	{ mov.b32 %f115, {0,%rs4};}

	// end inline asm

$L__BB2_20:
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs5, %f115;}

	// end inline asm
	add.s32 	%r112, %r145, %r36;
	shl.b32 	%r113, %r112, 1;
	add.s32 	%r115, %r104, %r113;
	st.shared.u16 	[%r115+4096], %rs5;
	add.s32 	%r145, %r145, %r12;
	setp.lt.s32 	%p17, %r145, 64;
	@%p17 bra 	$L__BB2_18;

$L__BB2_21:
	add.s32 	%r144, %r144, %r8;
	setp.lt.s32 	%p18, %r144, 32;
	@%p18 bra 	$L__BB2_16;

$L__BB2_22:
	bar.sync 	0;
	mov.u32 	%r116, 0;
	mov.u32 	%r146, %r116;

$L__BB2_23:
	shr.u32 	%r118, %r146, 5;
	cvt.u64.u32 	%rd6, %r118;
	shl.b32 	%r119, %r146, 6;
	add.s32 	%r42, %r104, %r119;
	ld.shared.u16 	%rs1, [%r42];
	mov.u32 	%r147, %r116;

$L__BB2_24:
	// begin inline asm
	{ mov.b32 %f14, {0,%rs1};}

	// end inline asm
	shl.b32 	%r121, %r147, 1;
	add.s32 	%r123, %r104, %r121;
	ld.shared.u16 	%rs7, [%r123+4096];
	// begin inline asm
	{ mov.b32 %f15, {0,%rs7};}

	// end inline asm
	fma.rn.ftz.f32 	%f78, %f14, %f15, 0f00000000;
	ld.shared.u16 	%rs8, [%r42+2];
	// begin inline asm
	{ mov.b32 %f16, {0,%rs8};}

	// end inline asm
	ld.shared.u16 	%rs9, [%r123+4224];
	// begin inline asm
	{ mov.b32 %f17, {0,%rs9};}

	// end inline asm
	fma.rn.ftz.f32 	%f79, %f16, %f17, %f78;
	ld.shared.u16 	%rs10, [%r42+4];
	// begin inline asm
	{ mov.b32 %f18, {0,%rs10};}

	// end inline asm
	ld.shared.u16 	%rs11, [%r123+4352];
	// begin inline asm
	{ mov.b32 %f19, {0,%rs11};}

	// end inline asm
	fma.rn.ftz.f32 	%f80, %f18, %f19, %f79;
	ld.shared.u16 	%rs12, [%r42+6];
	// begin inline asm
	{ mov.b32 %f20, {0,%rs12};}

	// end inline asm
	ld.shared.u16 	%rs13, [%r123+4480];
	// begin inline asm
	{ mov.b32 %f21, {0,%rs13};}

	// end inline asm
	fma.rn.ftz.f32 	%f81, %f20, %f21, %f80;
	ld.shared.u16 	%rs14, [%r42+8];
	// begin inline asm
	{ mov.b32 %f22, {0,%rs14};}

	// end inline asm
	ld.shared.u16 	%rs15, [%r123+4608];
	// begin inline asm
	{ mov.b32 %f23, {0,%rs15};}

	// end inline asm
	fma.rn.ftz.f32 	%f82, %f22, %f23, %f81;
	ld.shared.u16 	%rs16, [%r42+10];
	// begin inline asm
	{ mov.b32 %f24, {0,%rs16};}

	// end inline asm
	ld.shared.u16 	%rs17, [%r123+4736];
	// begin inline asm
	{ mov.b32 %f25, {0,%rs17};}

	// end inline asm
	fma.rn.ftz.f32 	%f83, %f24, %f25, %f82;
	ld.shared.u16 	%rs18, [%r42+12];
	// begin inline asm
	{ mov.b32 %f26, {0,%rs18};}

	// end inline asm
	ld.shared.u16 	%rs19, [%r123+4864];
	// begin inline asm
	{ mov.b32 %f27, {0,%rs19};}

	// end inline asm
	fma.rn.ftz.f32 	%f84, %f26, %f27, %f83;
	ld.shared.u16 	%rs20, [%r42+14];
	// begin inline asm
	{ mov.b32 %f28, {0,%rs20};}

	// end inline asm
	ld.shared.u16 	%rs21, [%r123+4992];
	// begin inline asm
	{ mov.b32 %f29, {0,%rs21};}

	// end inline asm
	fma.rn.ftz.f32 	%f85, %f28, %f29, %f84;
	ld.shared.u16 	%rs22, [%r42+16];
	// begin inline asm
	{ mov.b32 %f30, {0,%rs22};}

	// end inline asm
	ld.shared.u16 	%rs23, [%r123+5120];
	// begin inline asm
	{ mov.b32 %f31, {0,%rs23};}

	// end inline asm
	fma.rn.ftz.f32 	%f86, %f30, %f31, %f85;
	ld.shared.u16 	%rs24, [%r42+18];
	// begin inline asm
	{ mov.b32 %f32, {0,%rs24};}

	// end inline asm
	ld.shared.u16 	%rs25, [%r123+5248];
	// begin inline asm
	{ mov.b32 %f33, {0,%rs25};}

	// end inline asm
	fma.rn.ftz.f32 	%f87, %f32, %f33, %f86;
	ld.shared.u16 	%rs26, [%r42+20];
	// begin inline asm
	{ mov.b32 %f34, {0,%rs26};}

	// end inline asm
	ld.shared.u16 	%rs27, [%r123+5376];
	// begin inline asm
	{ mov.b32 %f35, {0,%rs27};}

	// end inline asm
	fma.rn.ftz.f32 	%f88, %f34, %f35, %f87;
	ld.shared.u16 	%rs28, [%r42+22];
	// begin inline asm
	{ mov.b32 %f36, {0,%rs28};}

	// end inline asm
	ld.shared.u16 	%rs29, [%r123+5504];
	// begin inline asm
	{ mov.b32 %f37, {0,%rs29};}

	// end inline asm
	fma.rn.ftz.f32 	%f89, %f36, %f37, %f88;
	ld.shared.u16 	%rs30, [%r42+24];
	// begin inline asm
	{ mov.b32 %f38, {0,%rs30};}

	// end inline asm
	ld.shared.u16 	%rs31, [%r123+5632];
	// begin inline asm
	{ mov.b32 %f39, {0,%rs31};}

	// end inline asm
	fma.rn.ftz.f32 	%f90, %f38, %f39, %f89;
	ld.shared.u16 	%rs32, [%r42+26];
	// begin inline asm
	{ mov.b32 %f40, {0,%rs32};}

	// end inline asm
	ld.shared.u16 	%rs33, [%r123+5760];
	// begin inline asm
	{ mov.b32 %f41, {0,%rs33};}

	// end inline asm
	fma.rn.ftz.f32 	%f91, %f40, %f41, %f90;
	ld.shared.u16 	%rs34, [%r42+28];
	// begin inline asm
	{ mov.b32 %f42, {0,%rs34};}

	// end inline asm
	ld.shared.u16 	%rs35, [%r123+5888];
	// begin inline asm
	{ mov.b32 %f43, {0,%rs35};}

	// end inline asm
	fma.rn.ftz.f32 	%f92, %f42, %f43, %f91;
	ld.shared.u16 	%rs36, [%r42+30];
	// begin inline asm
	{ mov.b32 %f44, {0,%rs36};}

	// end inline asm
	ld.shared.u16 	%rs37, [%r123+6016];
	// begin inline asm
	{ mov.b32 %f45, {0,%rs37};}

	// end inline asm
	fma.rn.ftz.f32 	%f93, %f44, %f45, %f92;
	ld.shared.u16 	%rs38, [%r42+32];
	// begin inline asm
	{ mov.b32 %f46, {0,%rs38};}

	// end inline asm
	ld.shared.u16 	%rs39, [%r123+6144];
	// begin inline asm
	{ mov.b32 %f47, {0,%rs39};}

	// end inline asm
	fma.rn.ftz.f32 	%f94, %f46, %f47, %f93;
	ld.shared.u16 	%rs40, [%r42+34];
	// begin inline asm
	{ mov.b32 %f48, {0,%rs40};}

	// end inline asm
	ld.shared.u16 	%rs41, [%r123+6272];
	// begin inline asm
	{ mov.b32 %f49, {0,%rs41};}

	// end inline asm
	fma.rn.ftz.f32 	%f95, %f48, %f49, %f94;
	ld.shared.u16 	%rs42, [%r42+36];
	// begin inline asm
	{ mov.b32 %f50, {0,%rs42};}

	// end inline asm
	ld.shared.u16 	%rs43, [%r123+6400];
	// begin inline asm
	{ mov.b32 %f51, {0,%rs43};}

	// end inline asm
	fma.rn.ftz.f32 	%f96, %f50, %f51, %f95;
	ld.shared.u16 	%rs44, [%r42+38];
	// begin inline asm
	{ mov.b32 %f52, {0,%rs44};}

	// end inline asm
	ld.shared.u16 	%rs45, [%r123+6528];
	// begin inline asm
	{ mov.b32 %f53, {0,%rs45};}

	// end inline asm
	fma.rn.ftz.f32 	%f97, %f52, %f53, %f96;
	ld.shared.u16 	%rs46, [%r42+40];
	// begin inline asm
	{ mov.b32 %f54, {0,%rs46};}

	// end inline asm
	ld.shared.u16 	%rs47, [%r123+6656];
	// begin inline asm
	{ mov.b32 %f55, {0,%rs47};}

	// end inline asm
	fma.rn.ftz.f32 	%f98, %f54, %f55, %f97;
	ld.shared.u16 	%rs48, [%r42+42];
	// begin inline asm
	{ mov.b32 %f56, {0,%rs48};}

	// end inline asm
	ld.shared.u16 	%rs49, [%r123+6784];
	// begin inline asm
	{ mov.b32 %f57, {0,%rs49};}

	// end inline asm
	fma.rn.ftz.f32 	%f99, %f56, %f57, %f98;
	ld.shared.u16 	%rs50, [%r42+44];
	// begin inline asm
	{ mov.b32 %f58, {0,%rs50};}

	// end inline asm
	ld.shared.u16 	%rs51, [%r123+6912];
	// begin inline asm
	{ mov.b32 %f59, {0,%rs51};}

	// end inline asm
	fma.rn.ftz.f32 	%f100, %f58, %f59, %f99;
	ld.shared.u16 	%rs52, [%r42+46];
	// begin inline asm
	{ mov.b32 %f60, {0,%rs52};}

	// end inline asm
	ld.shared.u16 	%rs53, [%r123+7040];
	// begin inline asm
	{ mov.b32 %f61, {0,%rs53};}

	// end inline asm
	fma.rn.ftz.f32 	%f101, %f60, %f61, %f100;
	ld.shared.u16 	%rs54, [%r42+48];
	// begin inline asm
	{ mov.b32 %f62, {0,%rs54};}

	// end inline asm
	ld.shared.u16 	%rs55, [%r123+7168];
	// begin inline asm
	{ mov.b32 %f63, {0,%rs55};}

	// end inline asm
	fma.rn.ftz.f32 	%f102, %f62, %f63, %f101;
	ld.shared.u16 	%rs56, [%r42+50];
	// begin inline asm
	{ mov.b32 %f64, {0,%rs56};}

	// end inline asm
	ld.shared.u16 	%rs57, [%r123+7296];
	// begin inline asm
	{ mov.b32 %f65, {0,%rs57};}

	// end inline asm
	fma.rn.ftz.f32 	%f103, %f64, %f65, %f102;
	ld.shared.u16 	%rs58, [%r42+52];
	// begin inline asm
	{ mov.b32 %f66, {0,%rs58};}

	// end inline asm
	ld.shared.u16 	%rs59, [%r123+7424];
	// begin inline asm
	{ mov.b32 %f67, {0,%rs59};}

	// end inline asm
	fma.rn.ftz.f32 	%f104, %f66, %f67, %f103;
	ld.shared.u16 	%rs60, [%r42+54];
	// begin inline asm
	{ mov.b32 %f68, {0,%rs60};}

	// end inline asm
	ld.shared.u16 	%rs61, [%r123+7552];
	// begin inline asm
	{ mov.b32 %f69, {0,%rs61};}

	// end inline asm
	fma.rn.ftz.f32 	%f105, %f68, %f69, %f104;
	ld.shared.u16 	%rs62, [%r42+56];
	// begin inline asm
	{ mov.b32 %f70, {0,%rs62};}

	// end inline asm
	ld.shared.u16 	%rs63, [%r123+7680];
	// begin inline asm
	{ mov.b32 %f71, {0,%rs63};}

	// end inline asm
	fma.rn.ftz.f32 	%f106, %f70, %f71, %f105;
	ld.shared.u16 	%rs64, [%r42+58];
	// begin inline asm
	{ mov.b32 %f72, {0,%rs64};}

	// end inline asm
	ld.shared.u16 	%rs65, [%r123+7808];
	// begin inline asm
	{ mov.b32 %f73, {0,%rs65};}

	// end inline asm
	fma.rn.ftz.f32 	%f107, %f72, %f73, %f106;
	ld.shared.u16 	%rs66, [%r42+60];
	// begin inline asm
	{ mov.b32 %f74, {0,%rs66};}

	// end inline asm
	ld.shared.u16 	%rs67, [%r123+7936];
	// begin inline asm
	{ mov.b32 %f75, {0,%rs67};}

	// end inline asm
	fma.rn.ftz.f32 	%f108, %f74, %f75, %f107;
	ld.shared.u16 	%rs68, [%r42+62];
	// begin inline asm
	{ mov.b32 %f76, {0,%rs68};}

	// end inline asm
	ld.shared.u16 	%rs69, [%r123+8064];
	// begin inline asm
	{ mov.b32 %f77, {0,%rs69};}

	// end inline asm
	fma.rn.ftz.f32 	%f5, %f76, %f77, %f108;
	or.b32  	%r124, %r147, %r146;
	and.b32  	%r125, %r124, 2147483584;
	setp.ne.s32 	%p19, %r125, 0;
	@%p19 bra 	$L__BB2_26;

	shr.u32 	%r126, %r147, 5;
	shl.b64 	%rd27, %rd6, 3;
	add.s64 	%rd28, %rd3, %rd27;
	mul.wide.u32 	%rd29, %r126, 4;
	add.s64 	%rd30, %rd28, %rd29;
	ld.local.f32 	%f109, [%rd30];
	add.ftz.f32 	%f110, %f5, %f109;
	st.local.f32 	[%rd30], %f110;

$L__BB2_26:
	add.s32 	%r147, %r147, 1;
	setp.ne.s32 	%p20, %r147, 64;
	@%p20 bra 	$L__BB2_24;

	add.s32 	%r146, %r146, 1;
	setp.lt.u32 	%p21, %r146, 64;
	@%p21 bra 	$L__BB2_23;

	bar.sync 	0;
	add.s32 	%r137, %r137, 1;
	setp.lt.s32 	%p22, %r137, %r14;
	@%p22 bra 	$L__BB2_4;

$L__BB2_29:
	mov.u32 	%r148, %tid.y;
	setp.gt.s32 	%p23, %r148, 63;
	@%p23 bra 	$L__BB2_48;

	mov.u32 	%r48, %ntid.y;
	mov.u32 	%r49, %tid.x;
	mov.u32 	%r50, %ntid.x;
	cvta.to.global.u64 	%rd7, %rd11;

$L__BB2_31:
	add.s32 	%r52, %r148, %r4;
	setp.ge.s32 	%p24, %r52, %r1;
	@%p24 bra 	$L__BB2_38;

	mul.wide.s32 	%rd31, %r52, 4;
	add.s64 	%rd32, %rd2, %rd31;
	ld.global.nc.u32 	%r53, [%rd32];
	setp.ge.s32 	%p25, %r53, %r74;
	@%p25 bra 	$L__BB2_38;

	setp.gt.s32 	%p26, %r49, 63;
	@%p26 bra 	$L__BB2_38;

	mul.wide.s32 	%rd33, %r53, 4;
	add.s64 	%rd34, %rd7, %rd33;
	ld.global.nc.f32 	%f6, [%rd34];
	shr.s32 	%r127, %r148, 31;
	shr.u32 	%r128, %r127, 27;
	add.s32 	%r129, %r148, %r128;
	shr.s32 	%r130, %r129, 5;
	cvt.s64.s32 	%rd8, %r130;
	mul.lo.s32 	%r54, %r53, %r80;
	mov.u32 	%r149, %r49;

$L__BB2_35:
	add.s32 	%r56, %r149, %r6;
	setp.ge.s32 	%p27, %r56, %r72;
	@%p27 bra 	$L__BB2_37;

	shr.s32 	%r131, %r149, 31;
	shr.u32 	%r132, %r131, 27;
	add.s32 	%r133, %r149, %r132;
	shr.s32 	%r134, %r133, 5;
	shl.b64 	%rd35, %rd8, 3;
	add.s64 	%rd36, %rd3, %rd35;
	mul.wide.s32 	%rd37, %r134, 4;
	add.s64 	%rd38, %rd36, %rd37;
	ld.local.f32 	%f112, [%rd38];
	mul.ftz.f32 	%f111, %f6, %f112;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs70, %f111;}

	// end inline asm
	mad.lo.s32 	%r135, %r56, %r81, %r54;
	mul.wide.s32 	%rd39, %r135, 2;
	add.s64 	%rd40, %rd1, %rd39;
	st.global.u16 	[%rd40], %rs70;

$L__BB2_37:
	add.s32 	%r149, %r149, %r50;
	setp.lt.s32 	%p28, %r149, 64;
	@%p28 bra 	$L__BB2_35;

$L__BB2_38:
	add.s32 	%r148, %r148, %r48;
	setp.lt.s32 	%p29, %r148, 64;
	@%p29 bra 	$L__BB2_31;
	bra.uni 	$L__BB2_48;

$L__BB2_39:
	mov.u32 	%r150, %tid.y;
	setp.gt.s32 	%p30, %r150, 63;
	@%p30 bra 	$L__BB2_48;

	mov.u32 	%r60, %ntid.y;
	mov.u32 	%r61, %tid.x;
	mov.u32 	%r62, %ntid.x;
	shl.b32 	%r63, %r3, 6;

$L__BB2_41:
	add.s32 	%r65, %r150, %r4;
	setp.ge.s32 	%p31, %r65, %r1;
	@%p31 bra 	$L__BB2_47;

	setp.gt.s32 	%p32, %r61, 63;
	mul.wide.s32 	%rd41, %r65, 4;
	add.s64 	%rd42, %rd2, %rd41;
	ld.global.nc.u32 	%r66, [%rd42];
	setp.ge.s32 	%p33, %r66, %r74;
	or.pred  	%p34, %p33, %p32;
	@%p34 bra 	$L__BB2_47;

	mul.lo.s32 	%r67, %r66, %r80;
	mov.u32 	%r151, %r61;

$L__BB2_44:
	add.s32 	%r69, %r151, %r63;
	setp.ge.s32 	%p35, %r69, %r72;
	@%p35 bra 	$L__BB2_46;

	mov.f32 	%f113, 0f00000000;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs71, %f113;}

	// end inline asm
	mad.lo.s32 	%r136, %r69, %r81, %r67;
	mul.wide.s32 	%rd43, %r136, 2;
	add.s64 	%rd44, %rd1, %rd43;
	st.global.u16 	[%rd44], %rs71;

$L__BB2_46:
	add.s32 	%r151, %r151, %r62;
	setp.lt.s32 	%p36, %r151, 64;
	@%p36 bra 	$L__BB2_44;

$L__BB2_47:
	add.s32 	%r150, %r150, %r60;
	setp.lt.s32 	%p37, %r150, 64;
	@%p37 bra 	$L__BB2_41;

$L__BB2_48:
	ret;

}
	// .globl	fused_moe_gemm_64_64_32_unweighted
.visible .entry fused_moe_gemm_64_64_32_unweighted(
	.param .u64 fused_moe_gemm_64_64_32_unweighted_param_0,
	.param .u64 fused_moe_gemm_64_64_32_unweighted_param_1,
	.param .u64 fused_moe_gemm_64_64_32_unweighted_param_2,
	.param .u64 fused_moe_gemm_64_64_32_unweighted_param_3,
	.param .u64 fused_moe_gemm_64_64_32_unweighted_param_4,
	.param .u64 fused_moe_gemm_64_64_32_unweighted_param_5,
	.param .u64 fused_moe_gemm_64_64_32_unweighted_param_6,
	.param .u32 fused_moe_gemm_64_64_32_unweighted_param_7,
	.param .u32 fused_moe_gemm_64_64_32_unweighted_param_8,
	.param .u32 fused_moe_gemm_64_64_32_unweighted_param_9,
	.param .u32 fused_moe_gemm_64_64_32_unweighted_param_10,
	.param .u32 fused_moe_gemm_64_64_32_unweighted_param_11,
	.param .u32 fused_moe_gemm_64_64_32_unweighted_param_12,
	.param .u32 fused_moe_gemm_64_64_32_unweighted_param_13,
	.param .u32 fused_moe_gemm_64_64_32_unweighted_param_14,
	.param .u32 fused_moe_gemm_64_64_32_unweighted_param_15,
	.param .u32 fused_moe_gemm_64_64_32_unweighted_param_16
)
{
	.local .align 16 .b8 	__local_depot3[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<39>;
	.reg .b16 	%rs<72>;
	.reg .f32 	%f<114>;
	.reg .b32 	%r<152>;
	.reg .b64 	%rd<41>;


	mov.u64 	%SPL, __local_depot3;
	ld.param.u64 	%rd8, [fused_moe_gemm_64_64_32_unweighted_param_0];
	ld.param.u64 	%rd9, [fused_moe_gemm_64_64_32_unweighted_param_1];
	ld.param.u64 	%rd11, [fused_moe_gemm_64_64_32_unweighted_param_2];
	ld.param.u64 	%rd12, [fused_moe_gemm_64_64_32_unweighted_param_4];
	ld.param.u64 	%rd10, [fused_moe_gemm_64_64_32_unweighted_param_5];
	ld.param.u64 	%rd13, [fused_moe_gemm_64_64_32_unweighted_param_6];
	ld.param.u32 	%r72, [fused_moe_gemm_64_64_32_unweighted_param_7];
	ld.param.u32 	%r73, [fused_moe_gemm_64_64_32_unweighted_param_8];
	ld.param.u32 	%r74, [fused_moe_gemm_64_64_32_unweighted_param_9];
	ld.param.u32 	%r75, [fused_moe_gemm_64_64_32_unweighted_param_10];
	ld.param.u32 	%r76, [fused_moe_gemm_64_64_32_unweighted_param_11];
	ld.param.u32 	%r77, [fused_moe_gemm_64_64_32_unweighted_param_12];
	ld.param.u32 	%r78, [fused_moe_gemm_64_64_32_unweighted_param_13];
	ld.param.u32 	%r79, [fused_moe_gemm_64_64_32_unweighted_param_14];
	ld.param.u32 	%r80, [fused_moe_gemm_64_64_32_unweighted_param_15];
	ld.param.u32 	%r81, [fused_moe_gemm_64_64_32_unweighted_param_16];
	cvta.to.global.u64 	%rd1, %rd11;
	cvta.to.global.u64 	%rd2, %rd12;
	add.u64 	%rd3, %SPL, 0;
	cvta.to.global.u64 	%rd15, %rd13;
	ld.global.nc.u32 	%r1, [%rd15];
	add.s32 	%r82, %r1, 63;
	shr.s32 	%r83, %r82, 31;
	shr.u32 	%r84, %r83, 26;
	add.s32 	%r85, %r82, %r84;
	shr.s32 	%r86, %r85, 6;
	add.s32 	%r87, %r72, 63;
	shr.s32 	%r88, %r87, 31;
	shr.u32 	%r89, %r88, 26;
	add.s32 	%r90, %r87, %r89;
	shr.s32 	%r91, %r90, 6;
	shl.b32 	%r92, %r91, 3;
	mov.u32 	%r93, %ctaid.x;
	div.s32 	%r94, %r93, %r92;
	shl.b32 	%r95, %r94, 3;
	sub.s32 	%r96, %r86, %r95;
	min.s32 	%r97, %r96, 8;
	mul.lo.s32 	%r98, %r94, %r92;
	sub.s32 	%r99, %r93, %r98;
	div.s32 	%r3, %r99, %r97;
	mul.lo.s32 	%r100, %r3, %r97;
	sub.s32 	%r101, %r99, %r100;
	add.s32 	%r2, %r101, %r95;
	shl.b32 	%r4, %r2, 6;
	setp.ge.s32 	%p1, %r4, %r1;
	@%p1 bra 	$L__BB3_47;

	cvta.to.global.u64 	%rd16, %rd10;
	mul.wide.s32 	%rd17, %r2, 4;
	add.s64 	%rd18, %rd16, %rd17;
	ld.global.nc.u32 	%r5, [%rd18];
	setp.eq.s32 	%p2, %r5, -1;
	@%p2 bra 	$L__BB3_38;

	mov.f32 	%f6, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f6, %f6, %f6, %f6};
	shl.b32 	%r6, %r3, 6;
	setp.gt.s32 	%p3, %r73, 0;
	@%p3 bra 	$L__BB3_3;
	bra.uni 	$L__BB3_29;

$L__BB3_3:
	mov.u32 	%r7, %tid.y;
	mov.u32 	%r8, %ntid.y;
	mul.lo.s32 	%r9, %r5, %r77;
	mov.u32 	%r10, %tid.x;
	shl.b32 	%r103, %r10, 1;
	mov.u32 	%r104, smem;
	add.s32 	%r11, %r104, %r103;
	mov.u32 	%r12, %ntid.x;
	shl.b32 	%r13, %r12, 1;
	cvta.to.global.u64 	%rd4, %rd9;
	cvta.to.global.u64 	%rd5, %rd8;
	add.s32 	%r105, %r73, 31;
	shr.s32 	%r106, %r105, 31;
	shr.u32 	%r107, %r106, 27;
	add.s32 	%r108, %r105, %r107;
	shr.s32 	%r14, %r108, 5;
	mov.u32 	%r137, 0;

$L__BB3_4:
	shl.b32 	%r16, %r137, 5;
	setp.gt.s32 	%p4, %r7, 63;
	@%p4 bra 	$L__BB3_14;

	add.s32 	%r17, %r10, %r16;
	mov.u32 	%r138, %r7;

$L__BB3_6:
	add.s32 	%r19, %r138, %r4;
	setp.ge.s32 	%p5, %r19, %r1;
	mov.u32 	%r139, %r74;
	@%p5 bra 	$L__BB3_8;

	mul.wide.s32 	%rd19, %r19, 4;
	add.s64 	%rd20, %rd2, %rd19;
	ld.global.nc.u32 	%r139, [%rd20];

$L__BB3_8:
	setp.gt.s32 	%p6, %r10, 31;
	@%p6 bra 	$L__BB3_13;

	div.s32 	%r109, %r139, %r75;
	mad.lo.s32 	%r142, %r76, %r109, %r17;
	shl.b32 	%r110, %r138, 6;
	add.s32 	%r140, %r11, %r110;
	mov.u32 	%r141, %r17;
	mov.u32 	%r143, %r10;

$L__BB3_10:
	setp.ge.s32 	%p7, %r141, %r73;
	setp.ge.s32 	%p8, %r139, %r74;
	mov.f32 	%f112, 0f00000000;
	or.pred  	%p9, %p8, %p7;
	@%p9 bra 	$L__BB3_12;

	mul.wide.s32 	%rd21, %r142, 2;
	add.s64 	%rd22, %rd5, %rd21;
	ld.global.nc.u16 	%rs2, [%rd22];
	// begin inline asm
	{ mov.b32 %f112, {0,%rs2};}

	// end inline asm

$L__BB3_12:
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs3, %f112;}

	// end inline asm
	st.shared.u16 	[%r140], %rs3;
	add.s32 	%r142, %r142, %r12;
	add.s32 	%r141, %r141, %r12;
	add.s32 	%r140, %r140, %r13;
	add.s32 	%r143, %r143, %r12;
	setp.lt.s32 	%p10, %r143, 32;
	@%p10 bra 	$L__BB3_10;

$L__BB3_13:
	add.s32 	%r138, %r138, %r8;
	setp.lt.s32 	%p11, %r138, 64;
	@%p11 bra 	$L__BB3_6;

$L__BB3_14:
	setp.gt.s32 	%p12, %r7, 31;
	@%p12 bra 	$L__BB3_22;

	mov.u32 	%r144, %r7;

$L__BB3_16:
	add.s32 	%r34, %r144, %r16;
	setp.gt.s32 	%p13, %r10, 63;
	@%p13 bra 	$L__BB3_21;

	mad.lo.s32 	%r35, %r34, %r78, %r9;
	shl.b32 	%r36, %r144, 6;
	mov.u32 	%r145, %r10;

$L__BB3_18:
	add.s32 	%r38, %r145, %r6;
	setp.ge.s32 	%p14, %r38, %r72;
	setp.ge.s32 	%p15, %r34, %r73;
	mov.f32 	%f113, 0f00000000;
	or.pred  	%p16, %p15, %p14;
	@%p16 bra 	$L__BB3_20;

	mad.lo.s32 	%r111, %r38, %r79, %r35;
	mul.wide.s32 	%rd23, %r111, 2;
	add.s64 	%rd24, %rd4, %rd23;
	ld.global.nc.u16 	%rs4, [%rd24];
	// begin inline asm
	{ mov.b32 %f113, {0,%rs4};}

	// end inline asm

$L__BB3_20:
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs5, %f113;}

	// end inline asm
	add.s32 	%r112, %r145, %r36;
	shl.b32 	%r113, %r112, 1;
	add.s32 	%r115, %r104, %r113;
	st.shared.u16 	[%r115+4096], %rs5;
	add.s32 	%r145, %r145, %r12;
	setp.lt.s32 	%p17, %r145, 64;
	@%p17 bra 	$L__BB3_18;

$L__BB3_21:
	add.s32 	%r144, %r144, %r8;
	setp.lt.s32 	%p18, %r144, 32;
	@%p18 bra 	$L__BB3_16;

$L__BB3_22:
	bar.sync 	0;
	mov.u32 	%r116, 0;
	mov.u32 	%r146, %r116;

$L__BB3_23:
	shr.u32 	%r118, %r146, 5;
	cvt.u64.u32 	%rd6, %r118;
	shl.b32 	%r119, %r146, 6;
	add.s32 	%r42, %r104, %r119;
	ld.shared.u16 	%rs1, [%r42];
	mov.u32 	%r147, %r116;

$L__BB3_24:
	// begin inline asm
	{ mov.b32 %f13, {0,%rs1};}

	// end inline asm
	shl.b32 	%r121, %r147, 1;
	add.s32 	%r123, %r104, %r121;
	ld.shared.u16 	%rs7, [%r123+4096];
	// begin inline asm
	{ mov.b32 %f14, {0,%rs7};}

	// end inline asm
	fma.rn.ftz.f32 	%f77, %f13, %f14, 0f00000000;
	ld.shared.u16 	%rs8, [%r42+2];
	// begin inline asm
	{ mov.b32 %f15, {0,%rs8};}

	// end inline asm
	ld.shared.u16 	%rs9, [%r123+4224];
	// begin inline asm
	{ mov.b32 %f16, {0,%rs9};}

	// end inline asm
	fma.rn.ftz.f32 	%f78, %f15, %f16, %f77;
	ld.shared.u16 	%rs10, [%r42+4];
	// begin inline asm
	{ mov.b32 %f17, {0,%rs10};}

	// end inline asm
	ld.shared.u16 	%rs11, [%r123+4352];
	// begin inline asm
	{ mov.b32 %f18, {0,%rs11};}

	// end inline asm
	fma.rn.ftz.f32 	%f79, %f17, %f18, %f78;
	ld.shared.u16 	%rs12, [%r42+6];
	// begin inline asm
	{ mov.b32 %f19, {0,%rs12};}

	// end inline asm
	ld.shared.u16 	%rs13, [%r123+4480];
	// begin inline asm
	{ mov.b32 %f20, {0,%rs13};}

	// end inline asm
	fma.rn.ftz.f32 	%f80, %f19, %f20, %f79;
	ld.shared.u16 	%rs14, [%r42+8];
	// begin inline asm
	{ mov.b32 %f21, {0,%rs14};}

	// end inline asm
	ld.shared.u16 	%rs15, [%r123+4608];
	// begin inline asm
	{ mov.b32 %f22, {0,%rs15};}

	// end inline asm
	fma.rn.ftz.f32 	%f81, %f21, %f22, %f80;
	ld.shared.u16 	%rs16, [%r42+10];
	// begin inline asm
	{ mov.b32 %f23, {0,%rs16};}

	// end inline asm
	ld.shared.u16 	%rs17, [%r123+4736];
	// begin inline asm
	{ mov.b32 %f24, {0,%rs17};}

	// end inline asm
	fma.rn.ftz.f32 	%f82, %f23, %f24, %f81;
	ld.shared.u16 	%rs18, [%r42+12];
	// begin inline asm
	{ mov.b32 %f25, {0,%rs18};}

	// end inline asm
	ld.shared.u16 	%rs19, [%r123+4864];
	// begin inline asm
	{ mov.b32 %f26, {0,%rs19};}

	// end inline asm
	fma.rn.ftz.f32 	%f83, %f25, %f26, %f82;
	ld.shared.u16 	%rs20, [%r42+14];
	// begin inline asm
	{ mov.b32 %f27, {0,%rs20};}

	// end inline asm
	ld.shared.u16 	%rs21, [%r123+4992];
	// begin inline asm
	{ mov.b32 %f28, {0,%rs21};}

	// end inline asm
	fma.rn.ftz.f32 	%f84, %f27, %f28, %f83;
	ld.shared.u16 	%rs22, [%r42+16];
	// begin inline asm
	{ mov.b32 %f29, {0,%rs22};}

	// end inline asm
	ld.shared.u16 	%rs23, [%r123+5120];
	// begin inline asm
	{ mov.b32 %f30, {0,%rs23};}

	// end inline asm
	fma.rn.ftz.f32 	%f85, %f29, %f30, %f84;
	ld.shared.u16 	%rs24, [%r42+18];
	// begin inline asm
	{ mov.b32 %f31, {0,%rs24};}

	// end inline asm
	ld.shared.u16 	%rs25, [%r123+5248];
	// begin inline asm
	{ mov.b32 %f32, {0,%rs25};}

	// end inline asm
	fma.rn.ftz.f32 	%f86, %f31, %f32, %f85;
	ld.shared.u16 	%rs26, [%r42+20];
	// begin inline asm
	{ mov.b32 %f33, {0,%rs26};}

	// end inline asm
	ld.shared.u16 	%rs27, [%r123+5376];
	// begin inline asm
	{ mov.b32 %f34, {0,%rs27};}

	// end inline asm
	fma.rn.ftz.f32 	%f87, %f33, %f34, %f86;
	ld.shared.u16 	%rs28, [%r42+22];
	// begin inline asm
	{ mov.b32 %f35, {0,%rs28};}

	// end inline asm
	ld.shared.u16 	%rs29, [%r123+5504];
	// begin inline asm
	{ mov.b32 %f36, {0,%rs29};}

	// end inline asm
	fma.rn.ftz.f32 	%f88, %f35, %f36, %f87;
	ld.shared.u16 	%rs30, [%r42+24];
	// begin inline asm
	{ mov.b32 %f37, {0,%rs30};}

	// end inline asm
	ld.shared.u16 	%rs31, [%r123+5632];
	// begin inline asm
	{ mov.b32 %f38, {0,%rs31};}

	// end inline asm
	fma.rn.ftz.f32 	%f89, %f37, %f38, %f88;
	ld.shared.u16 	%rs32, [%r42+26];
	// begin inline asm
	{ mov.b32 %f39, {0,%rs32};}

	// end inline asm
	ld.shared.u16 	%rs33, [%r123+5760];
	// begin inline asm
	{ mov.b32 %f40, {0,%rs33};}

	// end inline asm
	fma.rn.ftz.f32 	%f90, %f39, %f40, %f89;
	ld.shared.u16 	%rs34, [%r42+28];
	// begin inline asm
	{ mov.b32 %f41, {0,%rs34};}

	// end inline asm
	ld.shared.u16 	%rs35, [%r123+5888];
	// begin inline asm
	{ mov.b32 %f42, {0,%rs35};}

	// end inline asm
	fma.rn.ftz.f32 	%f91, %f41, %f42, %f90;
	ld.shared.u16 	%rs36, [%r42+30];
	// begin inline asm
	{ mov.b32 %f43, {0,%rs36};}

	// end inline asm
	ld.shared.u16 	%rs37, [%r123+6016];
	// begin inline asm
	{ mov.b32 %f44, {0,%rs37};}

	// end inline asm
	fma.rn.ftz.f32 	%f92, %f43, %f44, %f91;
	ld.shared.u16 	%rs38, [%r42+32];
	// begin inline asm
	{ mov.b32 %f45, {0,%rs38};}

	// end inline asm
	ld.shared.u16 	%rs39, [%r123+6144];
	// begin inline asm
	{ mov.b32 %f46, {0,%rs39};}

	// end inline asm
	fma.rn.ftz.f32 	%f93, %f45, %f46, %f92;
	ld.shared.u16 	%rs40, [%r42+34];
	// begin inline asm
	{ mov.b32 %f47, {0,%rs40};}

	// end inline asm
	ld.shared.u16 	%rs41, [%r123+6272];
	// begin inline asm
	{ mov.b32 %f48, {0,%rs41};}

	// end inline asm
	fma.rn.ftz.f32 	%f94, %f47, %f48, %f93;
	ld.shared.u16 	%rs42, [%r42+36];
	// begin inline asm
	{ mov.b32 %f49, {0,%rs42};}

	// end inline asm
	ld.shared.u16 	%rs43, [%r123+6400];
	// begin inline asm
	{ mov.b32 %f50, {0,%rs43};}

	// end inline asm
	fma.rn.ftz.f32 	%f95, %f49, %f50, %f94;
	ld.shared.u16 	%rs44, [%r42+38];
	// begin inline asm
	{ mov.b32 %f51, {0,%rs44};}

	// end inline asm
	ld.shared.u16 	%rs45, [%r123+6528];
	// begin inline asm
	{ mov.b32 %f52, {0,%rs45};}

	// end inline asm
	fma.rn.ftz.f32 	%f96, %f51, %f52, %f95;
	ld.shared.u16 	%rs46, [%r42+40];
	// begin inline asm
	{ mov.b32 %f53, {0,%rs46};}

	// end inline asm
	ld.shared.u16 	%rs47, [%r123+6656];
	// begin inline asm
	{ mov.b32 %f54, {0,%rs47};}

	// end inline asm
	fma.rn.ftz.f32 	%f97, %f53, %f54, %f96;
	ld.shared.u16 	%rs48, [%r42+42];
	// begin inline asm
	{ mov.b32 %f55, {0,%rs48};}

	// end inline asm
	ld.shared.u16 	%rs49, [%r123+6784];
	// begin inline asm
	{ mov.b32 %f56, {0,%rs49};}

	// end inline asm
	fma.rn.ftz.f32 	%f98, %f55, %f56, %f97;
	ld.shared.u16 	%rs50, [%r42+44];
	// begin inline asm
	{ mov.b32 %f57, {0,%rs50};}

	// end inline asm
	ld.shared.u16 	%rs51, [%r123+6912];
	// begin inline asm
	{ mov.b32 %f58, {0,%rs51};}

	// end inline asm
	fma.rn.ftz.f32 	%f99, %f57, %f58, %f98;
	ld.shared.u16 	%rs52, [%r42+46];
	// begin inline asm
	{ mov.b32 %f59, {0,%rs52};}

	// end inline asm
	ld.shared.u16 	%rs53, [%r123+7040];
	// begin inline asm
	{ mov.b32 %f60, {0,%rs53};}

	// end inline asm
	fma.rn.ftz.f32 	%f100, %f59, %f60, %f99;
	ld.shared.u16 	%rs54, [%r42+48];
	// begin inline asm
	{ mov.b32 %f61, {0,%rs54};}

	// end inline asm
	ld.shared.u16 	%rs55, [%r123+7168];
	// begin inline asm
	{ mov.b32 %f62, {0,%rs55};}

	// end inline asm
	fma.rn.ftz.f32 	%f101, %f61, %f62, %f100;
	ld.shared.u16 	%rs56, [%r42+50];
	// begin inline asm
	{ mov.b32 %f63, {0,%rs56};}

	// end inline asm
	ld.shared.u16 	%rs57, [%r123+7296];
	// begin inline asm
	{ mov.b32 %f64, {0,%rs57};}

	// end inline asm
	fma.rn.ftz.f32 	%f102, %f63, %f64, %f101;
	ld.shared.u16 	%rs58, [%r42+52];
	// begin inline asm
	{ mov.b32 %f65, {0,%rs58};}

	// end inline asm
	ld.shared.u16 	%rs59, [%r123+7424];
	// begin inline asm
	{ mov.b32 %f66, {0,%rs59};}

	// end inline asm
	fma.rn.ftz.f32 	%f103, %f65, %f66, %f102;
	ld.shared.u16 	%rs60, [%r42+54];
	// begin inline asm
	{ mov.b32 %f67, {0,%rs60};}

	// end inline asm
	ld.shared.u16 	%rs61, [%r123+7552];
	// begin inline asm
	{ mov.b32 %f68, {0,%rs61};}

	// end inline asm
	fma.rn.ftz.f32 	%f104, %f67, %f68, %f103;
	ld.shared.u16 	%rs62, [%r42+56];
	// begin inline asm
	{ mov.b32 %f69, {0,%rs62};}

	// end inline asm
	ld.shared.u16 	%rs63, [%r123+7680];
	// begin inline asm
	{ mov.b32 %f70, {0,%rs63};}

	// end inline asm
	fma.rn.ftz.f32 	%f105, %f69, %f70, %f104;
	ld.shared.u16 	%rs64, [%r42+58];
	// begin inline asm
	{ mov.b32 %f71, {0,%rs64};}

	// end inline asm
	ld.shared.u16 	%rs65, [%r123+7808];
	// begin inline asm
	{ mov.b32 %f72, {0,%rs65};}

	// end inline asm
	fma.rn.ftz.f32 	%f106, %f71, %f72, %f105;
	ld.shared.u16 	%rs66, [%r42+60];
	// begin inline asm
	{ mov.b32 %f73, {0,%rs66};}

	// end inline asm
	ld.shared.u16 	%rs67, [%r123+7936];
	// begin inline asm
	{ mov.b32 %f74, {0,%rs67};}

	// end inline asm
	fma.rn.ftz.f32 	%f107, %f73, %f74, %f106;
	ld.shared.u16 	%rs68, [%r42+62];
	// begin inline asm
	{ mov.b32 %f75, {0,%rs68};}

	// end inline asm
	ld.shared.u16 	%rs69, [%r123+8064];
	// begin inline asm
	{ mov.b32 %f76, {0,%rs69};}

	// end inline asm
	fma.rn.ftz.f32 	%f5, %f75, %f76, %f107;
	or.b32  	%r124, %r147, %r146;
	and.b32  	%r125, %r124, 2147483584;
	setp.ne.s32 	%p19, %r125, 0;
	@%p19 bra 	$L__BB3_26;

	shr.u32 	%r126, %r147, 5;
	shl.b64 	%rd25, %rd6, 3;
	add.s64 	%rd26, %rd3, %rd25;
	mul.wide.u32 	%rd27, %r126, 4;
	add.s64 	%rd28, %rd26, %rd27;
	ld.local.f32 	%f108, [%rd28];
	add.ftz.f32 	%f109, %f5, %f108;
	st.local.f32 	[%rd28], %f109;

$L__BB3_26:
	add.s32 	%r147, %r147, 1;
	setp.ne.s32 	%p20, %r147, 64;
	@%p20 bra 	$L__BB3_24;

	add.s32 	%r146, %r146, 1;
	setp.lt.u32 	%p21, %r146, 64;
	@%p21 bra 	$L__BB3_23;

	bar.sync 	0;
	add.s32 	%r137, %r137, 1;
	setp.lt.s32 	%p22, %r137, %r14;
	@%p22 bra 	$L__BB3_4;

$L__BB3_29:
	mov.u32 	%r148, %tid.y;
	setp.gt.s32 	%p23, %r148, 63;
	@%p23 bra 	$L__BB3_47;

	mov.u32 	%r48, %ntid.y;
	mov.u32 	%r49, %tid.x;
	mov.u32 	%r50, %ntid.x;

$L__BB3_31:
	add.s32 	%r52, %r148, %r4;
	setp.ge.s32 	%p24, %r52, %r1;
	@%p24 bra 	$L__BB3_37;

	setp.gt.s32 	%p25, %r49, 63;
	mul.wide.s32 	%rd29, %r52, 4;
	add.s64 	%rd30, %rd2, %rd29;
	ld.global.nc.u32 	%r53, [%rd30];
	setp.ge.s32 	%p26, %r53, %r74;
	or.pred  	%p27, %p26, %p25;
	@%p27 bra 	$L__BB3_37;

	shr.s32 	%r127, %r148, 31;
	shr.u32 	%r128, %r127, 27;
	add.s32 	%r129, %r148, %r128;
	shr.s32 	%r130, %r129, 5;
	cvt.s64.s32 	%rd7, %r130;
	mul.lo.s32 	%r54, %r53, %r80;
	mov.u32 	%r149, %r49;

$L__BB3_34:
	add.s32 	%r56, %r149, %r6;
	setp.ge.s32 	%p28, %r56, %r72;
	@%p28 bra 	$L__BB3_36;

	shr.s32 	%r131, %r149, 31;
	shr.u32 	%r132, %r131, 27;
	add.s32 	%r133, %r149, %r132;
	shr.s32 	%r134, %r133, 5;
	shl.b64 	%rd31, %rd7, 3;
	add.s64 	%rd32, %rd3, %rd31;
	mul.wide.s32 	%rd33, %r134, 4;
	add.s64 	%rd34, %rd32, %rd33;
	ld.local.f32 	%f110, [%rd34];
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs70, %f110;}

	// end inline asm
	mad.lo.s32 	%r135, %r56, %r81, %r54;
	mul.wide.s32 	%rd35, %r135, 2;
	add.s64 	%rd36, %rd1, %rd35;
	st.global.u16 	[%rd36], %rs70;

$L__BB3_36:
	add.s32 	%r149, %r149, %r50;
	setp.lt.s32 	%p29, %r149, 64;
	@%p29 bra 	$L__BB3_34;

$L__BB3_37:
	add.s32 	%r148, %r148, %r48;
	setp.lt.s32 	%p30, %r148, 64;
	@%p30 bra 	$L__BB3_31;
	bra.uni 	$L__BB3_47;

$L__BB3_38:
	mov.u32 	%r150, %tid.y;
	setp.gt.s32 	%p31, %r150, 63;
	@%p31 bra 	$L__BB3_47;

	mov.u32 	%r60, %ntid.y;
	mov.u32 	%r61, %tid.x;
	mov.u32 	%r62, %ntid.x;
	shl.b32 	%r63, %r3, 6;

$L__BB3_40:
	add.s32 	%r65, %r150, %r4;
	setp.ge.s32 	%p32, %r65, %r1;
	@%p32 bra 	$L__BB3_46;

	setp.gt.s32 	%p33, %r61, 63;
	mul.wide.s32 	%rd37, %r65, 4;
	add.s64 	%rd38, %rd2, %rd37;
	ld.global.nc.u32 	%r66, [%rd38];
	setp.ge.s32 	%p34, %r66, %r74;
	or.pred  	%p35, %p34, %p33;
	@%p35 bra 	$L__BB3_46;

	mul.lo.s32 	%r67, %r66, %r80;
	mov.u32 	%r151, %r61;

$L__BB3_43:
	add.s32 	%r69, %r151, %r63;
	setp.ge.s32 	%p36, %r69, %r72;
	@%p36 bra 	$L__BB3_45;

	mov.f32 	%f111, 0f00000000;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs71, %f111;}

	// end inline asm
	mad.lo.s32 	%r136, %r69, %r81, %r67;
	mul.wide.s32 	%rd39, %r136, 2;
	add.s64 	%rd40, %rd1, %rd39;
	st.global.u16 	[%rd40], %rs71;

$L__BB3_45:
	add.s32 	%r151, %r151, %r62;
	setp.lt.s32 	%p37, %r151, 64;
	@%p37 bra 	$L__BB3_43;

$L__BB3_46:
	add.s32 	%r150, %r150, %r60;
	setp.lt.s32 	%p38, %r150, 64;
	@%p38 bra 	$L__BB3_40;

$L__BB3_47:
	ret;

}
	// .globl	fused_moe_gemm_simple_weighted
.visible .entry fused_moe_gemm_simple_weighted(
	.param .u64 fused_moe_gemm_simple_weighted_param_0,
	.param .u64 fused_moe_gemm_simple_weighted_param_1,
	.param .u64 fused_moe_gemm_simple_weighted_param_2,
	.param .u64 fused_moe_gemm_simple_weighted_param_3,
	.param .u64 fused_moe_gemm_simple_weighted_param_4,
	.param .u64 fused_moe_gemm_simple_weighted_param_5,
	.param .u64 fused_moe_gemm_simple_weighted_param_6,
	.param .u32 fused_moe_gemm_simple_weighted_param_7,
	.param .u32 fused_moe_gemm_simple_weighted_param_8,
	.param .u32 fused_moe_gemm_simple_weighted_param_9,
	.param .u32 fused_moe_gemm_simple_weighted_param_10,
	.param .u32 fused_moe_gemm_simple_weighted_param_11
)
{
	.reg .pred 	%p<25>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<37>;
	.reg .b32 	%r<82>;
	.reg .b64 	%rd<27>;


	ld.param.u64 	%rd4, [fused_moe_gemm_simple_weighted_param_0];
	ld.param.u64 	%rd5, [fused_moe_gemm_simple_weighted_param_1];
	ld.param.u64 	%rd6, [fused_moe_gemm_simple_weighted_param_2];
	ld.param.u64 	%rd7, [fused_moe_gemm_simple_weighted_param_3];
	ld.param.u64 	%rd8, [fused_moe_gemm_simple_weighted_param_4];
	ld.param.u64 	%rd9, [fused_moe_gemm_simple_weighted_param_5];
	ld.param.u64 	%rd10, [fused_moe_gemm_simple_weighted_param_6];
	ld.param.u32 	%r19, [fused_moe_gemm_simple_weighted_param_7];
	ld.param.u32 	%r20, [fused_moe_gemm_simple_weighted_param_8];
	ld.param.u32 	%r21, [fused_moe_gemm_simple_weighted_param_9];
	ld.param.u32 	%r22, [fused_moe_gemm_simple_weighted_param_10];
	ld.param.u32 	%r23, [fused_moe_gemm_simple_weighted_param_11];
	cvta.to.global.u64 	%rd11, %rd10;
	ld.global.nc.u32 	%r24, [%rd11];
	mov.u32 	%r1, %ctaid.x;
	setp.ge.s32 	%p1, %r1, %r24;
	@%p1 bra 	$L__BB4_18;

	cvta.to.global.u64 	%rd12, %rd8;
	mul.wide.s32 	%rd13, %r1, 4;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.nc.u32 	%r2, [%rd14];
	setp.ge.s32 	%p2, %r2, %r21;
	@%p2 bra 	$L__BB4_18;

	shr.s32 	%r25, %r1, 31;
	shr.u32 	%r26, %r25, 26;
	add.s32 	%r27, %r1, %r26;
	shr.s32 	%r28, %r27, 6;
	cvta.to.global.u64 	%rd15, %rd9;
	mul.wide.s32 	%rd16, %r28, 4;
	add.s64 	%rd17, %rd15, %rd16;
	ld.global.nc.u32 	%r3, [%rd17];
	setp.lt.s32 	%p3, %r3, 0;
	setp.ge.s32 	%p4, %r3, %r23;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB4_18;

	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.x;
	setp.lt.s32 	%p6, %r19, 1;
	@%p6 bra 	$L__BB4_18;

	cvta.to.global.u64 	%rd18, %rd7;
	mul.wide.s32 	%rd19, %r2, 4;
	add.s64 	%rd20, %rd18, %rd19;
	ld.global.nc.f32 	%f1, [%rd20];
	shr.s32 	%r30, %r5, 31;
	shr.u32 	%r31, %r30, 27;
	add.s32 	%r32, %r5, %r31;
	shr.s32 	%r6, %r32, 5;
	div.s32 	%r33, %r2, %r22;
	mul.lo.s32 	%r7, %r33, %r20;
	mul.lo.s32 	%r8, %r3, %r19;
	shr.s32 	%r34, %r4, 31;
	shr.u32 	%r35, %r34, 27;
	add.s32 	%r36, %r4, %r35;
	shr.s32 	%r37, %r36, 5;
	shl.b32 	%r38, %r37, 2;
	mov.u32 	%r39, simple_smem_raw;
	add.s32 	%r9, %r39, %r38;
	add.s32 	%r10, %r4, 31;
	and.b32  	%r40, %r36, -32;
	sub.s32 	%r11, %r4, %r40;
	shl.b32 	%r41, %r11, 2;
	add.s32 	%r12, %r39, %r41;
	mul.lo.s32 	%r13, %r2, %r19;
	cvta.to.global.u64 	%rd1, %rd5;
	cvta.to.global.u64 	%rd2, %rd4;
	cvta.to.global.u64 	%rd3, %rd6;
	mov.u32 	%r80, 0;

$L__BB4_5:
	setp.ge.s32 	%p7, %r4, %r20;
	mov.f32 	%f35, 0f00000000;
	@%p7 bra 	$L__BB4_8;

	add.s32 	%r42, %r80, %r8;
	mul.lo.s32 	%r15, %r42, %r20;
	mov.u32 	%r81, %r4;

$L__BB4_7:
	add.s32 	%r43, %r81, %r7;
	mul.wide.s32 	%rd21, %r43, 2;
	add.s64 	%rd22, %rd2, %rd21;
	ld.global.nc.u16 	%rs1, [%rd22];
	// begin inline asm
	{ mov.b32 %f11, {0,%rs1};}

	// end inline asm
	add.s32 	%r44, %r81, %r15;
	mul.wide.s32 	%rd23, %r44, 2;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.nc.u16 	%rs2, [%rd24];
	// begin inline asm
	{ mov.b32 %f12, {0,%rs2};}

	// end inline asm
	fma.rn.ftz.f32 	%f35, %f11, %f12, %f35;
	add.s32 	%r81, %r81, %r5;
	setp.lt.s32 	%p8, %r81, %r20;
	@%p8 bra 	$L__BB4_7;

$L__BB4_8:
	mov.b32 	%r45, %f35;
	mov.u32 	%r46, 31;
	mov.u32 	%r47, 16;
	mov.u32 	%r48, -1;
	shfl.sync.bfly.b32 	%r49|%p9, %r45, %r47, %r46, %r48;
	mov.b32 	%f13, %r49;
	add.ftz.f32 	%f14, %f35, %f13;
	mov.b32 	%r50, %f14;
	mov.u32 	%r51, 8;
	shfl.sync.bfly.b32 	%r52|%p10, %r50, %r51, %r46, %r48;
	mov.b32 	%f15, %r52;
	add.ftz.f32 	%f16, %f14, %f15;
	mov.b32 	%r53, %f16;
	mov.u32 	%r54, 4;
	shfl.sync.bfly.b32 	%r55|%p11, %r53, %r54, %r46, %r48;
	mov.b32 	%f17, %r55;
	add.ftz.f32 	%f18, %f16, %f17;
	mov.b32 	%r56, %f18;
	mov.u32 	%r57, 2;
	shfl.sync.bfly.b32 	%r58|%p12, %r56, %r57, %r46, %r48;
	mov.b32 	%f19, %r58;
	add.ftz.f32 	%f20, %f18, %f19;
	mov.b32 	%r59, %f20;
	mov.u32 	%r60, 1;
	shfl.sync.bfly.b32 	%r61|%p13, %r59, %r60, %r46, %r48;
	mov.b32 	%f21, %r61;
	add.ftz.f32 	%f5, %f20, %f21;
	setp.ne.s32 	%p14, %r11, 0;
	@%p14 bra 	$L__BB4_10;

	st.shared.f32 	[%r9], %f5;

$L__BB4_10:
	setp.gt.u32 	%p15, %r10, 62;
	bar.sync 	0;
	@%p15 bra 	$L__BB4_15;

	setp.ge.s32 	%p16, %r11, %r6;
	mov.f32 	%f36, 0f00000000;
	@%p16 bra 	$L__BB4_13;

	ld.shared.f32 	%f36, [%r12];

$L__BB4_13:
	mov.b32 	%r62, %f36;
	mov.u32 	%r63, 31;
	mov.u32 	%r64, 16;
	mov.u32 	%r65, -1;
	shfl.sync.bfly.b32 	%r66|%p17, %r62, %r64, %r63, %r65;
	mov.b32 	%f23, %r66;
	add.ftz.f32 	%f24, %f36, %f23;
	mov.b32 	%r67, %f24;
	mov.u32 	%r68, 8;
	shfl.sync.bfly.b32 	%r69|%p18, %r67, %r68, %r63, %r65;
	mov.b32 	%f25, %r69;
	add.ftz.f32 	%f26, %f24, %f25;
	mov.b32 	%r70, %f26;
	mov.u32 	%r71, 4;
	shfl.sync.bfly.b32 	%r72|%p19, %r70, %r71, %r63, %r65;
	mov.b32 	%f27, %r72;
	add.ftz.f32 	%f28, %f26, %f27;
	mov.b32 	%r73, %f28;
	mov.u32 	%r74, 2;
	shfl.sync.bfly.b32 	%r75|%p20, %r73, %r74, %r63, %r65;
	mov.b32 	%f29, %r75;
	add.ftz.f32 	%f30, %f28, %f29;
	mov.b32 	%r76, %f30;
	mov.u32 	%r77, 1;
	shfl.sync.bfly.b32 	%r78|%p21, %r76, %r77, %r63, %r65;
	mov.b32 	%f31, %r78;
	add.ftz.f32 	%f8, %f30, %f31;
	@%p14 bra 	$L__BB4_15;

	st.shared.f32 	[simple_smem_raw], %f8;

$L__BB4_15:
	setp.ne.s32 	%p23, %r4, 0;
	bar.sync 	0;
	@%p23 bra 	$L__BB4_17;

	ld.shared.f32 	%f33, [simple_smem_raw];
	mul.ftz.f32 	%f32, %f1, %f33;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs3, %f32;}

	// end inline asm
	add.s32 	%r79, %r80, %r13;
	mul.wide.s32 	%rd25, %r79, 2;
	add.s64 	%rd26, %rd3, %rd25;
	st.global.u16 	[%rd26], %rs3;

$L__BB4_17:
	bar.sync 	0;
	add.s32 	%r80, %r80, 1;
	setp.lt.s32 	%p24, %r80, %r19;
	@%p24 bra 	$L__BB4_5;

$L__BB4_18:
	ret;

}
	// .globl	fused_moe_gemm_simple_unweighted
.visible .entry fused_moe_gemm_simple_unweighted(
	.param .u64 fused_moe_gemm_simple_unweighted_param_0,
	.param .u64 fused_moe_gemm_simple_unweighted_param_1,
	.param .u64 fused_moe_gemm_simple_unweighted_param_2,
	.param .u64 fused_moe_gemm_simple_unweighted_param_3,
	.param .u64 fused_moe_gemm_simple_unweighted_param_4,
	.param .u64 fused_moe_gemm_simple_unweighted_param_5,
	.param .u64 fused_moe_gemm_simple_unweighted_param_6,
	.param .u32 fused_moe_gemm_simple_unweighted_param_7,
	.param .u32 fused_moe_gemm_simple_unweighted_param_8,
	.param .u32 fused_moe_gemm_simple_unweighted_param_9,
	.param .u32 fused_moe_gemm_simple_unweighted_param_10,
	.param .u32 fused_moe_gemm_simple_unweighted_param_11
)
{
	.reg .pred 	%p<25>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<35>;
	.reg .b32 	%r<82>;
	.reg .b64 	%rd<23>;


	ld.param.u64 	%rd4, [fused_moe_gemm_simple_unweighted_param_0];
	ld.param.u64 	%rd5, [fused_moe_gemm_simple_unweighted_param_1];
	ld.param.u64 	%rd6, [fused_moe_gemm_simple_unweighted_param_2];
	ld.param.u64 	%rd7, [fused_moe_gemm_simple_unweighted_param_4];
	ld.param.u64 	%rd8, [fused_moe_gemm_simple_unweighted_param_5];
	ld.param.u64 	%rd9, [fused_moe_gemm_simple_unweighted_param_6];
	ld.param.u32 	%r19, [fused_moe_gemm_simple_unweighted_param_7];
	ld.param.u32 	%r20, [fused_moe_gemm_simple_unweighted_param_8];
	ld.param.u32 	%r21, [fused_moe_gemm_simple_unweighted_param_9];
	ld.param.u32 	%r22, [fused_moe_gemm_simple_unweighted_param_10];
	ld.param.u32 	%r23, [fused_moe_gemm_simple_unweighted_param_11];
	cvta.to.global.u64 	%rd10, %rd9;
	ld.global.nc.u32 	%r24, [%rd10];
	mov.u32 	%r1, %ctaid.x;
	setp.ge.s32 	%p1, %r1, %r24;
	@%p1 bra 	$L__BB5_18;

	cvta.to.global.u64 	%rd11, %rd7;
	mul.wide.s32 	%rd12, %r1, 4;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.nc.u32 	%r2, [%rd13];
	setp.ge.s32 	%p2, %r2, %r21;
	@%p2 bra 	$L__BB5_18;

	shr.s32 	%r25, %r1, 31;
	shr.u32 	%r26, %r25, 26;
	add.s32 	%r27, %r1, %r26;
	shr.s32 	%r28, %r27, 6;
	cvta.to.global.u64 	%rd14, %rd8;
	mul.wide.s32 	%rd15, %r28, 4;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.nc.u32 	%r3, [%rd16];
	setp.lt.s32 	%p3, %r3, 0;
	setp.ge.s32 	%p4, %r3, %r23;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB5_18;

	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ntid.x;
	setp.lt.s32 	%p6, %r19, 1;
	@%p6 bra 	$L__BB5_18;

	shr.s32 	%r30, %r4, 31;
	shr.u32 	%r31, %r30, 27;
	add.s32 	%r32, %r4, %r31;
	shr.s32 	%r33, %r32, 5;
	shr.s32 	%r34, %r5, 31;
	shr.u32 	%r35, %r34, 27;
	add.s32 	%r36, %r5, %r35;
	shr.s32 	%r6, %r36, 5;
	div.s32 	%r37, %r2, %r22;
	mul.lo.s32 	%r7, %r37, %r20;
	mul.lo.s32 	%r8, %r3, %r19;
	shl.b32 	%r38, %r33, 2;
	mov.u32 	%r39, simple_smem_raw;
	add.s32 	%r9, %r39, %r38;
	add.s32 	%r10, %r4, 31;
	and.b32  	%r40, %r32, -32;
	sub.s32 	%r11, %r4, %r40;
	shl.b32 	%r41, %r11, 2;
	add.s32 	%r12, %r39, %r41;
	mul.lo.s32 	%r13, %r2, %r19;
	cvta.to.global.u64 	%rd1, %rd5;
	cvta.to.global.u64 	%rd2, %rd4;
	cvta.to.global.u64 	%rd3, %rd6;
	mov.u32 	%r80, 0;

$L__BB5_5:
	setp.ge.s32 	%p7, %r4, %r20;
	mov.f32 	%f33, 0f00000000;
	@%p7 bra 	$L__BB5_8;

	add.s32 	%r42, %r80, %r8;
	mul.lo.s32 	%r15, %r42, %r20;
	mov.u32 	%r81, %r4;

$L__BB5_7:
	add.s32 	%r43, %r81, %r7;
	mul.wide.s32 	%rd17, %r43, 2;
	add.s64 	%rd18, %rd2, %rd17;
	ld.global.nc.u16 	%rs1, [%rd18];
	// begin inline asm
	{ mov.b32 %f10, {0,%rs1};}

	// end inline asm
	add.s32 	%r44, %r81, %r15;
	mul.wide.s32 	%rd19, %r44, 2;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.nc.u16 	%rs2, [%rd20];
	// begin inline asm
	{ mov.b32 %f11, {0,%rs2};}

	// end inline asm
	fma.rn.ftz.f32 	%f33, %f10, %f11, %f33;
	add.s32 	%r81, %r81, %r5;
	setp.lt.s32 	%p8, %r81, %r20;
	@%p8 bra 	$L__BB5_7;

$L__BB5_8:
	mov.b32 	%r45, %f33;
	mov.u32 	%r46, 31;
	mov.u32 	%r47, 16;
	mov.u32 	%r48, -1;
	shfl.sync.bfly.b32 	%r49|%p9, %r45, %r47, %r46, %r48;
	mov.b32 	%f12, %r49;
	add.ftz.f32 	%f13, %f33, %f12;
	mov.b32 	%r50, %f13;
	mov.u32 	%r51, 8;
	shfl.sync.bfly.b32 	%r52|%p10, %r50, %r51, %r46, %r48;
	mov.b32 	%f14, %r52;
	add.ftz.f32 	%f15, %f13, %f14;
	mov.b32 	%r53, %f15;
	mov.u32 	%r54, 4;
	shfl.sync.bfly.b32 	%r55|%p11, %r53, %r54, %r46, %r48;
	mov.b32 	%f16, %r55;
	add.ftz.f32 	%f17, %f15, %f16;
	mov.b32 	%r56, %f17;
	mov.u32 	%r57, 2;
	shfl.sync.bfly.b32 	%r58|%p12, %r56, %r57, %r46, %r48;
	mov.b32 	%f18, %r58;
	add.ftz.f32 	%f19, %f17, %f18;
	mov.b32 	%r59, %f19;
	mov.u32 	%r60, 1;
	shfl.sync.bfly.b32 	%r61|%p13, %r59, %r60, %r46, %r48;
	mov.b32 	%f20, %r61;
	add.ftz.f32 	%f4, %f19, %f20;
	setp.ne.s32 	%p14, %r11, 0;
	@%p14 bra 	$L__BB5_10;

	st.shared.f32 	[%r9], %f4;

$L__BB5_10:
	setp.gt.u32 	%p15, %r10, 62;
	bar.sync 	0;
	@%p15 bra 	$L__BB5_15;

	setp.ge.s32 	%p16, %r11, %r6;
	mov.f32 	%f34, 0f00000000;
	@%p16 bra 	$L__BB5_13;

	ld.shared.f32 	%f34, [%r12];

$L__BB5_13:
	mov.b32 	%r62, %f34;
	mov.u32 	%r63, 31;
	mov.u32 	%r64, 16;
	mov.u32 	%r65, -1;
	shfl.sync.bfly.b32 	%r66|%p17, %r62, %r64, %r63, %r65;
	mov.b32 	%f22, %r66;
	add.ftz.f32 	%f23, %f34, %f22;
	mov.b32 	%r67, %f23;
	mov.u32 	%r68, 8;
	shfl.sync.bfly.b32 	%r69|%p18, %r67, %r68, %r63, %r65;
	mov.b32 	%f24, %r69;
	add.ftz.f32 	%f25, %f23, %f24;
	mov.b32 	%r70, %f25;
	mov.u32 	%r71, 4;
	shfl.sync.bfly.b32 	%r72|%p19, %r70, %r71, %r63, %r65;
	mov.b32 	%f26, %r72;
	add.ftz.f32 	%f27, %f25, %f26;
	mov.b32 	%r73, %f27;
	mov.u32 	%r74, 2;
	shfl.sync.bfly.b32 	%r75|%p20, %r73, %r74, %r63, %r65;
	mov.b32 	%f28, %r75;
	add.ftz.f32 	%f29, %f27, %f28;
	mov.b32 	%r76, %f29;
	mov.u32 	%r77, 1;
	shfl.sync.bfly.b32 	%r78|%p21, %r76, %r77, %r63, %r65;
	mov.b32 	%f30, %r78;
	add.ftz.f32 	%f7, %f29, %f30;
	@%p14 bra 	$L__BB5_15;

	st.shared.f32 	[simple_smem_raw], %f7;

$L__BB5_15:
	setp.ne.s32 	%p23, %r4, 0;
	bar.sync 	0;
	@%p23 bra 	$L__BB5_17;

	ld.shared.f32 	%f31, [simple_smem_raw];
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs3, %f31;}

	// end inline asm
	add.s32 	%r79, %r80, %r13;
	mul.wide.s32 	%rd21, %r79, 2;
	add.s64 	%rd22, %rd3, %rd21;
	st.global.u16 	[%rd22], %rs3;

$L__BB5_17:
	bar.sync 	0;
	add.s32 	%r80, %r80, 1;
	setp.lt.s32 	%p24, %r80, %r19;
	@%p24 bra 	$L__BB5_5;

$L__BB5_18:
	ret;

}

