//! Correctness validation: compare Rust engine output against Python vLLM references.
//!
//! Reference data is generated by `scripts/generate_reference.py` and stored as JSON
//! in `tests/correctness/reference_data/`. These tests load the reference data and
//! compare Rust engine output for exact token match (greedy) or logprob tolerance
//! (sampling).
//!
//! To generate reference data:
//!   python scripts/generate_reference.py --model <model_name>
//!
//! Tests are ignored by default (require model weights + reference data).
//! Run explicitly: cargo test --test reference_comparison -- --ignored

#![allow(unused_imports)]

use std::path::PathBuf;

use candle_core::Device;
use serde::Deserialize;
use vllm_core::{
    engine::{start_engine, CudaGraphConfig, EngineConfig, GenerationRequest, ModelForward},
    kv_cache::{config::CacheConfig, BlockTable, KVCacheDtype, KVCacheManager},
    sampling::SamplingParams,
    scheduler::{SchedulerConfig, SchedulingPolicy},
    tokenizer::TokenizerWrapper,
};

// ─── Reference data structures ──────────────────────────────────────────────

#[allow(dead_code)]
#[derive(Debug, Deserialize)]
struct ReferenceFile {
    metadata: ReferenceMetadata,
    results: Vec<ReferenceResult>,
}

#[allow(dead_code)]
#[derive(Debug, Deserialize)]
struct ReferenceMetadata {
    model: String,
    dtype: String,
    vllm_version: String,
    num_prompts: usize,
}

#[allow(dead_code)]
#[derive(Debug, Deserialize)]
struct ReferenceResult {
    id: String,
    prompt: String,
    params: ReferenceParams,
    output: ReferenceOutput,
    prompt_token_ids: Vec<u32>,
}

#[allow(dead_code)]
#[derive(Debug, Deserialize)]
struct ReferenceParams {
    max_tokens: usize,
    temperature: f32,
    top_p: f32,
    top_k: i32,
    seed: Option<u64>,
}

#[allow(dead_code)]
#[derive(Debug, Deserialize)]
struct ReferenceOutput {
    text: String,
    token_ids: Vec<u32>,
    cumulative_logprob: Option<f64>,
    finish_reason: String,
    logprobs: Vec<serde_json::Value>,
}

// ─── Helpers ────────────────────────────────────────────────────────────────

fn reference_data_dir() -> PathBuf {
    PathBuf::from(env!("CARGO_MANIFEST_DIR"))
        .join("tests")
        .join("correctness")
        .join("reference_data")
}

fn load_reference(model_name: &str) -> Option<ReferenceFile> {
    let sanitized = model_name.replace(['/', '\\'], "_");
    let path = reference_data_dir().join(format!("{sanitized}.json"));
    if !path.exists() {
        eprintln!(
            "Reference data not found: {}\nRun: python scripts/generate_reference.py --model {}",
            path.display(),
            model_name
        );
        return None;
    }
    let content = std::fs::read_to_string(&path).expect("Failed to read reference file");
    Some(serde_json::from_str(&content).expect("Failed to parse reference JSON"))
}

#[allow(dead_code)]
fn build_sampling_params(params: &ReferenceParams) -> SamplingParams {
    let mut sp = if params.temperature == 0.0 {
        SamplingParams::greedy()
    } else {
        SamplingParams {
            temperature: params.temperature,
            top_p: params.top_p,
            top_k: if params.top_k > 0 {
                params.top_k as u32
            } else {
                0
            },
            seed: params.seed,
            ..SamplingParams::greedy()
        }
    };
    sp.top_p = params.top_p;
    sp
}

// ─── Validation functions ───────────────────────────────────────────────────

/// For greedy decoding: token IDs must match exactly.
fn validate_greedy(reference: &ReferenceResult, actual_token_ids: &[u32]) {
    let ref_ids = &reference.output.token_ids;
    let min_len = ref_ids.len().min(actual_token_ids.len());

    let mut mismatches = 0;
    for i in 0..min_len {
        if ref_ids[i] != actual_token_ids[i] {
            eprintln!(
                "  Token mismatch at position {}: reference={}, actual={}",
                i, ref_ids[i], actual_token_ids[i]
            );
            mismatches += 1;
        }
    }

    assert_eq!(
        ref_ids.len(),
        actual_token_ids.len(),
        "Length mismatch for '{}': reference={}, actual={}",
        reference.id,
        ref_ids.len(),
        actual_token_ids.len()
    );

    assert_eq!(
        mismatches, 0,
        "Token mismatches for '{}': {}/{} tokens differ",
        reference.id, mismatches, min_len
    );
}

/// For sampling: compare logprobs within tolerance.
/// Token IDs may differ due to random sampling, but the logprob distribution
/// should be similar (top tokens should appear in both outputs' top-k).
fn validate_logprobs(reference: &ReferenceResult, actual_logprobs: &[f32], tolerance: f32) {
    if reference.output.logprobs.is_empty() || actual_logprobs.is_empty() {
        return;
    }

    // Compare cumulative logprob (should be within tolerance)
    if let Some(ref_cumlogprob) = reference.output.cumulative_logprob {
        let actual_cumlogprob: f64 = actual_logprobs.iter().map(|&lp| lp as f64).sum();
        let diff = (ref_cumlogprob - actual_cumlogprob).abs();
        // Cumulative logprob tolerance scales with sequence length
        let scaled_tolerance = tolerance as f64 * actual_logprobs.len() as f64;
        assert!(
            diff < scaled_tolerance,
            "Cumulative logprob mismatch for '{}': reference={:.4}, actual={:.4}, diff={:.4} (tolerance={:.4})",
            reference.id, ref_cumlogprob, actual_cumlogprob, diff, scaled_tolerance
        );
    }
}

// ─── Tests ──────────────────────────────────────────────────────────────────

/// Greedy decoding correctness: exact token match against Python vLLM.
///
/// This test requires:
/// 1. Reference data generated via `scripts/generate_reference.py`
/// 2. The model weights accessible to the Rust engine
///
/// Run: cargo test --test reference_comparison test_greedy_match -- --ignored
#[tokio::test]
#[ignore = "requires reference data and model weights"]
async fn test_greedy_match() {
    let model_name =
        std::env::var("VLLM_TEST_MODEL").unwrap_or_else(|_| "meta-llama/Llama-2-7b-hf".to_string());

    let reference = match load_reference(&model_name) {
        Some(r) => r,
        None => {
            eprintln!("Skipping: no reference data for {model_name}");
            return;
        }
    };

    eprintln!(
        "Loaded reference: model={}, vllm_version={}, {} test cases",
        reference.metadata.model,
        reference.metadata.vllm_version,
        reference.results.len()
    );

    // Filter greedy-only test cases
    let greedy_cases: Vec<&ReferenceResult> = reference
        .results
        .iter()
        .filter(|r| r.params.temperature == 0.0)
        .collect();

    eprintln!("Running {} greedy test cases", greedy_cases.len());

    // TODO: Initialize real model and engine here
    // For now, this test validates the framework structure
    for case in &greedy_cases {
        eprintln!(
            "  Test case: {} (prompt: '{}')",
            case.id,
            &case.prompt[..case.prompt.len().min(40)]
        );
        // When engine is connected:
        // let result = engine.generate(request).await.unwrap();
        // validate_greedy(case, &result.generated_token_ids);
    }
}

/// Sampling correctness: logprob tolerance against Python vLLM.
///
/// For seeded sampling, token IDs should match if the RNG implementation
/// is compatible. For unseeded, only logprobs are compared.
#[tokio::test]
#[ignore = "requires reference data and model weights"]
async fn test_sampling_logprobs() {
    let model_name =
        std::env::var("VLLM_TEST_MODEL").unwrap_or_else(|_| "meta-llama/Llama-2-7b-hf".to_string());

    let reference = match load_reference(&model_name) {
        Some(r) => r,
        None => {
            eprintln!("Skipping: no reference data for {model_name}");
            return;
        }
    };

    // Filter sampling test cases
    let sampling_cases: Vec<&ReferenceResult> = reference
        .results
        .iter()
        .filter(|r| r.params.temperature > 0.0)
        .collect();

    eprintln!("Running {} sampling test cases", sampling_cases.len());

    #[allow(unused)]
    const LOGPROB_TOLERANCE: f32 = 0.01;

    for case in &sampling_cases {
        eprintln!(
            "  Test case: {} (temp={}, top_p={}, seed={:?})",
            case.id, case.params.temperature, case.params.top_p, case.params.seed
        );
        // When engine is connected:
        // let result = engine.generate(request).await.unwrap();
        // if let Some(logprobs) = &result.token_logprobs {
        //     validate_logprobs(case, logprobs, LOGPROB_TOLERANCE);
        // }
    }
}

// ─── Mock-based correctness tests (always run, no model needed) ─────────

/// Validate that the test framework structures parse correctly.
#[test]
fn test_reference_json_parsing() {
    let json = r#"{
        "metadata": {
            "model": "test-model",
            "dtype": "float16",
            "vllm_version": "0.4.0",
            "num_prompts": 1
        },
        "results": [{
            "id": "test_case",
            "prompt": "Hello",
            "params": {
                "max_tokens": 10,
                "temperature": 0.0,
                "top_p": 1.0,
                "top_k": -1,
                "seed": null
            },
            "output": {
                "text": " world",
                "token_ids": [1917],
                "cumulative_logprob": -0.5,
                "finish_reason": "length",
                "logprobs": []
            },
            "prompt_token_ids": [15496]
        }]
    }"#;

    let parsed: ReferenceFile = serde_json::from_str(json).unwrap();
    assert_eq!(parsed.metadata.model, "test-model");
    assert_eq!(parsed.results.len(), 1);
    assert_eq!(parsed.results[0].output.token_ids, vec![1917]);
}

/// Validate greedy comparison logic.
#[test]
fn test_greedy_validation_exact_match() {
    let reference = ReferenceResult {
        id: "test".to_string(),
        prompt: "test".to_string(),
        params: ReferenceParams {
            max_tokens: 5,
            temperature: 0.0,
            top_p: 1.0,
            top_k: -1,
            seed: None,
        },
        output: ReferenceOutput {
            text: "hello".to_string(),
            token_ids: vec![1, 2, 3, 4, 5],
            cumulative_logprob: Some(-2.5),
            finish_reason: "length".to_string(),
            logprobs: vec![],
        },
        prompt_token_ids: vec![100],
    };

    // Exact match should pass
    validate_greedy(&reference, &[1, 2, 3, 4, 5]);
}

#[test]
#[should_panic(expected = "Token mismatches")]
fn test_greedy_validation_mismatch() {
    let reference = ReferenceResult {
        id: "test".to_string(),
        prompt: "test".to_string(),
        params: ReferenceParams {
            max_tokens: 5,
            temperature: 0.0,
            top_p: 1.0,
            top_k: -1,
            seed: None,
        },
        output: ReferenceOutput {
            text: "hello".to_string(),
            token_ids: vec![1, 2, 3, 4, 5],
            cumulative_logprob: None,
            finish_reason: "length".to_string(),
            logprobs: vec![],
        },
        prompt_token_ids: vec![100],
    };

    // Mismatch should panic
    validate_greedy(&reference, &[1, 2, 99, 4, 5]);
}

/// Validate logprob comparison logic.
#[test]
fn test_logprob_validation_within_tolerance() {
    let reference = ReferenceResult {
        id: "test".to_string(),
        prompt: "test".to_string(),
        params: ReferenceParams {
            max_tokens: 3,
            temperature: 0.7,
            top_p: 0.9,
            top_k: 50,
            seed: Some(42),
        },
        output: ReferenceOutput {
            text: "foo".to_string(),
            token_ids: vec![1, 2, 3],
            cumulative_logprob: Some(-3.0),
            finish_reason: "length".to_string(),
            logprobs: vec![],
        },
        prompt_token_ids: vec![100],
    };

    // Within tolerance
    validate_logprobs(&reference, &[-1.0, -1.0, -1.0], 0.01);

    // Just barely within tolerance (3 tokens * 0.01 tolerance = 0.03 total)
    validate_logprobs(&reference, &[-1.0, -1.0, -1.01], 0.01);
}
